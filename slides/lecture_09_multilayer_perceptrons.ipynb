{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5154b750",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Multilayer Perceptrons (MLPs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db67292",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"dnn.png\" alt=\"ANN Diagram\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2682f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single-Layer Perceptron\n",
    "- A perceptron is building block of artificial neural networks (will be defined later in the lecture)\n",
    "- Given a vector of inputs (x) and weights (w), the perceptron computes the weighted sum of the inputs:\n",
    "---\n",
    "$y = u \\left( \\sum_{i=1}^{n} w_i x_i + b \\right)$ \n",
    "\n",
    "---\n",
    "\n",
    "where $u$ is the step function:\n",
    "\n",
    "---\n",
    "$u(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "---  \n",
    "$w_i$ are the weights, $x_i$ are the inputs, and $b$ is the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8487baa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "- The activation function in the perceptron above is the step function.\n",
    "- In practice, we use other activation functions such as the sigmoid or tanh functions.\n",
    "    - Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "    - Tanh: $\\mathrm{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "- When the sigmoid is used, the single layer perceptron is simply a logistic regression model.\n",
    "- The activation function introduces non-linearity to the model.\n",
    "- The non-linearity allows the model to learn complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e08e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Rectified Linear Unit (ReLU)\n",
    "- The rectified linear unit (ReLU) is another popular activation function.\n",
    "- The ReLU function is defined as:\n",
    "---\n",
    "\n",
    "$\\mathrm{ReLU}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "---\n",
    "- Notice that if the input into the function is positive, the ReLU function returns the input value, otherwise it returns zero.\n",
    "- The gradient of the ReLU activation function is thus very easy to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2a669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's plot the sigmoid, tanh, and ReLU functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97769aef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "tanh = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "relu = np.maximum(0, z)\n",
    "\n",
    "plt.plot(z, sigmoid, label='Sigmoid')\n",
    "plt.plot(z, tanh, label='Tanh')\n",
    "plt.plot(z, relu, label='ReLU')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Activation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d9a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A perceptron with multiple output units\n",
    "- A perceptron can have multiple output units.\n",
    "- In this case, the weights are represented as a matrix.\n",
    "---\n",
    "$y = u \\left( W x + b \\right)$\n",
    "\n",
    "---\n",
    "- where $W$ is a matrix of weights, $x$ is the vector of inputs (features), and $b$ is a vector of biases.\n",
    "- Note that the bias term is applied *element-wise* to each output unit.\n",
    "- Note also that the output is now a vector of values, one for each output unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f245b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's implement a perceptron with multiple output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45a5237",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96770454 0.76852478]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# input vector\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "# weight matrix containing the weights for 2 output units\n",
    "W = np.array([[.1, .2, .3], [.4, .5, .6]])\n",
    "\n",
    "# the bias term for each output unit\n",
    "b = np.array([2, -2])\n",
    "\n",
    "# Let's use the sigmoidal nonlinearity\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "# compute the weighted sum of inputs\n",
    "z = np.dot(W, x) + b \n",
    "\n",
    "# apply the activation function (non-linearity) element-wise\n",
    "y = sigmoid(z) \n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1d828",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Layers\n",
    "- We now know how to implement a single-layer perceptron with multiple output units\n",
    "- The last ingredient that we need to build a multilayer perceptron is the **hidden layer**\n",
    "- A hidden layer resides between the inputs and outputs, and is tasked with learning intermediate features that help to map a given input to the correct output.\n",
    "- The *first* hidden layer is composed of a set of $m$ hidden units, each of which is a perceptron.\n",
    "---\n",
    "\n",
    "<center> hidden unit 1: $h_1 = u \\left( \\sum_{i=1}^{n} w_{1i} x_i + b_1 \\right)$ </center>\n",
    "\n",
    "<center> hidden unit 2: $h_2 = u \\left( \\sum_{i=1}^{n} w_{2i} x_i + b_2 \\right)$ </center>\n",
    "\n",
    "<center> ... </center>\n",
    "\n",
    "<center> hidden unit m: $h_m = u \\left( \\sum_{i=1}^{n} w_{mi} x_i + b_m \\right)$ </center>\n",
    "\n",
    "---\n",
    "- where $w_{ji}$ are the weights, $x_i$ are the inputs, and $b_j$ is the bias term for hidden unit $j$.\n",
    "- Collectively, the hidden units may be assembled into an $m$-dimensional vector $h$:\n",
    "---\n",
    "<center> $h = u \\left( W x + b \\right)$ </center>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936900fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Hidden Layers\n",
    "- We see above that the first hidden layer is essentially a perceptron from the inputs.\n",
    "- Importantly, we can have multiple hidden layers.\n",
    "- For example, the second hidden layer is connected to the output of the first hidden layer:\n",
    "---\n",
    "$h_2 = u(W_2 h_1 + b_2)$\n",
    "\n",
    "---\n",
    "- and the third hidden layer is connected to the output of the second hidden layer:\n",
    "\n",
    "---\n",
    "$h_3 = u(W_3 h_2 + b_3)$\n",
    "\n",
    "---\n",
    "- The number of elements (\"units\") in each hidden layer is a hyperparameter that can be tuned.\n",
    "- The number of units in each hidden layer is not necessarily the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808d3d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The output layer is connected to the last hidden layer.\n",
    "- Assuming that we have 3 hidden layers, the output layer is given by:\n",
    "---\n",
    "$y = u(W_o h_3 + b_o)$\n",
    "\n",
    "---\n",
    "- It is commonplace to use a softmax activation function in the output layer. ($u$ is the softmax function)\n",
    "- In this case, the value of each unit in the output layer represents a probability.\n",
    "    - For example, for a binary classification problem, the output layer will have two units, one for each class.\n",
    "    - The value of the first unit is the probability that the input belongs to class 0, and the value of the second unit is the probability that the input belongs to class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec0366",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks\n",
    "- A multilayer perceptron with multiple hidden layers is called an artificial neural network (ANN).\n",
    "- ANNs are loosely inspired by biological neural networks.\n",
    "- Below is the canonical structure of an ANN (in this case with two hidden layers):\n",
    "- \n",
    "[Artificial Neural Network](dnn.png)\n",
    "- The \"deep\" in deep learning refers to the number of hidden layers in the network.\n",
    "- As we will see later in the course, it has been empirically shown that adding layers to a neural network can improve its performance more so than adding units to each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccfcca",
   "metadata": {},
   "source": [
    "## Training Artificial Neural Networks\n",
    "- Training an artificial neural network involves adjusting the weights and biases of the network to minimize a loss function.\n",
    "- We have already seen the loss functions used to train ANNs: cross-entropy loss for classification and mean squared error for regression.\n",
    "- The weights of all of the layers are assembled into one large vector, and the biases are assembled into another vector.\n",
    "- The loss function is a function of the weights and biases.\n",
    "- Stochastic gradient descent is then used to find the values of all the weights and all the biases that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41729c",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "- The algorithm that actually implements stochastic gradient descent in ANNs is called \"backpropagation\"\n",
    "- The job of the backpropagation algorithm is to compute the gradients of the loss function with respect to the weights and biases.\n",
    "- Strictly speaking, we only need to implement the chain rule in order to compute these gradients.\n",
    "- However, the backpropagation algorithm is a clever way to compute these gradients efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13c258",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is PyTorch?\n",
    "- PyTorch is an open-source machine learning library developed by Facebook.\n",
    "- PyTorch provides a flexible and dynamic computational graph that allows for easy experimentation.\n",
    "- It is widely used in academia and industry for research and production.\n",
    "- PyTorch is used for building neural networks and other machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0e093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyTorch Tensors\n",
    "- PyTorch provides a powerful N-dimensional array object called a tensor.\n",
    "- Tensors are similar to NumPy arrays, but with additional features.\n",
    "- Tensors can be used on CPUs and GPUs.\n",
    "- Tensors support automatic differentiation for building neural networks.\n",
    "- Tensors can be created from Python lists, NumPy arrays, and other data types.\n",
    "- Tensors can be manipulated using a wide range of operations and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f6a1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Tensors\n",
    "- We can create tensors in PyTorch using the `torch.tensor` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f0f264da947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create a tensor from a list\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "\n",
    "# Create a tensor from a NumPy array\n",
    "x = torch.tensor(np.array([4, 5, 6]))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547911b003c4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate tensor operations\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "z = x + y\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33157e80dcb7259f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:49:42.919715Z",
     "start_time": "2024-10-28T19:49:42.915087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2654],\n",
       "        [ 2.0187],\n",
       "        [-1.2591]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a random 3 by 3 tensor\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 1)\n",
    "x @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d22f15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing a Neural Network in PyTorch\n",
    "- The following code implements a simple feedfoward neural network a specified number of hidden layers in PyTorch\n",
    "- We first need to import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cbbbfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:51:13.760542Z",
     "start_time": "2024-10-28T19:51:12.754741Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bcdee3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "- We will use a synthetic dataset generated using the `make_classification` function from scikit-learn.\n",
    "- The dataset has 10000 samples, 20 features, and 2 classes.\n",
    "- Below we will convert the dataset into PyTorch tensors and split it into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "586079ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:54:52.186056Z",
     "start_time": "2024-10-28T19:54:52.173746Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 20), (10000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "X = X.astype(np.float32)  # Convert to float32 for PyTorch\n",
    "y = y.astype(np.longlong)  # Convert to long for classification\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ebfdb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:55:33.793560Z",
     "start_time": "2024-10-28T19:55:33.787522Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 20]), torch.Size([10000]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e4470",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyTorch Datasets\n",
    "- PyTorch provides the `TensorDataset` class to create a dataset from tensors (\"X\" and \"y\").\n",
    "- We can use the `random_split` function to split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd3d893569dcf771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:05:59.427547Z",
     "start_time": "2024-10-28T20:05:59.424207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66d44f0aecbe000b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:18:19.764727Z",
     "start_time": "2024-10-28T20:18:19.758278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a8c80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Loaders\n",
    "- PyTorch provides the `DataLoader` class to create loaders from datasets.\n",
    "- These loaders are used to bring in the data in *batches* during training and evaluation.\n",
    "- The batch size specifies the number of samples to be used in each iteration -- this is the number of examples that is used to compute gradients and update the model parameters.\n",
    "- In modern-day machine learning, we rarely use the entire dataset at once due to memory constraints (datasets are often very large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "772108e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:25:22.159751Z",
     "start_time": "2024-10-28T20:25:22.156402Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46832824a6275b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:26:37.290472Z",
     "start_time": "2024-10-28T20:26:37.286769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x1160d5940>,\n",
       " <torch.utils.data.dataset.Subset at 0x1160d9c40>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset, val_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09661448",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a Multilayer Perceptron (MLP) in PyTorch\n",
    "- PyTorch provides the `nn.Module` class to create neural network models.\n",
    "- In this class, the user specifies the operations that the model should perform in order to map the input feature to the output of the network.\n",
    "- Below we define a simple MLP with a specified number of hidden layers.\n",
    "- The building block of the MLP is the `nn.Linear` class, which represents a linear transformation of the input data.\n",
    "    - This is essentially just a matrix multiplication followed by a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdde3b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple Multilayer Perceptron (MLP) class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        self.model = nn.Sequential(*layers) # the star operator unpacks the list\n",
    "\n",
    "    def forward(self, x): # the forward method is required and specifies how the model maps inputs to outputs\n",
    "        return self.model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560899fc1a6bb680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:13:25.035534Z",
     "start_time": "2024-10-28T19:13:24.989051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1238,  0.7192, -0.3244,  0.0961, -0.4307], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "my_linear = torch.nn.Linear(10, 5)\n",
    "sample_input = torch.randn(10)\n",
    "my_linear(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c067bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instantiating the model\n",
    "- So far we've define the model structure, but haven't yet created an *instance* of the model.\n",
    "- We can do this by simply calling the class with the appropriate arguments.\n",
    "    - These are the number of features in our input, the number of neurons in each hidden layer, and the number of output classes.\n",
    "- Our model will have a 20-dimensional input layer, two hidden layers with 64 and 32 units each, and a 2-dimensional output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372cb85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the MLP with 2 hidden layers\n",
    "input_size = X.shape[1]  # 20 features\n",
    "hidden_sizes = [64, 32]  # Two hidden layers with 64 and 32 neurons\n",
    "output_size = 2  # Binary classification (2 classes)\n",
    "\n",
    "mlp = MLP(input_size, hidden_sizes, output_size)\n",
    "print(mlp) # print out our model specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d9b88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the loss function and optimizer\n",
    "- We need to tell PyTorch how to train the model.\n",
    "- This consists of specifying the loss function (here will be the cross-entropy loss), and the optimizer (here we will use the \"Adam\" optimizer).\n",
    "    - The Adam optimizer is just a more fancy way of performing stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824a84a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e849d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model\n",
    "- It is standard practice to define a function that performs model training.\n",
    "- This function will:\n",
    "    - loop over the training data for a prescribed number of *epochs* (i.e., 10)\n",
    "    - compute the loss\n",
    "    - compute the gradients\n",
    "    - update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c55ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, loader, criterion, optimizer, epochs=30):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) # compute the loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward() # compute the gradients\n",
    "            optimizer.step() # update the model parameters using the gradients\n",
    "            \n",
    "            running_loss += loss.item() # keep track of the loss\n",
    "        \n",
    "        avg_loss = running_loss / len(loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84d115",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we actually perform the training by calling our 'train' function\n",
    "train(mlp, train_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12920a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a validation function\n",
    "- We can define a function to evaluate the model on the *validation* set.\n",
    "- The validation set is similar to a test set, but is often used to tune the model hyperparameters.\n",
    "- Importantly, the validation loop does not update the model parameters, but simply calls the model to make predictions so that we can measure model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ca8e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Get predicted class\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = val_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8cf7b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validate(mlp, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68364b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How did our model do?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
