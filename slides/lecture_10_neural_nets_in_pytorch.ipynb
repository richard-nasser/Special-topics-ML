{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd38c16f6e760cf6",
   "metadata": {},
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Neural Networks in PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c8b521801667a",
   "metadata": {},
   "source": [
    "## Classes and objects in Python\n",
    "- An ```object``` is a structure that contains both data (variables) and functions (methods)\n",
    "- A ```class``` is a blueprint for creating objects\n",
    "- Classes are used to organize programs and make them more modular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ede7219a5ca06d",
   "metadata": {},
   "source": [
    "## Example: Medical record class\n",
    "- We will create a class to represent a patient's medical record\n",
    "- The class will have the following attributes:\n",
    "    - Name\n",
    "    - Age\n",
    "    - Height\n",
    "    - Weight\n",
    "    - BMI\n",
    "- The class will have the following methods:\n",
    "    - ```calculate_bmi```: calculates the BMI of the patient\n",
    "    - ```print_record```: prints the patient's record\n",
    "    - ```update_weight```: updates the patient's weight\n",
    "    - ```update_height```: updates the patient's height\n",
    "    - ```update_age```: updates the patient's age\n",
    "    - ```update_name```: updates the patient's name\n",
    "    - ```update_record```: updates the patient's record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42ffbe5a66dc785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:38:47.761728Z",
     "start_time": "2024-11-04T19:38:47.753714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/ll8tk31d5kxgzdh7qt8w6q8w0000gn/T/ipykernel_2634/1471338438.py:1: DeprecationWarning: \n",
      "\n",
      "  `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "  of the deprecation of `distutils` itself. It will be removed for\n",
      "  Python >= 3.12. For older Python versions it will remain present.\n",
      "  It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "  For more details, see:\n",
      "    https://numpy.org/devdocs/reference/distutils_status_migration.html \n",
      "\n",
      "\n",
      "  from numpy.distutils.system_info import x11_info\n"
     ]
    }
   ],
   "source": [
    "from numpy.distutils.system_info import x11_info\n",
    "\n",
    "\n",
    "class Address:\n",
    "    def __init__(self, street, city, state, zip_code):\n",
    "        self.street = street\n",
    "        self.city = city\n",
    "        self.state = state\n",
    "        self.zip_code = zip_code\n",
    "\n",
    "    def print_address(self):\n",
    "        print(f'Street: {self.street}')\n",
    "        print(f'City: {self.city}')\n",
    "        print(f'State: {self.state}')\n",
    "        print(f'Zip Code: {self.zip_code}')\n",
    "\n",
    "class MedicalRecord: # medical_record is non-camel notation\n",
    "    def __init__(self, name, age, height, weight, address):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "        self.bmi = self.calculate_bmi()\n",
    "        self.address = address\n",
    "        \n",
    "    def calculate_bmi(self):\n",
    "        return self.weight / (self.height**2)\n",
    "\n",
    "    def print_record(self):\n",
    "        print(f'Name: {self.name}')\n",
    "        print(f'Age: {self.age}')\n",
    "        print(f'Height: {self.height}')\n",
    "        print(f'Weight: {self.weight}')\n",
    "        print(f'BMI: {self.bmi}')\n",
    "        self.address.print_address()\n",
    "\n",
    "    def update_weight(self, weight):\n",
    "        self.weight = weight\n",
    "        self.bmi = self.calculate_bmi()\n",
    "\n",
    "    def update_height(self, height):\n",
    "        self.height = height\n",
    "        self.bmi = self.calculate_bmi()\n",
    "\n",
    "    def update_age(self, age):\n",
    "        self.age = age\n",
    "\n",
    "    def update_name(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def update_record(self, name, age, height, weight):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "        self.bmi = self.calculate_bmi()\n",
    "        \n",
    "    def update_address(self, street, city, state, zip_code):\n",
    "        self.address.street = street\n",
    "        self.address.city = city\n",
    "        self.address.state = state\n",
    "        self.address.zip_code = zip_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563bda05ecec1d7",
   "metadata": {},
   "source": [
    "## Creating an object of the MedicalRecord class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6deab8f87d8c64c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:39:43.596209Z",
     "start_time": "2024-11-04T19:39:43.593531Z"
    }
   },
   "outputs": [],
   "source": [
    "patient_address = Address('123 Main St.', 'New York', 'NY', '10001') # create an instance of Address\n",
    "patient = MedicalRecord('Jacek', 29, 1.77, 73, patient_address ) # create an instance of MedicalRecord\n",
    "# patient is an instance of the MedicalRecord class (an object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51091badb893646b",
   "metadata": {},
   "source": [
    "## Obtaining *attributes* of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993e48d7ac328a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:39:45.414097Z",
     "start_time": "2024-11-04T19:39:45.411133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacek\n",
      "29\n",
      "Street: 123 Main St.\n",
      "City: New York\n",
      "State: NY\n",
      "Zip Code: 10001\n"
     ]
    }
   ],
   "source": [
    "print(patient.name)\n",
    "print(patient.age)\n",
    "patient.address.print_address()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc820f0bbe0edde8",
   "metadata": {},
   "source": [
    "## Calling *methods* of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa94e5dd78d22589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:39:46.774824Z",
     "start_time": "2024-11-04T19:39:46.771692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Jacek\n",
      "Age: 29\n",
      "Height: 1.77\n",
      "Weight: 73\n",
      "BMI: 23.301094832264035\n",
      "Street: 123 Main St.\n",
      "City: New York\n",
      "State: NY\n",
      "Zip Code: 10001\n"
     ]
    }
   ],
   "source": [
    "patient.print_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c294f55c365e881",
   "metadata": {},
   "source": [
    "## From theory to practice\n",
    "- We have seen the theory of multilayer perceptrons (MLPs) in the previous lecture\n",
    "- We now turn to the practical aspects of implementing MLPs in PyTorch\n",
    "- We will begin by implementing logistic regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06693d253744048",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25bb4183db340b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:00:02.496241Z",
     "start_time": "2024-11-04T21:00:00.814174Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f971fb6fdc5ee2b",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset\n",
    "- We will make use of scikit-learn's ```make_classification``` function to generate a simple binary classification dataset\n",
    "- We will split the dataset into training and validation sets\n",
    "- We will then convert the NumPy arrays to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c41a84519ff00e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:16:38.066875Z",
     "start_time": "2024-11-04T21:16:38.040975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 50]),\n",
       " torch.Size([8000]),\n",
       " torch.Size([2000, 50]),\n",
       " torch.Size([2000]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate features and labels\n",
    "X, y = make_classification(n_samples=10000, n_features=50, n_informative=5, n_redundant=0, random_state=42)\n",
    "\n",
    "# train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert features and labels into PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b7ee6ffe1193e",
   "metadata": {},
   "source": [
    "## Logistic regression in PyTorch\n",
    "- A neural network in PyTorch is an instance of the ```nn.Module``` class\n",
    "- Each neural network consists of a series of *layers*\n",
    "- In our simple case, we will use a single layer for logistic regression\n",
    "- Each nn.Module subclass must implement a ```forward``` method that defines the forward pass of the network\n",
    "- Remember that ``forward pass'' means computing the output of the network given the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6b4683b258fabc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:00:05.248862Z",
     "start_time": "2024-11-04T21:00:05.242487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([-0.7172, -0.6050, -0.0801, -0.5211, -1.5329])\n",
      "y tensor([ 0.7824, -0.3023, -0.7245], grad_fn=<ViewBackward0>)\n",
      "z tensor([0.6569], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_linear_block = nn.Linear(5, 3)\n",
    "my_second_linear_block = nn.Linear(3, 1)\n",
    "\n",
    "# create a random tensor of 5 elements\n",
    "x = torch.randn(5)\n",
    "\n",
    "# pass the tensor through the linear block\n",
    "y = my_linear_block(x)\n",
    "z = my_second_linear_block(y)\n",
    "\n",
    "print(\"x\", x)\n",
    "print(\"y\", y)\n",
    "print(\"z\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e436f1c5b255c436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:00:06.151298Z",
     "start_time": "2024-11-04T21:00:06.147980Z"
    }
   },
   "outputs": [],
   "source": [
    "# y = Wx + b (W: weight matrix, b: bias vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a771525566881143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:00:06.704210Z",
     "start_time": "2024-11-04T21:00:06.701148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.3502,  0.0911,  0.0267, -0.1784, -0.3567],\n",
       "         [-0.2245, -0.2281,  0.0855,  0.2696,  0.0455],\n",
       "         [ 0.0065, -0.1640,  0.2623, -0.2389,  0.3890]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0513, -0.3842, -0.3263], requires_grad=True))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_block.weight, my_linear_block.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b77720d49b2a555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:16:45.603708Z",
     "start_time": "2024-11-04T21:16:45.600079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size): # input_size: number of features, output_size: number of classes\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        \n",
    "        # Single layer for logistic regression\n",
    "        self.linear = nn.Linear(input_size, output_size) # \"linear\" is the *name* of our layer\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax activation function to convert outputs to probabilities\n",
    "\n",
    "    def forward(self, x): # implements the forward pass\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d2cbc993494b9",
   "metadata": {},
   "source": [
    "## Creating an instance of the model\n",
    "- So far we have defined a class, which is a blueprint for the model\n",
    "- To actually create a model that we can work with, we need to *instantiate* the class\n",
    "- Instantiating a class means creating an *object* of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20ec570bcfea3422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:13.128778Z",
     "start_time": "2024-11-04T21:17:13.123962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is 50\n",
      "The number of classes is 2\n",
      "Now creating model...\n",
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "# Number of features\n",
    "input_size = X_train.shape[1]\n",
    "print(f\"The number of features is {input_size}\")\n",
    "\n",
    "# Number of classes\n",
    "output_size = 2  # Binary classification\n",
    "print(f\"The number of classes is {output_size}\")\n",
    "\n",
    "print(\"Now creating model...\")\n",
    "model = LogisticRegressionModel(input_size, output_size)\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c351acaba7fa0",
   "metadata": {},
   "source": [
    "## Inspecting the model\n",
    "- The model object provides a convenient way to access the layers and parameters of the model\n",
    "- Let's take a look at the weights and biases of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d12a297ca2a63f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:14.549447Z",
     "start_time": "2024-11-04T21:17:14.544454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0879,  0.0508,  0.0985,  0.0662,  0.1055, -0.0887,  0.0794, -0.0403,\n",
       "         -0.0117, -0.0565,  0.0405, -0.0024, -0.1264,  0.0983,  0.0860, -0.0514,\n",
       "          0.0462,  0.0588,  0.0087, -0.0756,  0.0532, -0.0193, -0.1364, -0.0958,\n",
       "          0.0557,  0.0315,  0.1041, -0.0313,  0.0164,  0.0916, -0.1101,  0.0252,\n",
       "          0.0274, -0.0025, -0.1339,  0.0169, -0.0894,  0.0179,  0.0156, -0.0924,\n",
       "          0.0334, -0.0667,  0.0488,  0.0324, -0.0386, -0.1048, -0.0550,  0.1120,\n",
       "          0.0422,  0.0882],\n",
       "        [-0.0076,  0.1303,  0.0632, -0.1282, -0.0014, -0.0281, -0.0903,  0.0260,\n",
       "          0.0672, -0.0780, -0.0336,  0.0372, -0.1154, -0.0998,  0.0315, -0.0182,\n",
       "          0.0537, -0.0940,  0.0158,  0.1361, -0.0546, -0.1182, -0.0054, -0.0188,\n",
       "          0.0150, -0.0564,  0.1295,  0.1362,  0.0269, -0.1276,  0.1147,  0.1192,\n",
       "         -0.0112,  0.1063, -0.0137,  0.0727, -0.0583, -0.0364, -0.0174, -0.0074,\n",
       "          0.0842,  0.0555,  0.0990,  0.1388,  0.0951, -0.0799, -0.0146,  0.0737,\n",
       "          0.0626, -0.0044]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d45feb1037351aec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:15.004016Z",
     "start_time": "2024-11-04T21:17:14.998532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0002, -0.0215], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1debb2745de4e",
   "metadata": {},
   "source": [
    "### output = softmax(  model.linear.weight * input + model.linear.bias )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d7839db80e700",
   "metadata": {},
   "source": [
    "### Q1: Why are there two rows in the weight matrix?\n",
    "### Q2: Why are there two columns in the weight matrix?\n",
    "### Q3: Where did the weights and biases come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3beefa88ac351",
   "metadata": {},
   "source": [
    "## Evaluating the (untrained) model on random inputs\n",
    "- Before training the model, let's see what the model predicts on random inputs\n",
    "- This will help us to understand how the forward pass works in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb1aa6dae9a57c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:16.513576Z",
     "start_time": "2024-11-04T21:17:16.507895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[ 0.4030, -0.3536, -1.8120,  1.4469, -1.0518, -0.2141, -0.2104,  0.0841,\n",
      "         -0.3992, -1.4182,  0.1195, -0.3839,  0.4040,  1.2041, -0.3754,  1.3666,\n",
      "          1.5386,  0.7277, -0.2854, -0.4748,  0.3696, -0.8776, -1.1102,  0.6735,\n",
      "          0.5964, -0.1601,  0.7012,  1.2989, -0.0540, -1.1074, -1.2232,  0.5936,\n",
      "          0.6685, -2.0412,  0.9688,  1.2765,  0.1177,  0.4753, -0.5763, -0.4539,\n",
      "         -0.3154,  1.2582,  0.8262,  0.7063,  1.5343, -1.2153,  1.0596,  1.1167,\n",
      "         -1.0032, -1.9738]])\n",
      "output: tensor([[0.4672, 0.5328]], grad_fn=<SoftmaxBackward0>)\n",
      "output: tensor([[0.4672, 0.5328]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Random input\n",
    "x = torch.randn(1, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output1 = model(x)\n",
    "output2 = model.forward(x)\n",
    "\n",
    "print(f\"input: {x}\")\n",
    "print(f\"output: {output1}\")\n",
    "print(f\"output: {output2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce00d70720f7b12",
   "metadata": {},
   "source": [
    "## Evaluating model on a batch of inputs\n",
    "- A *batch* refers to a set of examples that is used to compute the gradient of the loss function during training\n",
    "- A typical batch size is 32, 64, or 128 examples\n",
    "- An *epoch* refers to a complete pass through the dataset, such that each examples has appeared in a batch once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f853b2e9463214b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:18.004961Z",
     "start_time": "2024-11-04T21:17:17.998127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5472, 0.4528],\n",
       "        [0.5633, 0.4367],\n",
       "        [0.3979, 0.6021],\n",
       "        [0.3983, 0.6017],\n",
       "        [0.1969, 0.8031]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random batch of inputs\n",
    "X_batch = torch.randn(5, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(X_batch)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556bd01e3509d65",
   "metadata": {},
   "source": [
    "## Defining the loss function and optimizer\n",
    "- PyTorch provides a wide range of loss functions and optimizers\n",
    "- For logistic regression, we will use the cross-entropy loss and stochastic gradient descent (SGD) optimizer\n",
    "- An important parameter for the optimizer is the *learning rate*, which controls the size of the updates to the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66deaba6ceaeebb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:19.314841Z",
     "start_time": "2024-11-04T21:17:19.309664Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # learning rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af062ef07236b1",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "- We are now ready to run our stochastic gradient descent (SGD) algorithm to train the model\n",
    "- For simplicity, we will use all of the data in a single batch (batch size = 100 = number of training examples)\n",
    "- We will train over 100 epochs\n",
    "- At each epoch, we will compute the loss, compute the gradients, and update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af2ed26b2ac317bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:52.744766Z",
     "start_time": "2024-11-04T21:17:20.355869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30000], Loss: 0.7249\n",
      "Epoch [40/30000], Loss: 0.7041\n",
      "Epoch [60/30000], Loss: 0.6855\n",
      "Epoch [80/30000], Loss: 0.6695\n",
      "Epoch [100/30000], Loss: 0.6558\n",
      "Epoch [120/30000], Loss: 0.6443\n",
      "Epoch [140/30000], Loss: 0.6344\n",
      "Epoch [160/30000], Loss: 0.6259\n",
      "Epoch [180/30000], Loss: 0.6186\n",
      "Epoch [200/30000], Loss: 0.6121\n",
      "Epoch [220/30000], Loss: 0.6065\n",
      "Epoch [240/30000], Loss: 0.6015\n",
      "Epoch [260/30000], Loss: 0.5970\n",
      "Epoch [280/30000], Loss: 0.5930\n",
      "Epoch [300/30000], Loss: 0.5893\n",
      "Epoch [320/30000], Loss: 0.5860\n",
      "Epoch [340/30000], Loss: 0.5830\n",
      "Epoch [360/30000], Loss: 0.5803\n",
      "Epoch [380/30000], Loss: 0.5777\n",
      "Epoch [400/30000], Loss: 0.5754\n",
      "Epoch [420/30000], Loss: 0.5732\n",
      "Epoch [440/30000], Loss: 0.5712\n",
      "Epoch [460/30000], Loss: 0.5693\n",
      "Epoch [480/30000], Loss: 0.5676\n",
      "Epoch [500/30000], Loss: 0.5660\n",
      "Epoch [520/30000], Loss: 0.5644\n",
      "Epoch [540/30000], Loss: 0.5630\n",
      "Epoch [560/30000], Loss: 0.5617\n",
      "Epoch [580/30000], Loss: 0.5604\n",
      "Epoch [600/30000], Loss: 0.5592\n",
      "Epoch [620/30000], Loss: 0.5581\n",
      "Epoch [640/30000], Loss: 0.5570\n",
      "Epoch [660/30000], Loss: 0.5560\n",
      "Epoch [680/30000], Loss: 0.5550\n",
      "Epoch [700/30000], Loss: 0.5541\n",
      "Epoch [720/30000], Loss: 0.5532\n",
      "Epoch [740/30000], Loss: 0.5524\n",
      "Epoch [760/30000], Loss: 0.5516\n",
      "Epoch [780/30000], Loss: 0.5508\n",
      "Epoch [800/30000], Loss: 0.5501\n",
      "Epoch [820/30000], Loss: 0.5494\n",
      "Epoch [840/30000], Loss: 0.5487\n",
      "Epoch [860/30000], Loss: 0.5481\n",
      "Epoch [880/30000], Loss: 0.5475\n",
      "Epoch [900/30000], Loss: 0.5469\n",
      "Epoch [920/30000], Loss: 0.5463\n",
      "Epoch [940/30000], Loss: 0.5458\n",
      "Epoch [960/30000], Loss: 0.5452\n",
      "Epoch [980/30000], Loss: 0.5447\n",
      "Epoch [1000/30000], Loss: 0.5442\n",
      "Epoch [1020/30000], Loss: 0.5438\n",
      "Epoch [1040/30000], Loss: 0.5433\n",
      "Epoch [1060/30000], Loss: 0.5429\n",
      "Epoch [1080/30000], Loss: 0.5424\n",
      "Epoch [1100/30000], Loss: 0.5420\n",
      "Epoch [1120/30000], Loss: 0.5416\n",
      "Epoch [1140/30000], Loss: 0.5412\n",
      "Epoch [1160/30000], Loss: 0.5409\n",
      "Epoch [1180/30000], Loss: 0.5405\n",
      "Epoch [1200/30000], Loss: 0.5401\n",
      "Epoch [1220/30000], Loss: 0.5398\n",
      "Epoch [1240/30000], Loss: 0.5395\n",
      "Epoch [1260/30000], Loss: 0.5391\n",
      "Epoch [1280/30000], Loss: 0.5388\n",
      "Epoch [1300/30000], Loss: 0.5385\n",
      "Epoch [1320/30000], Loss: 0.5382\n",
      "Epoch [1340/30000], Loss: 0.5379\n",
      "Epoch [1360/30000], Loss: 0.5376\n",
      "Epoch [1380/30000], Loss: 0.5374\n",
      "Epoch [1400/30000], Loss: 0.5371\n",
      "Epoch [1420/30000], Loss: 0.5368\n",
      "Epoch [1440/30000], Loss: 0.5366\n",
      "Epoch [1460/30000], Loss: 0.5363\n",
      "Epoch [1480/30000], Loss: 0.5361\n",
      "Epoch [1500/30000], Loss: 0.5358\n",
      "Epoch [1520/30000], Loss: 0.5356\n",
      "Epoch [1540/30000], Loss: 0.5354\n",
      "Epoch [1560/30000], Loss: 0.5352\n",
      "Epoch [1580/30000], Loss: 0.5349\n",
      "Epoch [1600/30000], Loss: 0.5347\n",
      "Epoch [1620/30000], Loss: 0.5345\n",
      "Epoch [1640/30000], Loss: 0.5343\n",
      "Epoch [1660/30000], Loss: 0.5341\n",
      "Epoch [1680/30000], Loss: 0.5339\n",
      "Epoch [1700/30000], Loss: 0.5337\n",
      "Epoch [1720/30000], Loss: 0.5335\n",
      "Epoch [1740/30000], Loss: 0.5333\n",
      "Epoch [1760/30000], Loss: 0.5332\n",
      "Epoch [1780/30000], Loss: 0.5330\n",
      "Epoch [1800/30000], Loss: 0.5328\n",
      "Epoch [1820/30000], Loss: 0.5326\n",
      "Epoch [1840/30000], Loss: 0.5325\n",
      "Epoch [1860/30000], Loss: 0.5323\n",
      "Epoch [1880/30000], Loss: 0.5321\n",
      "Epoch [1900/30000], Loss: 0.5320\n",
      "Epoch [1920/30000], Loss: 0.5318\n",
      "Epoch [1940/30000], Loss: 0.5317\n",
      "Epoch [1960/30000], Loss: 0.5315\n",
      "Epoch [1980/30000], Loss: 0.5314\n",
      "Epoch [2000/30000], Loss: 0.5312\n",
      "Epoch [2020/30000], Loss: 0.5311\n",
      "Epoch [2040/30000], Loss: 0.5310\n",
      "Epoch [2060/30000], Loss: 0.5308\n",
      "Epoch [2080/30000], Loss: 0.5307\n",
      "Epoch [2100/30000], Loss: 0.5306\n",
      "Epoch [2120/30000], Loss: 0.5304\n",
      "Epoch [2140/30000], Loss: 0.5303\n",
      "Epoch [2160/30000], Loss: 0.5302\n",
      "Epoch [2180/30000], Loss: 0.5300\n",
      "Epoch [2200/30000], Loss: 0.5299\n",
      "Epoch [2220/30000], Loss: 0.5298\n",
      "Epoch [2240/30000], Loss: 0.5297\n",
      "Epoch [2260/30000], Loss: 0.5296\n",
      "Epoch [2280/30000], Loss: 0.5294\n",
      "Epoch [2300/30000], Loss: 0.5293\n",
      "Epoch [2320/30000], Loss: 0.5292\n",
      "Epoch [2340/30000], Loss: 0.5291\n",
      "Epoch [2360/30000], Loss: 0.5290\n",
      "Epoch [2380/30000], Loss: 0.5289\n",
      "Epoch [2400/30000], Loss: 0.5288\n",
      "Epoch [2420/30000], Loss: 0.5287\n",
      "Epoch [2440/30000], Loss: 0.5286\n",
      "Epoch [2460/30000], Loss: 0.5285\n",
      "Epoch [2480/30000], Loss: 0.5284\n",
      "Epoch [2500/30000], Loss: 0.5283\n",
      "Epoch [2520/30000], Loss: 0.5282\n",
      "Epoch [2540/30000], Loss: 0.5281\n",
      "Epoch [2560/30000], Loss: 0.5280\n",
      "Epoch [2580/30000], Loss: 0.5279\n",
      "Epoch [2600/30000], Loss: 0.5278\n",
      "Epoch [2620/30000], Loss: 0.5277\n",
      "Epoch [2640/30000], Loss: 0.5276\n",
      "Epoch [2660/30000], Loss: 0.5275\n",
      "Epoch [2680/30000], Loss: 0.5274\n",
      "Epoch [2700/30000], Loss: 0.5274\n",
      "Epoch [2720/30000], Loss: 0.5273\n",
      "Epoch [2740/30000], Loss: 0.5272\n",
      "Epoch [2760/30000], Loss: 0.5271\n",
      "Epoch [2780/30000], Loss: 0.5270\n",
      "Epoch [2800/30000], Loss: 0.5269\n",
      "Epoch [2820/30000], Loss: 0.5269\n",
      "Epoch [2840/30000], Loss: 0.5268\n",
      "Epoch [2860/30000], Loss: 0.5267\n",
      "Epoch [2880/30000], Loss: 0.5266\n",
      "Epoch [2900/30000], Loss: 0.5265\n",
      "Epoch [2920/30000], Loss: 0.5265\n",
      "Epoch [2940/30000], Loss: 0.5264\n",
      "Epoch [2960/30000], Loss: 0.5263\n",
      "Epoch [2980/30000], Loss: 0.5262\n",
      "Epoch [3000/30000], Loss: 0.5262\n",
      "Epoch [3020/30000], Loss: 0.5261\n",
      "Epoch [3040/30000], Loss: 0.5260\n",
      "Epoch [3060/30000], Loss: 0.5259\n",
      "Epoch [3080/30000], Loss: 0.5259\n",
      "Epoch [3100/30000], Loss: 0.5258\n",
      "Epoch [3120/30000], Loss: 0.5257\n",
      "Epoch [3140/30000], Loss: 0.5257\n",
      "Epoch [3160/30000], Loss: 0.5256\n",
      "Epoch [3180/30000], Loss: 0.5255\n",
      "Epoch [3200/30000], Loss: 0.5255\n",
      "Epoch [3220/30000], Loss: 0.5254\n",
      "Epoch [3240/30000], Loss: 0.5253\n",
      "Epoch [3260/30000], Loss: 0.5253\n",
      "Epoch [3280/30000], Loss: 0.5252\n",
      "Epoch [3300/30000], Loss: 0.5252\n",
      "Epoch [3320/30000], Loss: 0.5251\n",
      "Epoch [3340/30000], Loss: 0.5250\n",
      "Epoch [3360/30000], Loss: 0.5250\n",
      "Epoch [3380/30000], Loss: 0.5249\n",
      "Epoch [3400/30000], Loss: 0.5249\n",
      "Epoch [3420/30000], Loss: 0.5248\n",
      "Epoch [3440/30000], Loss: 0.5247\n",
      "Epoch [3460/30000], Loss: 0.5247\n",
      "Epoch [3480/30000], Loss: 0.5246\n",
      "Epoch [3500/30000], Loss: 0.5246\n",
      "Epoch [3520/30000], Loss: 0.5245\n",
      "Epoch [3540/30000], Loss: 0.5245\n",
      "Epoch [3560/30000], Loss: 0.5244\n",
      "Epoch [3580/30000], Loss: 0.5243\n",
      "Epoch [3600/30000], Loss: 0.5243\n",
      "Epoch [3620/30000], Loss: 0.5242\n",
      "Epoch [3640/30000], Loss: 0.5242\n",
      "Epoch [3660/30000], Loss: 0.5241\n",
      "Epoch [3680/30000], Loss: 0.5241\n",
      "Epoch [3700/30000], Loss: 0.5240\n",
      "Epoch [3720/30000], Loss: 0.5240\n",
      "Epoch [3740/30000], Loss: 0.5239\n",
      "Epoch [3760/30000], Loss: 0.5239\n",
      "Epoch [3780/30000], Loss: 0.5238\n",
      "Epoch [3800/30000], Loss: 0.5238\n",
      "Epoch [3820/30000], Loss: 0.5237\n",
      "Epoch [3840/30000], Loss: 0.5237\n",
      "Epoch [3860/30000], Loss: 0.5236\n",
      "Epoch [3880/30000], Loss: 0.5236\n",
      "Epoch [3900/30000], Loss: 0.5235\n",
      "Epoch [3920/30000], Loss: 0.5235\n",
      "Epoch [3940/30000], Loss: 0.5234\n",
      "Epoch [3960/30000], Loss: 0.5234\n",
      "Epoch [3980/30000], Loss: 0.5234\n",
      "Epoch [4000/30000], Loss: 0.5233\n",
      "Epoch [4020/30000], Loss: 0.5233\n",
      "Epoch [4040/30000], Loss: 0.5232\n",
      "Epoch [4060/30000], Loss: 0.5232\n",
      "Epoch [4080/30000], Loss: 0.5231\n",
      "Epoch [4100/30000], Loss: 0.5231\n",
      "Epoch [4120/30000], Loss: 0.5230\n",
      "Epoch [4140/30000], Loss: 0.5230\n",
      "Epoch [4160/30000], Loss: 0.5230\n",
      "Epoch [4180/30000], Loss: 0.5229\n",
      "Epoch [4200/30000], Loss: 0.5229\n",
      "Epoch [4220/30000], Loss: 0.5228\n",
      "Epoch [4240/30000], Loss: 0.5228\n",
      "Epoch [4260/30000], Loss: 0.5228\n",
      "Epoch [4280/30000], Loss: 0.5227\n",
      "Epoch [4300/30000], Loss: 0.5227\n",
      "Epoch [4320/30000], Loss: 0.5226\n",
      "Epoch [4340/30000], Loss: 0.5226\n",
      "Epoch [4360/30000], Loss: 0.5226\n",
      "Epoch [4380/30000], Loss: 0.5225\n",
      "Epoch [4400/30000], Loss: 0.5225\n",
      "Epoch [4420/30000], Loss: 0.5224\n",
      "Epoch [4440/30000], Loss: 0.5224\n",
      "Epoch [4460/30000], Loss: 0.5224\n",
      "Epoch [4480/30000], Loss: 0.5223\n",
      "Epoch [4500/30000], Loss: 0.5223\n",
      "Epoch [4520/30000], Loss: 0.5222\n",
      "Epoch [4540/30000], Loss: 0.5222\n",
      "Epoch [4560/30000], Loss: 0.5222\n",
      "Epoch [4580/30000], Loss: 0.5221\n",
      "Epoch [4600/30000], Loss: 0.5221\n",
      "Epoch [4620/30000], Loss: 0.5221\n",
      "Epoch [4640/30000], Loss: 0.5220\n",
      "Epoch [4660/30000], Loss: 0.5220\n",
      "Epoch [4680/30000], Loss: 0.5220\n",
      "Epoch [4700/30000], Loss: 0.5219\n",
      "Epoch [4720/30000], Loss: 0.5219\n",
      "Epoch [4740/30000], Loss: 0.5219\n",
      "Epoch [4760/30000], Loss: 0.5218\n",
      "Epoch [4780/30000], Loss: 0.5218\n",
      "Epoch [4800/30000], Loss: 0.5218\n",
      "Epoch [4820/30000], Loss: 0.5217\n",
      "Epoch [4840/30000], Loss: 0.5217\n",
      "Epoch [4860/30000], Loss: 0.5217\n",
      "Epoch [4880/30000], Loss: 0.5216\n",
      "Epoch [4900/30000], Loss: 0.5216\n",
      "Epoch [4920/30000], Loss: 0.5216\n",
      "Epoch [4940/30000], Loss: 0.5215\n",
      "Epoch [4960/30000], Loss: 0.5215\n",
      "Epoch [4980/30000], Loss: 0.5215\n",
      "Epoch [5000/30000], Loss: 0.5214\n",
      "Epoch [5020/30000], Loss: 0.5214\n",
      "Epoch [5040/30000], Loss: 0.5214\n",
      "Epoch [5060/30000], Loss: 0.5213\n",
      "Epoch [5080/30000], Loss: 0.5213\n",
      "Epoch [5100/30000], Loss: 0.5213\n",
      "Epoch [5120/30000], Loss: 0.5212\n",
      "Epoch [5140/30000], Loss: 0.5212\n",
      "Epoch [5160/30000], Loss: 0.5212\n",
      "Epoch [5180/30000], Loss: 0.5212\n",
      "Epoch [5200/30000], Loss: 0.5211\n",
      "Epoch [5220/30000], Loss: 0.5211\n",
      "Epoch [5240/30000], Loss: 0.5211\n",
      "Epoch [5260/30000], Loss: 0.5210\n",
      "Epoch [5280/30000], Loss: 0.5210\n",
      "Epoch [5300/30000], Loss: 0.5210\n",
      "Epoch [5320/30000], Loss: 0.5209\n",
      "Epoch [5340/30000], Loss: 0.5209\n",
      "Epoch [5360/30000], Loss: 0.5209\n",
      "Epoch [5380/30000], Loss: 0.5209\n",
      "Epoch [5400/30000], Loss: 0.5208\n",
      "Epoch [5420/30000], Loss: 0.5208\n",
      "Epoch [5440/30000], Loss: 0.5208\n",
      "Epoch [5460/30000], Loss: 0.5208\n",
      "Epoch [5480/30000], Loss: 0.5207\n",
      "Epoch [5500/30000], Loss: 0.5207\n",
      "Epoch [5520/30000], Loss: 0.5207\n",
      "Epoch [5540/30000], Loss: 0.5206\n",
      "Epoch [5560/30000], Loss: 0.5206\n",
      "Epoch [5580/30000], Loss: 0.5206\n",
      "Epoch [5600/30000], Loss: 0.5206\n",
      "Epoch [5620/30000], Loss: 0.5205\n",
      "Epoch [5640/30000], Loss: 0.5205\n",
      "Epoch [5660/30000], Loss: 0.5205\n",
      "Epoch [5680/30000], Loss: 0.5205\n",
      "Epoch [5700/30000], Loss: 0.5204\n",
      "Epoch [5720/30000], Loss: 0.5204\n",
      "Epoch [5740/30000], Loss: 0.5204\n",
      "Epoch [5760/30000], Loss: 0.5204\n",
      "Epoch [5780/30000], Loss: 0.5203\n",
      "Epoch [5800/30000], Loss: 0.5203\n",
      "Epoch [5820/30000], Loss: 0.5203\n",
      "Epoch [5840/30000], Loss: 0.5203\n",
      "Epoch [5860/30000], Loss: 0.5202\n",
      "Epoch [5880/30000], Loss: 0.5202\n",
      "Epoch [5900/30000], Loss: 0.5202\n",
      "Epoch [5920/30000], Loss: 0.5202\n",
      "Epoch [5940/30000], Loss: 0.5201\n",
      "Epoch [5960/30000], Loss: 0.5201\n",
      "Epoch [5980/30000], Loss: 0.5201\n",
      "Epoch [6000/30000], Loss: 0.5201\n",
      "Epoch [6020/30000], Loss: 0.5200\n",
      "Epoch [6040/30000], Loss: 0.5200\n",
      "Epoch [6060/30000], Loss: 0.5200\n",
      "Epoch [6080/30000], Loss: 0.5200\n",
      "Epoch [6100/30000], Loss: 0.5200\n",
      "Epoch [6120/30000], Loss: 0.5199\n",
      "Epoch [6140/30000], Loss: 0.5199\n",
      "Epoch [6160/30000], Loss: 0.5199\n",
      "Epoch [6180/30000], Loss: 0.5199\n",
      "Epoch [6200/30000], Loss: 0.5198\n",
      "Epoch [6220/30000], Loss: 0.5198\n",
      "Epoch [6240/30000], Loss: 0.5198\n",
      "Epoch [6260/30000], Loss: 0.5198\n",
      "Epoch [6280/30000], Loss: 0.5198\n",
      "Epoch [6300/30000], Loss: 0.5197\n",
      "Epoch [6320/30000], Loss: 0.5197\n",
      "Epoch [6340/30000], Loss: 0.5197\n",
      "Epoch [6360/30000], Loss: 0.5197\n",
      "Epoch [6380/30000], Loss: 0.5196\n",
      "Epoch [6400/30000], Loss: 0.5196\n",
      "Epoch [6420/30000], Loss: 0.5196\n",
      "Epoch [6440/30000], Loss: 0.5196\n",
      "Epoch [6460/30000], Loss: 0.5196\n",
      "Epoch [6480/30000], Loss: 0.5195\n",
      "Epoch [6500/30000], Loss: 0.5195\n",
      "Epoch [6520/30000], Loss: 0.5195\n",
      "Epoch [6540/30000], Loss: 0.5195\n",
      "Epoch [6560/30000], Loss: 0.5195\n",
      "Epoch [6580/30000], Loss: 0.5194\n",
      "Epoch [6600/30000], Loss: 0.5194\n",
      "Epoch [6620/30000], Loss: 0.5194\n",
      "Epoch [6640/30000], Loss: 0.5194\n",
      "Epoch [6660/30000], Loss: 0.5194\n",
      "Epoch [6680/30000], Loss: 0.5193\n",
      "Epoch [6700/30000], Loss: 0.5193\n",
      "Epoch [6720/30000], Loss: 0.5193\n",
      "Epoch [6740/30000], Loss: 0.5193\n",
      "Epoch [6760/30000], Loss: 0.5193\n",
      "Epoch [6780/30000], Loss: 0.5192\n",
      "Epoch [6800/30000], Loss: 0.5192\n",
      "Epoch [6820/30000], Loss: 0.5192\n",
      "Epoch [6840/30000], Loss: 0.5192\n",
      "Epoch [6860/30000], Loss: 0.5192\n",
      "Epoch [6880/30000], Loss: 0.5192\n",
      "Epoch [6900/30000], Loss: 0.5191\n",
      "Epoch [6920/30000], Loss: 0.5191\n",
      "Epoch [6940/30000], Loss: 0.5191\n",
      "Epoch [6960/30000], Loss: 0.5191\n",
      "Epoch [6980/30000], Loss: 0.5191\n",
      "Epoch [7000/30000], Loss: 0.5190\n",
      "Epoch [7020/30000], Loss: 0.5190\n",
      "Epoch [7040/30000], Loss: 0.5190\n",
      "Epoch [7060/30000], Loss: 0.5190\n",
      "Epoch [7080/30000], Loss: 0.5190\n",
      "Epoch [7100/30000], Loss: 0.5190\n",
      "Epoch [7120/30000], Loss: 0.5189\n",
      "Epoch [7140/30000], Loss: 0.5189\n",
      "Epoch [7160/30000], Loss: 0.5189\n",
      "Epoch [7180/30000], Loss: 0.5189\n",
      "Epoch [7200/30000], Loss: 0.5189\n",
      "Epoch [7220/30000], Loss: 0.5188\n",
      "Epoch [7240/30000], Loss: 0.5188\n",
      "Epoch [7260/30000], Loss: 0.5188\n",
      "Epoch [7280/30000], Loss: 0.5188\n",
      "Epoch [7300/30000], Loss: 0.5188\n",
      "Epoch [7320/30000], Loss: 0.5188\n",
      "Epoch [7340/30000], Loss: 0.5187\n",
      "Epoch [7360/30000], Loss: 0.5187\n",
      "Epoch [7380/30000], Loss: 0.5187\n",
      "Epoch [7400/30000], Loss: 0.5187\n",
      "Epoch [7420/30000], Loss: 0.5187\n",
      "Epoch [7440/30000], Loss: 0.5187\n",
      "Epoch [7460/30000], Loss: 0.5186\n",
      "Epoch [7480/30000], Loss: 0.5186\n",
      "Epoch [7500/30000], Loss: 0.5186\n",
      "Epoch [7520/30000], Loss: 0.5186\n",
      "Epoch [7540/30000], Loss: 0.5186\n",
      "Epoch [7560/30000], Loss: 0.5186\n",
      "Epoch [7580/30000], Loss: 0.5185\n",
      "Epoch [7600/30000], Loss: 0.5185\n",
      "Epoch [7620/30000], Loss: 0.5185\n",
      "Epoch [7640/30000], Loss: 0.5185\n",
      "Epoch [7660/30000], Loss: 0.5185\n",
      "Epoch [7680/30000], Loss: 0.5185\n",
      "Epoch [7700/30000], Loss: 0.5185\n",
      "Epoch [7720/30000], Loss: 0.5184\n",
      "Epoch [7740/30000], Loss: 0.5184\n",
      "Epoch [7760/30000], Loss: 0.5184\n",
      "Epoch [7780/30000], Loss: 0.5184\n",
      "Epoch [7800/30000], Loss: 0.5184\n",
      "Epoch [7820/30000], Loss: 0.5184\n",
      "Epoch [7840/30000], Loss: 0.5183\n",
      "Epoch [7860/30000], Loss: 0.5183\n",
      "Epoch [7880/30000], Loss: 0.5183\n",
      "Epoch [7900/30000], Loss: 0.5183\n",
      "Epoch [7920/30000], Loss: 0.5183\n",
      "Epoch [7940/30000], Loss: 0.5183\n",
      "Epoch [7960/30000], Loss: 0.5183\n",
      "Epoch [7980/30000], Loss: 0.5182\n",
      "Epoch [8000/30000], Loss: 0.5182\n",
      "Epoch [8020/30000], Loss: 0.5182\n",
      "Epoch [8040/30000], Loss: 0.5182\n",
      "Epoch [8060/30000], Loss: 0.5182\n",
      "Epoch [8080/30000], Loss: 0.5182\n",
      "Epoch [8100/30000], Loss: 0.5182\n",
      "Epoch [8120/30000], Loss: 0.5181\n",
      "Epoch [8140/30000], Loss: 0.5181\n",
      "Epoch [8160/30000], Loss: 0.5181\n",
      "Epoch [8180/30000], Loss: 0.5181\n",
      "Epoch [8200/30000], Loss: 0.5181\n",
      "Epoch [8220/30000], Loss: 0.5181\n",
      "Epoch [8240/30000], Loss: 0.5181\n",
      "Epoch [8260/30000], Loss: 0.5180\n",
      "Epoch [8280/30000], Loss: 0.5180\n",
      "Epoch [8300/30000], Loss: 0.5180\n",
      "Epoch [8320/30000], Loss: 0.5180\n",
      "Epoch [8340/30000], Loss: 0.5180\n",
      "Epoch [8360/30000], Loss: 0.5180\n",
      "Epoch [8380/30000], Loss: 0.5180\n",
      "Epoch [8400/30000], Loss: 0.5179\n",
      "Epoch [8420/30000], Loss: 0.5179\n",
      "Epoch [8440/30000], Loss: 0.5179\n",
      "Epoch [8460/30000], Loss: 0.5179\n",
      "Epoch [8480/30000], Loss: 0.5179\n",
      "Epoch [8500/30000], Loss: 0.5179\n",
      "Epoch [8520/30000], Loss: 0.5179\n",
      "Epoch [8540/30000], Loss: 0.5179\n",
      "Epoch [8560/30000], Loss: 0.5178\n",
      "Epoch [8580/30000], Loss: 0.5178\n",
      "Epoch [8600/30000], Loss: 0.5178\n",
      "Epoch [8620/30000], Loss: 0.5178\n",
      "Epoch [8640/30000], Loss: 0.5178\n",
      "Epoch [8660/30000], Loss: 0.5178\n",
      "Epoch [8680/30000], Loss: 0.5178\n",
      "Epoch [8700/30000], Loss: 0.5178\n",
      "Epoch [8720/30000], Loss: 0.5177\n",
      "Epoch [8740/30000], Loss: 0.5177\n",
      "Epoch [8760/30000], Loss: 0.5177\n",
      "Epoch [8780/30000], Loss: 0.5177\n",
      "Epoch [8800/30000], Loss: 0.5177\n",
      "Epoch [8820/30000], Loss: 0.5177\n",
      "Epoch [8840/30000], Loss: 0.5177\n",
      "Epoch [8860/30000], Loss: 0.5177\n",
      "Epoch [8880/30000], Loss: 0.5176\n",
      "Epoch [8900/30000], Loss: 0.5176\n",
      "Epoch [8920/30000], Loss: 0.5176\n",
      "Epoch [8940/30000], Loss: 0.5176\n",
      "Epoch [8960/30000], Loss: 0.5176\n",
      "Epoch [8980/30000], Loss: 0.5176\n",
      "Epoch [9000/30000], Loss: 0.5176\n",
      "Epoch [9020/30000], Loss: 0.5176\n",
      "Epoch [9040/30000], Loss: 0.5175\n",
      "Epoch [9060/30000], Loss: 0.5175\n",
      "Epoch [9080/30000], Loss: 0.5175\n",
      "Epoch [9100/30000], Loss: 0.5175\n",
      "Epoch [9120/30000], Loss: 0.5175\n",
      "Epoch [9140/30000], Loss: 0.5175\n",
      "Epoch [9160/30000], Loss: 0.5175\n",
      "Epoch [9180/30000], Loss: 0.5175\n",
      "Epoch [9200/30000], Loss: 0.5174\n",
      "Epoch [9220/30000], Loss: 0.5174\n",
      "Epoch [9240/30000], Loss: 0.5174\n",
      "Epoch [9260/30000], Loss: 0.5174\n",
      "Epoch [9280/30000], Loss: 0.5174\n",
      "Epoch [9300/30000], Loss: 0.5174\n",
      "Epoch [9320/30000], Loss: 0.5174\n",
      "Epoch [9340/30000], Loss: 0.5174\n",
      "Epoch [9360/30000], Loss: 0.5174\n",
      "Epoch [9380/30000], Loss: 0.5173\n",
      "Epoch [9400/30000], Loss: 0.5173\n",
      "Epoch [9420/30000], Loss: 0.5173\n",
      "Epoch [9440/30000], Loss: 0.5173\n",
      "Epoch [9460/30000], Loss: 0.5173\n",
      "Epoch [9480/30000], Loss: 0.5173\n",
      "Epoch [9500/30000], Loss: 0.5173\n",
      "Epoch [9520/30000], Loss: 0.5173\n",
      "Epoch [9540/30000], Loss: 0.5173\n",
      "Epoch [9560/30000], Loss: 0.5172\n",
      "Epoch [9580/30000], Loss: 0.5172\n",
      "Epoch [9600/30000], Loss: 0.5172\n",
      "Epoch [9620/30000], Loss: 0.5172\n",
      "Epoch [9640/30000], Loss: 0.5172\n",
      "Epoch [9660/30000], Loss: 0.5172\n",
      "Epoch [9680/30000], Loss: 0.5172\n",
      "Epoch [9700/30000], Loss: 0.5172\n",
      "Epoch [9720/30000], Loss: 0.5172\n",
      "Epoch [9740/30000], Loss: 0.5172\n",
      "Epoch [9760/30000], Loss: 0.5171\n",
      "Epoch [9780/30000], Loss: 0.5171\n",
      "Epoch [9800/30000], Loss: 0.5171\n",
      "Epoch [9820/30000], Loss: 0.5171\n",
      "Epoch [9840/30000], Loss: 0.5171\n",
      "Epoch [9860/30000], Loss: 0.5171\n",
      "Epoch [9880/30000], Loss: 0.5171\n",
      "Epoch [9900/30000], Loss: 0.5171\n",
      "Epoch [9920/30000], Loss: 0.5171\n",
      "Epoch [9940/30000], Loss: 0.5170\n",
      "Epoch [9960/30000], Loss: 0.5170\n",
      "Epoch [9980/30000], Loss: 0.5170\n",
      "Epoch [10000/30000], Loss: 0.5170\n",
      "Epoch [10020/30000], Loss: 0.5170\n",
      "Epoch [10040/30000], Loss: 0.5170\n",
      "Epoch [10060/30000], Loss: 0.5170\n",
      "Epoch [10080/30000], Loss: 0.5170\n",
      "Epoch [10100/30000], Loss: 0.5170\n",
      "Epoch [10120/30000], Loss: 0.5170\n",
      "Epoch [10140/30000], Loss: 0.5169\n",
      "Epoch [10160/30000], Loss: 0.5169\n",
      "Epoch [10180/30000], Loss: 0.5169\n",
      "Epoch [10200/30000], Loss: 0.5169\n",
      "Epoch [10220/30000], Loss: 0.5169\n",
      "Epoch [10240/30000], Loss: 0.5169\n",
      "Epoch [10260/30000], Loss: 0.5169\n",
      "Epoch [10280/30000], Loss: 0.5169\n",
      "Epoch [10300/30000], Loss: 0.5169\n",
      "Epoch [10320/30000], Loss: 0.5169\n",
      "Epoch [10340/30000], Loss: 0.5169\n",
      "Epoch [10360/30000], Loss: 0.5168\n",
      "Epoch [10380/30000], Loss: 0.5168\n",
      "Epoch [10400/30000], Loss: 0.5168\n",
      "Epoch [10420/30000], Loss: 0.5168\n",
      "Epoch [10440/30000], Loss: 0.5168\n",
      "Epoch [10460/30000], Loss: 0.5168\n",
      "Epoch [10480/30000], Loss: 0.5168\n",
      "Epoch [10500/30000], Loss: 0.5168\n",
      "Epoch [10520/30000], Loss: 0.5168\n",
      "Epoch [10540/30000], Loss: 0.5168\n",
      "Epoch [10560/30000], Loss: 0.5168\n",
      "Epoch [10580/30000], Loss: 0.5167\n",
      "Epoch [10600/30000], Loss: 0.5167\n",
      "Epoch [10620/30000], Loss: 0.5167\n",
      "Epoch [10640/30000], Loss: 0.5167\n",
      "Epoch [10660/30000], Loss: 0.5167\n",
      "Epoch [10680/30000], Loss: 0.5167\n",
      "Epoch [10700/30000], Loss: 0.5167\n",
      "Epoch [10720/30000], Loss: 0.5167\n",
      "Epoch [10740/30000], Loss: 0.5167\n",
      "Epoch [10760/30000], Loss: 0.5167\n",
      "Epoch [10780/30000], Loss: 0.5167\n",
      "Epoch [10800/30000], Loss: 0.5166\n",
      "Epoch [10820/30000], Loss: 0.5166\n",
      "Epoch [10840/30000], Loss: 0.5166\n",
      "Epoch [10860/30000], Loss: 0.5166\n",
      "Epoch [10880/30000], Loss: 0.5166\n",
      "Epoch [10900/30000], Loss: 0.5166\n",
      "Epoch [10920/30000], Loss: 0.5166\n",
      "Epoch [10940/30000], Loss: 0.5166\n",
      "Epoch [10960/30000], Loss: 0.5166\n",
      "Epoch [10980/30000], Loss: 0.5166\n",
      "Epoch [11000/30000], Loss: 0.5166\n",
      "Epoch [11020/30000], Loss: 0.5165\n",
      "Epoch [11040/30000], Loss: 0.5165\n",
      "Epoch [11060/30000], Loss: 0.5165\n",
      "Epoch [11080/30000], Loss: 0.5165\n",
      "Epoch [11100/30000], Loss: 0.5165\n",
      "Epoch [11120/30000], Loss: 0.5165\n",
      "Epoch [11140/30000], Loss: 0.5165\n",
      "Epoch [11160/30000], Loss: 0.5165\n",
      "Epoch [11180/30000], Loss: 0.5165\n",
      "Epoch [11200/30000], Loss: 0.5165\n",
      "Epoch [11220/30000], Loss: 0.5165\n",
      "Epoch [11240/30000], Loss: 0.5165\n",
      "Epoch [11260/30000], Loss: 0.5164\n",
      "Epoch [11280/30000], Loss: 0.5164\n",
      "Epoch [11300/30000], Loss: 0.5164\n",
      "Epoch [11320/30000], Loss: 0.5164\n",
      "Epoch [11340/30000], Loss: 0.5164\n",
      "Epoch [11360/30000], Loss: 0.5164\n",
      "Epoch [11380/30000], Loss: 0.5164\n",
      "Epoch [11400/30000], Loss: 0.5164\n",
      "Epoch [11420/30000], Loss: 0.5164\n",
      "Epoch [11440/30000], Loss: 0.5164\n",
      "Epoch [11460/30000], Loss: 0.5164\n",
      "Epoch [11480/30000], Loss: 0.5164\n",
      "Epoch [11500/30000], Loss: 0.5164\n",
      "Epoch [11520/30000], Loss: 0.5163\n",
      "Epoch [11540/30000], Loss: 0.5163\n",
      "Epoch [11560/30000], Loss: 0.5163\n",
      "Epoch [11580/30000], Loss: 0.5163\n",
      "Epoch [11600/30000], Loss: 0.5163\n",
      "Epoch [11620/30000], Loss: 0.5163\n",
      "Epoch [11640/30000], Loss: 0.5163\n",
      "Epoch [11660/30000], Loss: 0.5163\n",
      "Epoch [11680/30000], Loss: 0.5163\n",
      "Epoch [11700/30000], Loss: 0.5163\n",
      "Epoch [11720/30000], Loss: 0.5163\n",
      "Epoch [11740/30000], Loss: 0.5163\n",
      "Epoch [11760/30000], Loss: 0.5163\n",
      "Epoch [11780/30000], Loss: 0.5162\n",
      "Epoch [11800/30000], Loss: 0.5162\n",
      "Epoch [11820/30000], Loss: 0.5162\n",
      "Epoch [11840/30000], Loss: 0.5162\n",
      "Epoch [11860/30000], Loss: 0.5162\n",
      "Epoch [11880/30000], Loss: 0.5162\n",
      "Epoch [11900/30000], Loss: 0.5162\n",
      "Epoch [11920/30000], Loss: 0.5162\n",
      "Epoch [11940/30000], Loss: 0.5162\n",
      "Epoch [11960/30000], Loss: 0.5162\n",
      "Epoch [11980/30000], Loss: 0.5162\n",
      "Epoch [12000/30000], Loss: 0.5162\n",
      "Epoch [12020/30000], Loss: 0.5162\n",
      "Epoch [12040/30000], Loss: 0.5161\n",
      "Epoch [12060/30000], Loss: 0.5161\n",
      "Epoch [12080/30000], Loss: 0.5161\n",
      "Epoch [12100/30000], Loss: 0.5161\n",
      "Epoch [12120/30000], Loss: 0.5161\n",
      "Epoch [12140/30000], Loss: 0.5161\n",
      "Epoch [12160/30000], Loss: 0.5161\n",
      "Epoch [12180/30000], Loss: 0.5161\n",
      "Epoch [12200/30000], Loss: 0.5161\n",
      "Epoch [12220/30000], Loss: 0.5161\n",
      "Epoch [12240/30000], Loss: 0.5161\n",
      "Epoch [12260/30000], Loss: 0.5161\n",
      "Epoch [12280/30000], Loss: 0.5161\n",
      "Epoch [12300/30000], Loss: 0.5161\n",
      "Epoch [12320/30000], Loss: 0.5160\n",
      "Epoch [12340/30000], Loss: 0.5160\n",
      "Epoch [12360/30000], Loss: 0.5160\n",
      "Epoch [12380/30000], Loss: 0.5160\n",
      "Epoch [12400/30000], Loss: 0.5160\n",
      "Epoch [12420/30000], Loss: 0.5160\n",
      "Epoch [12440/30000], Loss: 0.5160\n",
      "Epoch [12460/30000], Loss: 0.5160\n",
      "Epoch [12480/30000], Loss: 0.5160\n",
      "Epoch [12500/30000], Loss: 0.5160\n",
      "Epoch [12520/30000], Loss: 0.5160\n",
      "Epoch [12540/30000], Loss: 0.5160\n",
      "Epoch [12560/30000], Loss: 0.5160\n",
      "Epoch [12580/30000], Loss: 0.5160\n",
      "Epoch [12600/30000], Loss: 0.5160\n",
      "Epoch [12620/30000], Loss: 0.5159\n",
      "Epoch [12640/30000], Loss: 0.5159\n",
      "Epoch [12660/30000], Loss: 0.5159\n",
      "Epoch [12680/30000], Loss: 0.5159\n",
      "Epoch [12700/30000], Loss: 0.5159\n",
      "Epoch [12720/30000], Loss: 0.5159\n",
      "Epoch [12740/30000], Loss: 0.5159\n",
      "Epoch [12760/30000], Loss: 0.5159\n",
      "Epoch [12780/30000], Loss: 0.5159\n",
      "Epoch [12800/30000], Loss: 0.5159\n",
      "Epoch [12820/30000], Loss: 0.5159\n",
      "Epoch [12840/30000], Loss: 0.5159\n",
      "Epoch [12860/30000], Loss: 0.5159\n",
      "Epoch [12880/30000], Loss: 0.5159\n",
      "Epoch [12900/30000], Loss: 0.5159\n",
      "Epoch [12920/30000], Loss: 0.5158\n",
      "Epoch [12940/30000], Loss: 0.5158\n",
      "Epoch [12960/30000], Loss: 0.5158\n",
      "Epoch [12980/30000], Loss: 0.5158\n",
      "Epoch [13000/30000], Loss: 0.5158\n",
      "Epoch [13020/30000], Loss: 0.5158\n",
      "Epoch [13040/30000], Loss: 0.5158\n",
      "Epoch [13060/30000], Loss: 0.5158\n",
      "Epoch [13080/30000], Loss: 0.5158\n",
      "Epoch [13100/30000], Loss: 0.5158\n",
      "Epoch [13120/30000], Loss: 0.5158\n",
      "Epoch [13140/30000], Loss: 0.5158\n",
      "Epoch [13160/30000], Loss: 0.5158\n",
      "Epoch [13180/30000], Loss: 0.5158\n",
      "Epoch [13200/30000], Loss: 0.5158\n",
      "Epoch [13220/30000], Loss: 0.5158\n",
      "Epoch [13240/30000], Loss: 0.5157\n",
      "Epoch [13260/30000], Loss: 0.5157\n",
      "Epoch [13280/30000], Loss: 0.5157\n",
      "Epoch [13300/30000], Loss: 0.5157\n",
      "Epoch [13320/30000], Loss: 0.5157\n",
      "Epoch [13340/30000], Loss: 0.5157\n",
      "Epoch [13360/30000], Loss: 0.5157\n",
      "Epoch [13380/30000], Loss: 0.5157\n",
      "Epoch [13400/30000], Loss: 0.5157\n",
      "Epoch [13420/30000], Loss: 0.5157\n",
      "Epoch [13440/30000], Loss: 0.5157\n",
      "Epoch [13460/30000], Loss: 0.5157\n",
      "Epoch [13480/30000], Loss: 0.5157\n",
      "Epoch [13500/30000], Loss: 0.5157\n",
      "Epoch [13520/30000], Loss: 0.5157\n",
      "Epoch [13540/30000], Loss: 0.5157\n",
      "Epoch [13560/30000], Loss: 0.5156\n",
      "Epoch [13580/30000], Loss: 0.5156\n",
      "Epoch [13600/30000], Loss: 0.5156\n",
      "Epoch [13620/30000], Loss: 0.5156\n",
      "Epoch [13640/30000], Loss: 0.5156\n",
      "Epoch [13660/30000], Loss: 0.5156\n",
      "Epoch [13680/30000], Loss: 0.5156\n",
      "Epoch [13700/30000], Loss: 0.5156\n",
      "Epoch [13720/30000], Loss: 0.5156\n",
      "Epoch [13740/30000], Loss: 0.5156\n",
      "Epoch [13760/30000], Loss: 0.5156\n",
      "Epoch [13780/30000], Loss: 0.5156\n",
      "Epoch [13800/30000], Loss: 0.5156\n",
      "Epoch [13820/30000], Loss: 0.5156\n",
      "Epoch [13840/30000], Loss: 0.5156\n",
      "Epoch [13860/30000], Loss: 0.5156\n",
      "Epoch [13880/30000], Loss: 0.5156\n",
      "Epoch [13900/30000], Loss: 0.5155\n",
      "Epoch [13920/30000], Loss: 0.5155\n",
      "Epoch [13940/30000], Loss: 0.5155\n",
      "Epoch [13960/30000], Loss: 0.5155\n",
      "Epoch [13980/30000], Loss: 0.5155\n",
      "Epoch [14000/30000], Loss: 0.5155\n",
      "Epoch [14020/30000], Loss: 0.5155\n",
      "Epoch [14040/30000], Loss: 0.5155\n",
      "Epoch [14060/30000], Loss: 0.5155\n",
      "Epoch [14080/30000], Loss: 0.5155\n",
      "Epoch [14100/30000], Loss: 0.5155\n",
      "Epoch [14120/30000], Loss: 0.5155\n",
      "Epoch [14140/30000], Loss: 0.5155\n",
      "Epoch [14160/30000], Loss: 0.5155\n",
      "Epoch [14180/30000], Loss: 0.5155\n",
      "Epoch [14200/30000], Loss: 0.5155\n",
      "Epoch [14220/30000], Loss: 0.5155\n",
      "Epoch [14240/30000], Loss: 0.5155\n",
      "Epoch [14260/30000], Loss: 0.5154\n",
      "Epoch [14280/30000], Loss: 0.5154\n",
      "Epoch [14300/30000], Loss: 0.5154\n",
      "Epoch [14320/30000], Loss: 0.5154\n",
      "Epoch [14340/30000], Loss: 0.5154\n",
      "Epoch [14360/30000], Loss: 0.5154\n",
      "Epoch [14380/30000], Loss: 0.5154\n",
      "Epoch [14400/30000], Loss: 0.5154\n",
      "Epoch [14420/30000], Loss: 0.5154\n",
      "Epoch [14440/30000], Loss: 0.5154\n",
      "Epoch [14460/30000], Loss: 0.5154\n",
      "Epoch [14480/30000], Loss: 0.5154\n",
      "Epoch [14500/30000], Loss: 0.5154\n",
      "Epoch [14520/30000], Loss: 0.5154\n",
      "Epoch [14540/30000], Loss: 0.5154\n",
      "Epoch [14560/30000], Loss: 0.5154\n",
      "Epoch [14580/30000], Loss: 0.5154\n",
      "Epoch [14600/30000], Loss: 0.5154\n",
      "Epoch [14620/30000], Loss: 0.5154\n",
      "Epoch [14640/30000], Loss: 0.5153\n",
      "Epoch [14660/30000], Loss: 0.5153\n",
      "Epoch [14680/30000], Loss: 0.5153\n",
      "Epoch [14700/30000], Loss: 0.5153\n",
      "Epoch [14720/30000], Loss: 0.5153\n",
      "Epoch [14740/30000], Loss: 0.5153\n",
      "Epoch [14760/30000], Loss: 0.5153\n",
      "Epoch [14780/30000], Loss: 0.5153\n",
      "Epoch [14800/30000], Loss: 0.5153\n",
      "Epoch [14820/30000], Loss: 0.5153\n",
      "Epoch [14840/30000], Loss: 0.5153\n",
      "Epoch [14860/30000], Loss: 0.5153\n",
      "Epoch [14880/30000], Loss: 0.5153\n",
      "Epoch [14900/30000], Loss: 0.5153\n",
      "Epoch [14920/30000], Loss: 0.5153\n",
      "Epoch [14940/30000], Loss: 0.5153\n",
      "Epoch [14960/30000], Loss: 0.5153\n",
      "Epoch [14980/30000], Loss: 0.5153\n",
      "Epoch [15000/30000], Loss: 0.5153\n",
      "Epoch [15020/30000], Loss: 0.5153\n",
      "Epoch [15040/30000], Loss: 0.5152\n",
      "Epoch [15060/30000], Loss: 0.5152\n",
      "Epoch [15080/30000], Loss: 0.5152\n",
      "Epoch [15100/30000], Loss: 0.5152\n",
      "Epoch [15120/30000], Loss: 0.5152\n",
      "Epoch [15140/30000], Loss: 0.5152\n",
      "Epoch [15160/30000], Loss: 0.5152\n",
      "Epoch [15180/30000], Loss: 0.5152\n",
      "Epoch [15200/30000], Loss: 0.5152\n",
      "Epoch [15220/30000], Loss: 0.5152\n",
      "Epoch [15240/30000], Loss: 0.5152\n",
      "Epoch [15260/30000], Loss: 0.5152\n",
      "Epoch [15280/30000], Loss: 0.5152\n",
      "Epoch [15300/30000], Loss: 0.5152\n",
      "Epoch [15320/30000], Loss: 0.5152\n",
      "Epoch [15340/30000], Loss: 0.5152\n",
      "Epoch [15360/30000], Loss: 0.5152\n",
      "Epoch [15380/30000], Loss: 0.5152\n",
      "Epoch [15400/30000], Loss: 0.5152\n",
      "Epoch [15420/30000], Loss: 0.5152\n",
      "Epoch [15440/30000], Loss: 0.5151\n",
      "Epoch [15460/30000], Loss: 0.5151\n",
      "Epoch [15480/30000], Loss: 0.5151\n",
      "Epoch [15500/30000], Loss: 0.5151\n",
      "Epoch [15520/30000], Loss: 0.5151\n",
      "Epoch [15540/30000], Loss: 0.5151\n",
      "Epoch [15560/30000], Loss: 0.5151\n",
      "Epoch [15580/30000], Loss: 0.5151\n",
      "Epoch [15600/30000], Loss: 0.5151\n",
      "Epoch [15620/30000], Loss: 0.5151\n",
      "Epoch [15640/30000], Loss: 0.5151\n",
      "Epoch [15660/30000], Loss: 0.5151\n",
      "Epoch [15680/30000], Loss: 0.5151\n",
      "Epoch [15700/30000], Loss: 0.5151\n",
      "Epoch [15720/30000], Loss: 0.5151\n",
      "Epoch [15740/30000], Loss: 0.5151\n",
      "Epoch [15760/30000], Loss: 0.5151\n",
      "Epoch [15780/30000], Loss: 0.5151\n",
      "Epoch [15800/30000], Loss: 0.5151\n",
      "Epoch [15820/30000], Loss: 0.5151\n",
      "Epoch [15840/30000], Loss: 0.5151\n",
      "Epoch [15860/30000], Loss: 0.5151\n",
      "Epoch [15880/30000], Loss: 0.5150\n",
      "Epoch [15900/30000], Loss: 0.5150\n",
      "Epoch [15920/30000], Loss: 0.5150\n",
      "Epoch [15940/30000], Loss: 0.5150\n",
      "Epoch [15960/30000], Loss: 0.5150\n",
      "Epoch [15980/30000], Loss: 0.5150\n",
      "Epoch [16000/30000], Loss: 0.5150\n",
      "Epoch [16020/30000], Loss: 0.5150\n",
      "Epoch [16040/30000], Loss: 0.5150\n",
      "Epoch [16060/30000], Loss: 0.5150\n",
      "Epoch [16080/30000], Loss: 0.5150\n",
      "Epoch [16100/30000], Loss: 0.5150\n",
      "Epoch [16120/30000], Loss: 0.5150\n",
      "Epoch [16140/30000], Loss: 0.5150\n",
      "Epoch [16160/30000], Loss: 0.5150\n",
      "Epoch [16180/30000], Loss: 0.5150\n",
      "Epoch [16200/30000], Loss: 0.5150\n",
      "Epoch [16220/30000], Loss: 0.5150\n",
      "Epoch [16240/30000], Loss: 0.5150\n",
      "Epoch [16260/30000], Loss: 0.5150\n",
      "Epoch [16280/30000], Loss: 0.5150\n",
      "Epoch [16300/30000], Loss: 0.5150\n",
      "Epoch [16320/30000], Loss: 0.5150\n",
      "Epoch [16340/30000], Loss: 0.5149\n",
      "Epoch [16360/30000], Loss: 0.5149\n",
      "Epoch [16380/30000], Loss: 0.5149\n",
      "Epoch [16400/30000], Loss: 0.5149\n",
      "Epoch [16420/30000], Loss: 0.5149\n",
      "Epoch [16440/30000], Loss: 0.5149\n",
      "Epoch [16460/30000], Loss: 0.5149\n",
      "Epoch [16480/30000], Loss: 0.5149\n",
      "Epoch [16500/30000], Loss: 0.5149\n",
      "Epoch [16520/30000], Loss: 0.5149\n",
      "Epoch [16540/30000], Loss: 0.5149\n",
      "Epoch [16560/30000], Loss: 0.5149\n",
      "Epoch [16580/30000], Loss: 0.5149\n",
      "Epoch [16600/30000], Loss: 0.5149\n",
      "Epoch [16620/30000], Loss: 0.5149\n",
      "Epoch [16640/30000], Loss: 0.5149\n",
      "Epoch [16660/30000], Loss: 0.5149\n",
      "Epoch [16680/30000], Loss: 0.5149\n",
      "Epoch [16700/30000], Loss: 0.5149\n",
      "Epoch [16720/30000], Loss: 0.5149\n",
      "Epoch [16740/30000], Loss: 0.5149\n",
      "Epoch [16760/30000], Loss: 0.5149\n",
      "Epoch [16780/30000], Loss: 0.5149\n",
      "Epoch [16800/30000], Loss: 0.5148\n",
      "Epoch [16820/30000], Loss: 0.5148\n",
      "Epoch [16840/30000], Loss: 0.5148\n",
      "Epoch [16860/30000], Loss: 0.5148\n",
      "Epoch [16880/30000], Loss: 0.5148\n",
      "Epoch [16900/30000], Loss: 0.5148\n",
      "Epoch [16920/30000], Loss: 0.5148\n",
      "Epoch [16940/30000], Loss: 0.5148\n",
      "Epoch [16960/30000], Loss: 0.5148\n",
      "Epoch [16980/30000], Loss: 0.5148\n",
      "Epoch [17000/30000], Loss: 0.5148\n",
      "Epoch [17020/30000], Loss: 0.5148\n",
      "Epoch [17040/30000], Loss: 0.5148\n",
      "Epoch [17060/30000], Loss: 0.5148\n",
      "Epoch [17080/30000], Loss: 0.5148\n",
      "Epoch [17100/30000], Loss: 0.5148\n",
      "Epoch [17120/30000], Loss: 0.5148\n",
      "Epoch [17140/30000], Loss: 0.5148\n",
      "Epoch [17160/30000], Loss: 0.5148\n",
      "Epoch [17180/30000], Loss: 0.5148\n",
      "Epoch [17200/30000], Loss: 0.5148\n",
      "Epoch [17220/30000], Loss: 0.5148\n",
      "Epoch [17240/30000], Loss: 0.5148\n",
      "Epoch [17260/30000], Loss: 0.5148\n",
      "Epoch [17280/30000], Loss: 0.5148\n",
      "Epoch [17300/30000], Loss: 0.5147\n",
      "Epoch [17320/30000], Loss: 0.5147\n",
      "Epoch [17340/30000], Loss: 0.5147\n",
      "Epoch [17360/30000], Loss: 0.5147\n",
      "Epoch [17380/30000], Loss: 0.5147\n",
      "Epoch [17400/30000], Loss: 0.5147\n",
      "Epoch [17420/30000], Loss: 0.5147\n",
      "Epoch [17440/30000], Loss: 0.5147\n",
      "Epoch [17460/30000], Loss: 0.5147\n",
      "Epoch [17480/30000], Loss: 0.5147\n",
      "Epoch [17500/30000], Loss: 0.5147\n",
      "Epoch [17520/30000], Loss: 0.5147\n",
      "Epoch [17540/30000], Loss: 0.5147\n",
      "Epoch [17560/30000], Loss: 0.5147\n",
      "Epoch [17580/30000], Loss: 0.5147\n",
      "Epoch [17600/30000], Loss: 0.5147\n",
      "Epoch [17620/30000], Loss: 0.5147\n",
      "Epoch [17640/30000], Loss: 0.5147\n",
      "Epoch [17660/30000], Loss: 0.5147\n",
      "Epoch [17680/30000], Loss: 0.5147\n",
      "Epoch [17700/30000], Loss: 0.5147\n",
      "Epoch [17720/30000], Loss: 0.5147\n",
      "Epoch [17740/30000], Loss: 0.5147\n",
      "Epoch [17760/30000], Loss: 0.5147\n",
      "Epoch [17780/30000], Loss: 0.5147\n",
      "Epoch [17800/30000], Loss: 0.5147\n",
      "Epoch [17820/30000], Loss: 0.5147\n",
      "Epoch [17840/30000], Loss: 0.5146\n",
      "Epoch [17860/30000], Loss: 0.5146\n",
      "Epoch [17880/30000], Loss: 0.5146\n",
      "Epoch [17900/30000], Loss: 0.5146\n",
      "Epoch [17920/30000], Loss: 0.5146\n",
      "Epoch [17940/30000], Loss: 0.5146\n",
      "Epoch [17960/30000], Loss: 0.5146\n",
      "Epoch [17980/30000], Loss: 0.5146\n",
      "Epoch [18000/30000], Loss: 0.5146\n",
      "Epoch [18020/30000], Loss: 0.5146\n",
      "Epoch [18040/30000], Loss: 0.5146\n",
      "Epoch [18060/30000], Loss: 0.5146\n",
      "Epoch [18080/30000], Loss: 0.5146\n",
      "Epoch [18100/30000], Loss: 0.5146\n",
      "Epoch [18120/30000], Loss: 0.5146\n",
      "Epoch [18140/30000], Loss: 0.5146\n",
      "Epoch [18160/30000], Loss: 0.5146\n",
      "Epoch [18180/30000], Loss: 0.5146\n",
      "Epoch [18200/30000], Loss: 0.5146\n",
      "Epoch [18220/30000], Loss: 0.5146\n",
      "Epoch [18240/30000], Loss: 0.5146\n",
      "Epoch [18260/30000], Loss: 0.5146\n",
      "Epoch [18280/30000], Loss: 0.5146\n",
      "Epoch [18300/30000], Loss: 0.5146\n",
      "Epoch [18320/30000], Loss: 0.5146\n",
      "Epoch [18340/30000], Loss: 0.5146\n",
      "Epoch [18360/30000], Loss: 0.5146\n",
      "Epoch [18380/30000], Loss: 0.5146\n",
      "Epoch [18400/30000], Loss: 0.5145\n",
      "Epoch [18420/30000], Loss: 0.5145\n",
      "Epoch [18440/30000], Loss: 0.5145\n",
      "Epoch [18460/30000], Loss: 0.5145\n",
      "Epoch [18480/30000], Loss: 0.5145\n",
      "Epoch [18500/30000], Loss: 0.5145\n",
      "Epoch [18520/30000], Loss: 0.5145\n",
      "Epoch [18540/30000], Loss: 0.5145\n",
      "Epoch [18560/30000], Loss: 0.5145\n",
      "Epoch [18580/30000], Loss: 0.5145\n",
      "Epoch [18600/30000], Loss: 0.5145\n",
      "Epoch [18620/30000], Loss: 0.5145\n",
      "Epoch [18640/30000], Loss: 0.5145\n",
      "Epoch [18660/30000], Loss: 0.5145\n",
      "Epoch [18680/30000], Loss: 0.5145\n",
      "Epoch [18700/30000], Loss: 0.5145\n",
      "Epoch [18720/30000], Loss: 0.5145\n",
      "Epoch [18740/30000], Loss: 0.5145\n",
      "Epoch [18760/30000], Loss: 0.5145\n",
      "Epoch [18780/30000], Loss: 0.5145\n",
      "Epoch [18800/30000], Loss: 0.5145\n",
      "Epoch [18820/30000], Loss: 0.5145\n",
      "Epoch [18840/30000], Loss: 0.5145\n",
      "Epoch [18860/30000], Loss: 0.5145\n",
      "Epoch [18880/30000], Loss: 0.5145\n",
      "Epoch [18900/30000], Loss: 0.5145\n",
      "Epoch [18920/30000], Loss: 0.5145\n",
      "Epoch [18940/30000], Loss: 0.5145\n",
      "Epoch [18960/30000], Loss: 0.5145\n",
      "Epoch [18980/30000], Loss: 0.5144\n",
      "Epoch [19000/30000], Loss: 0.5144\n",
      "Epoch [19020/30000], Loss: 0.5144\n",
      "Epoch [19040/30000], Loss: 0.5144\n",
      "Epoch [19060/30000], Loss: 0.5144\n",
      "Epoch [19080/30000], Loss: 0.5144\n",
      "Epoch [19100/30000], Loss: 0.5144\n",
      "Epoch [19120/30000], Loss: 0.5144\n",
      "Epoch [19140/30000], Loss: 0.5144\n",
      "Epoch [19160/30000], Loss: 0.5144\n",
      "Epoch [19180/30000], Loss: 0.5144\n",
      "Epoch [19200/30000], Loss: 0.5144\n",
      "Epoch [19220/30000], Loss: 0.5144\n",
      "Epoch [19240/30000], Loss: 0.5144\n",
      "Epoch [19260/30000], Loss: 0.5144\n",
      "Epoch [19280/30000], Loss: 0.5144\n",
      "Epoch [19300/30000], Loss: 0.5144\n",
      "Epoch [19320/30000], Loss: 0.5144\n",
      "Epoch [19340/30000], Loss: 0.5144\n",
      "Epoch [19360/30000], Loss: 0.5144\n",
      "Epoch [19380/30000], Loss: 0.5144\n",
      "Epoch [19400/30000], Loss: 0.5144\n",
      "Epoch [19420/30000], Loss: 0.5144\n",
      "Epoch [19440/30000], Loss: 0.5144\n",
      "Epoch [19460/30000], Loss: 0.5144\n",
      "Epoch [19480/30000], Loss: 0.5144\n",
      "Epoch [19500/30000], Loss: 0.5144\n",
      "Epoch [19520/30000], Loss: 0.5144\n",
      "Epoch [19540/30000], Loss: 0.5144\n",
      "Epoch [19560/30000], Loss: 0.5144\n",
      "Epoch [19580/30000], Loss: 0.5144\n",
      "Epoch [19600/30000], Loss: 0.5143\n",
      "Epoch [19620/30000], Loss: 0.5143\n",
      "Epoch [19640/30000], Loss: 0.5143\n",
      "Epoch [19660/30000], Loss: 0.5143\n",
      "Epoch [19680/30000], Loss: 0.5143\n",
      "Epoch [19700/30000], Loss: 0.5143\n",
      "Epoch [19720/30000], Loss: 0.5143\n",
      "Epoch [19740/30000], Loss: 0.5143\n",
      "Epoch [19760/30000], Loss: 0.5143\n",
      "Epoch [19780/30000], Loss: 0.5143\n",
      "Epoch [19800/30000], Loss: 0.5143\n",
      "Epoch [19820/30000], Loss: 0.5143\n",
      "Epoch [19840/30000], Loss: 0.5143\n",
      "Epoch [19860/30000], Loss: 0.5143\n",
      "Epoch [19880/30000], Loss: 0.5143\n",
      "Epoch [19900/30000], Loss: 0.5143\n",
      "Epoch [19920/30000], Loss: 0.5143\n",
      "Epoch [19940/30000], Loss: 0.5143\n",
      "Epoch [19960/30000], Loss: 0.5143\n",
      "Epoch [19980/30000], Loss: 0.5143\n",
      "Epoch [20000/30000], Loss: 0.5143\n",
      "Epoch [20020/30000], Loss: 0.5143\n",
      "Epoch [20040/30000], Loss: 0.5143\n",
      "Epoch [20060/30000], Loss: 0.5143\n",
      "Epoch [20080/30000], Loss: 0.5143\n",
      "Epoch [20100/30000], Loss: 0.5143\n",
      "Epoch [20120/30000], Loss: 0.5143\n",
      "Epoch [20140/30000], Loss: 0.5143\n",
      "Epoch [20160/30000], Loss: 0.5143\n",
      "Epoch [20180/30000], Loss: 0.5143\n",
      "Epoch [20200/30000], Loss: 0.5143\n",
      "Epoch [20220/30000], Loss: 0.5143\n",
      "Epoch [20240/30000], Loss: 0.5143\n",
      "Epoch [20260/30000], Loss: 0.5142\n",
      "Epoch [20280/30000], Loss: 0.5142\n",
      "Epoch [20300/30000], Loss: 0.5142\n",
      "Epoch [20320/30000], Loss: 0.5142\n",
      "Epoch [20340/30000], Loss: 0.5142\n",
      "Epoch [20360/30000], Loss: 0.5142\n",
      "Epoch [20380/30000], Loss: 0.5142\n",
      "Epoch [20400/30000], Loss: 0.5142\n",
      "Epoch [20420/30000], Loss: 0.5142\n",
      "Epoch [20440/30000], Loss: 0.5142\n",
      "Epoch [20460/30000], Loss: 0.5142\n",
      "Epoch [20480/30000], Loss: 0.5142\n",
      "Epoch [20500/30000], Loss: 0.5142\n",
      "Epoch [20520/30000], Loss: 0.5142\n",
      "Epoch [20540/30000], Loss: 0.5142\n",
      "Epoch [20560/30000], Loss: 0.5142\n",
      "Epoch [20580/30000], Loss: 0.5142\n",
      "Epoch [20600/30000], Loss: 0.5142\n",
      "Epoch [20620/30000], Loss: 0.5142\n",
      "Epoch [20640/30000], Loss: 0.5142\n",
      "Epoch [20660/30000], Loss: 0.5142\n",
      "Epoch [20680/30000], Loss: 0.5142\n",
      "Epoch [20700/30000], Loss: 0.5142\n",
      "Epoch [20720/30000], Loss: 0.5142\n",
      "Epoch [20740/30000], Loss: 0.5142\n",
      "Epoch [20760/30000], Loss: 0.5142\n",
      "Epoch [20780/30000], Loss: 0.5142\n",
      "Epoch [20800/30000], Loss: 0.5142\n",
      "Epoch [20820/30000], Loss: 0.5142\n",
      "Epoch [20840/30000], Loss: 0.5142\n",
      "Epoch [20860/30000], Loss: 0.5142\n",
      "Epoch [20880/30000], Loss: 0.5142\n",
      "Epoch [20900/30000], Loss: 0.5142\n",
      "Epoch [20920/30000], Loss: 0.5142\n",
      "Epoch [20940/30000], Loss: 0.5142\n",
      "Epoch [20960/30000], Loss: 0.5141\n",
      "Epoch [20980/30000], Loss: 0.5141\n",
      "Epoch [21000/30000], Loss: 0.5141\n",
      "Epoch [21020/30000], Loss: 0.5141\n",
      "Epoch [21040/30000], Loss: 0.5141\n",
      "Epoch [21060/30000], Loss: 0.5141\n",
      "Epoch [21080/30000], Loss: 0.5141\n",
      "Epoch [21100/30000], Loss: 0.5141\n",
      "Epoch [21120/30000], Loss: 0.5141\n",
      "Epoch [21140/30000], Loss: 0.5141\n",
      "Epoch [21160/30000], Loss: 0.5141\n",
      "Epoch [21180/30000], Loss: 0.5141\n",
      "Epoch [21200/30000], Loss: 0.5141\n",
      "Epoch [21220/30000], Loss: 0.5141\n",
      "Epoch [21240/30000], Loss: 0.5141\n",
      "Epoch [21260/30000], Loss: 0.5141\n",
      "Epoch [21280/30000], Loss: 0.5141\n",
      "Epoch [21300/30000], Loss: 0.5141\n",
      "Epoch [21320/30000], Loss: 0.5141\n",
      "Epoch [21340/30000], Loss: 0.5141\n",
      "Epoch [21360/30000], Loss: 0.5141\n",
      "Epoch [21380/30000], Loss: 0.5141\n",
      "Epoch [21400/30000], Loss: 0.5141\n",
      "Epoch [21420/30000], Loss: 0.5141\n",
      "Epoch [21440/30000], Loss: 0.5141\n",
      "Epoch [21460/30000], Loss: 0.5141\n",
      "Epoch [21480/30000], Loss: 0.5141\n",
      "Epoch [21500/30000], Loss: 0.5141\n",
      "Epoch [21520/30000], Loss: 0.5141\n",
      "Epoch [21540/30000], Loss: 0.5141\n",
      "Epoch [21560/30000], Loss: 0.5141\n",
      "Epoch [21580/30000], Loss: 0.5141\n",
      "Epoch [21600/30000], Loss: 0.5141\n",
      "Epoch [21620/30000], Loss: 0.5141\n",
      "Epoch [21640/30000], Loss: 0.5141\n",
      "Epoch [21660/30000], Loss: 0.5141\n",
      "Epoch [21680/30000], Loss: 0.5141\n",
      "Epoch [21700/30000], Loss: 0.5140\n",
      "Epoch [21720/30000], Loss: 0.5140\n",
      "Epoch [21740/30000], Loss: 0.5140\n",
      "Epoch [21760/30000], Loss: 0.5140\n",
      "Epoch [21780/30000], Loss: 0.5140\n",
      "Epoch [21800/30000], Loss: 0.5140\n",
      "Epoch [21820/30000], Loss: 0.5140\n",
      "Epoch [21840/30000], Loss: 0.5140\n",
      "Epoch [21860/30000], Loss: 0.5140\n",
      "Epoch [21880/30000], Loss: 0.5140\n",
      "Epoch [21900/30000], Loss: 0.5140\n",
      "Epoch [21920/30000], Loss: 0.5140\n",
      "Epoch [21940/30000], Loss: 0.5140\n",
      "Epoch [21960/30000], Loss: 0.5140\n",
      "Epoch [21980/30000], Loss: 0.5140\n",
      "Epoch [22000/30000], Loss: 0.5140\n",
      "Epoch [22020/30000], Loss: 0.5140\n",
      "Epoch [22040/30000], Loss: 0.5140\n",
      "Epoch [22060/30000], Loss: 0.5140\n",
      "Epoch [22080/30000], Loss: 0.5140\n",
      "Epoch [22100/30000], Loss: 0.5140\n",
      "Epoch [22120/30000], Loss: 0.5140\n",
      "Epoch [22140/30000], Loss: 0.5140\n",
      "Epoch [22160/30000], Loss: 0.5140\n",
      "Epoch [22180/30000], Loss: 0.5140\n",
      "Epoch [22200/30000], Loss: 0.5140\n",
      "Epoch [22220/30000], Loss: 0.5140\n",
      "Epoch [22240/30000], Loss: 0.5140\n",
      "Epoch [22260/30000], Loss: 0.5140\n",
      "Epoch [22280/30000], Loss: 0.5140\n",
      "Epoch [22300/30000], Loss: 0.5140\n",
      "Epoch [22320/30000], Loss: 0.5140\n",
      "Epoch [22340/30000], Loss: 0.5140\n",
      "Epoch [22360/30000], Loss: 0.5140\n",
      "Epoch [22380/30000], Loss: 0.5140\n",
      "Epoch [22400/30000], Loss: 0.5140\n",
      "Epoch [22420/30000], Loss: 0.5140\n",
      "Epoch [22440/30000], Loss: 0.5140\n",
      "Epoch [22460/30000], Loss: 0.5140\n",
      "Epoch [22480/30000], Loss: 0.5140\n",
      "Epoch [22500/30000], Loss: 0.5139\n",
      "Epoch [22520/30000], Loss: 0.5139\n",
      "Epoch [22540/30000], Loss: 0.5139\n",
      "Epoch [22560/30000], Loss: 0.5139\n",
      "Epoch [22580/30000], Loss: 0.5139\n",
      "Epoch [22600/30000], Loss: 0.5139\n",
      "Epoch [22620/30000], Loss: 0.5139\n",
      "Epoch [22640/30000], Loss: 0.5139\n",
      "Epoch [22660/30000], Loss: 0.5139\n",
      "Epoch [22680/30000], Loss: 0.5139\n",
      "Epoch [22700/30000], Loss: 0.5139\n",
      "Epoch [22720/30000], Loss: 0.5139\n",
      "Epoch [22740/30000], Loss: 0.5139\n",
      "Epoch [22760/30000], Loss: 0.5139\n",
      "Epoch [22780/30000], Loss: 0.5139\n",
      "Epoch [22800/30000], Loss: 0.5139\n",
      "Epoch [22820/30000], Loss: 0.5139\n",
      "Epoch [22840/30000], Loss: 0.5139\n",
      "Epoch [22860/30000], Loss: 0.5139\n",
      "Epoch [22880/30000], Loss: 0.5139\n",
      "Epoch [22900/30000], Loss: 0.5139\n",
      "Epoch [22920/30000], Loss: 0.5139\n",
      "Epoch [22940/30000], Loss: 0.5139\n",
      "Epoch [22960/30000], Loss: 0.5139\n",
      "Epoch [22980/30000], Loss: 0.5139\n",
      "Epoch [23000/30000], Loss: 0.5139\n",
      "Epoch [23020/30000], Loss: 0.5139\n",
      "Epoch [23040/30000], Loss: 0.5139\n",
      "Epoch [23060/30000], Loss: 0.5139\n",
      "Epoch [23080/30000], Loss: 0.5139\n",
      "Epoch [23100/30000], Loss: 0.5139\n",
      "Epoch [23120/30000], Loss: 0.5139\n",
      "Epoch [23140/30000], Loss: 0.5139\n",
      "Epoch [23160/30000], Loss: 0.5139\n",
      "Epoch [23180/30000], Loss: 0.5139\n",
      "Epoch [23200/30000], Loss: 0.5139\n",
      "Epoch [23220/30000], Loss: 0.5139\n",
      "Epoch [23240/30000], Loss: 0.5139\n",
      "Epoch [23260/30000], Loss: 0.5139\n",
      "Epoch [23280/30000], Loss: 0.5139\n",
      "Epoch [23300/30000], Loss: 0.5139\n",
      "Epoch [23320/30000], Loss: 0.5139\n",
      "Epoch [23340/30000], Loss: 0.5138\n",
      "Epoch [23360/30000], Loss: 0.5138\n",
      "Epoch [23380/30000], Loss: 0.5138\n",
      "Epoch [23400/30000], Loss: 0.5138\n",
      "Epoch [23420/30000], Loss: 0.5138\n",
      "Epoch [23440/30000], Loss: 0.5138\n",
      "Epoch [23460/30000], Loss: 0.5138\n",
      "Epoch [23480/30000], Loss: 0.5138\n",
      "Epoch [23500/30000], Loss: 0.5138\n",
      "Epoch [23520/30000], Loss: 0.5138\n",
      "Epoch [23540/30000], Loss: 0.5138\n",
      "Epoch [23560/30000], Loss: 0.5138\n",
      "Epoch [23580/30000], Loss: 0.5138\n",
      "Epoch [23600/30000], Loss: 0.5138\n",
      "Epoch [23620/30000], Loss: 0.5138\n",
      "Epoch [23640/30000], Loss: 0.5138\n",
      "Epoch [23660/30000], Loss: 0.5138\n",
      "Epoch [23680/30000], Loss: 0.5138\n",
      "Epoch [23700/30000], Loss: 0.5138\n",
      "Epoch [23720/30000], Loss: 0.5138\n",
      "Epoch [23740/30000], Loss: 0.5138\n",
      "Epoch [23760/30000], Loss: 0.5138\n",
      "Epoch [23780/30000], Loss: 0.5138\n",
      "Epoch [23800/30000], Loss: 0.5138\n",
      "Epoch [23820/30000], Loss: 0.5138\n",
      "Epoch [23840/30000], Loss: 0.5138\n",
      "Epoch [23860/30000], Loss: 0.5138\n",
      "Epoch [23880/30000], Loss: 0.5138\n",
      "Epoch [23900/30000], Loss: 0.5138\n",
      "Epoch [23920/30000], Loss: 0.5138\n",
      "Epoch [23940/30000], Loss: 0.5138\n",
      "Epoch [23960/30000], Loss: 0.5138\n",
      "Epoch [23980/30000], Loss: 0.5138\n",
      "Epoch [24000/30000], Loss: 0.5138\n",
      "Epoch [24020/30000], Loss: 0.5138\n",
      "Epoch [24040/30000], Loss: 0.5138\n",
      "Epoch [24060/30000], Loss: 0.5138\n",
      "Epoch [24080/30000], Loss: 0.5138\n",
      "Epoch [24100/30000], Loss: 0.5138\n",
      "Epoch [24120/30000], Loss: 0.5138\n",
      "Epoch [24140/30000], Loss: 0.5138\n",
      "Epoch [24160/30000], Loss: 0.5138\n",
      "Epoch [24180/30000], Loss: 0.5138\n",
      "Epoch [24200/30000], Loss: 0.5138\n",
      "Epoch [24220/30000], Loss: 0.5137\n",
      "Epoch [24240/30000], Loss: 0.5137\n",
      "Epoch [24260/30000], Loss: 0.5137\n",
      "Epoch [24280/30000], Loss: 0.5137\n",
      "Epoch [24300/30000], Loss: 0.5137\n",
      "Epoch [24320/30000], Loss: 0.5137\n",
      "Epoch [24340/30000], Loss: 0.5137\n",
      "Epoch [24360/30000], Loss: 0.5137\n",
      "Epoch [24380/30000], Loss: 0.5137\n",
      "Epoch [24400/30000], Loss: 0.5137\n",
      "Epoch [24420/30000], Loss: 0.5137\n",
      "Epoch [24440/30000], Loss: 0.5137\n",
      "Epoch [24460/30000], Loss: 0.5137\n",
      "Epoch [24480/30000], Loss: 0.5137\n",
      "Epoch [24500/30000], Loss: 0.5137\n",
      "Epoch [24520/30000], Loss: 0.5137\n",
      "Epoch [24540/30000], Loss: 0.5137\n",
      "Epoch [24560/30000], Loss: 0.5137\n",
      "Epoch [24580/30000], Loss: 0.5137\n",
      "Epoch [24600/30000], Loss: 0.5137\n",
      "Epoch [24620/30000], Loss: 0.5137\n",
      "Epoch [24640/30000], Loss: 0.5137\n",
      "Epoch [24660/30000], Loss: 0.5137\n",
      "Epoch [24680/30000], Loss: 0.5137\n",
      "Epoch [24700/30000], Loss: 0.5137\n",
      "Epoch [24720/30000], Loss: 0.5137\n",
      "Epoch [24740/30000], Loss: 0.5137\n",
      "Epoch [24760/30000], Loss: 0.5137\n",
      "Epoch [24780/30000], Loss: 0.5137\n",
      "Epoch [24800/30000], Loss: 0.5137\n",
      "Epoch [24820/30000], Loss: 0.5137\n",
      "Epoch [24840/30000], Loss: 0.5137\n",
      "Epoch [24860/30000], Loss: 0.5137\n",
      "Epoch [24880/30000], Loss: 0.5137\n",
      "Epoch [24900/30000], Loss: 0.5137\n",
      "Epoch [24920/30000], Loss: 0.5137\n",
      "Epoch [24940/30000], Loss: 0.5137\n",
      "Epoch [24960/30000], Loss: 0.5137\n",
      "Epoch [24980/30000], Loss: 0.5137\n",
      "Epoch [25000/30000], Loss: 0.5137\n",
      "Epoch [25020/30000], Loss: 0.5137\n",
      "Epoch [25040/30000], Loss: 0.5137\n",
      "Epoch [25060/30000], Loss: 0.5137\n",
      "Epoch [25080/30000], Loss: 0.5137\n",
      "Epoch [25100/30000], Loss: 0.5137\n",
      "Epoch [25120/30000], Loss: 0.5137\n",
      "Epoch [25140/30000], Loss: 0.5137\n",
      "Epoch [25160/30000], Loss: 0.5137\n",
      "Epoch [25180/30000], Loss: 0.5136\n",
      "Epoch [25200/30000], Loss: 0.5136\n",
      "Epoch [25220/30000], Loss: 0.5136\n",
      "Epoch [25240/30000], Loss: 0.5136\n",
      "Epoch [25260/30000], Loss: 0.5136\n",
      "Epoch [25280/30000], Loss: 0.5136\n",
      "Epoch [25300/30000], Loss: 0.5136\n",
      "Epoch [25320/30000], Loss: 0.5136\n",
      "Epoch [25340/30000], Loss: 0.5136\n",
      "Epoch [25360/30000], Loss: 0.5136\n",
      "Epoch [25380/30000], Loss: 0.5136\n",
      "Epoch [25400/30000], Loss: 0.5136\n",
      "Epoch [25420/30000], Loss: 0.5136\n",
      "Epoch [25440/30000], Loss: 0.5136\n",
      "Epoch [25460/30000], Loss: 0.5136\n",
      "Epoch [25480/30000], Loss: 0.5136\n",
      "Epoch [25500/30000], Loss: 0.5136\n",
      "Epoch [25520/30000], Loss: 0.5136\n",
      "Epoch [25540/30000], Loss: 0.5136\n",
      "Epoch [25560/30000], Loss: 0.5136\n",
      "Epoch [25580/30000], Loss: 0.5136\n",
      "Epoch [25600/30000], Loss: 0.5136\n",
      "Epoch [25620/30000], Loss: 0.5136\n",
      "Epoch [25640/30000], Loss: 0.5136\n",
      "Epoch [25660/30000], Loss: 0.5136\n",
      "Epoch [25680/30000], Loss: 0.5136\n",
      "Epoch [25700/30000], Loss: 0.5136\n",
      "Epoch [25720/30000], Loss: 0.5136\n",
      "Epoch [25740/30000], Loss: 0.5136\n",
      "Epoch [25760/30000], Loss: 0.5136\n",
      "Epoch [25780/30000], Loss: 0.5136\n",
      "Epoch [25800/30000], Loss: 0.5136\n",
      "Epoch [25820/30000], Loss: 0.5136\n",
      "Epoch [25840/30000], Loss: 0.5136\n",
      "Epoch [25860/30000], Loss: 0.5136\n",
      "Epoch [25880/30000], Loss: 0.5136\n",
      "Epoch [25900/30000], Loss: 0.5136\n",
      "Epoch [25920/30000], Loss: 0.5136\n",
      "Epoch [25940/30000], Loss: 0.5136\n",
      "Epoch [25960/30000], Loss: 0.5136\n",
      "Epoch [25980/30000], Loss: 0.5136\n",
      "Epoch [26000/30000], Loss: 0.5136\n",
      "Epoch [26020/30000], Loss: 0.5136\n",
      "Epoch [26040/30000], Loss: 0.5136\n",
      "Epoch [26060/30000], Loss: 0.5136\n",
      "Epoch [26080/30000], Loss: 0.5136\n",
      "Epoch [26100/30000], Loss: 0.5136\n",
      "Epoch [26120/30000], Loss: 0.5136\n",
      "Epoch [26140/30000], Loss: 0.5136\n",
      "Epoch [26160/30000], Loss: 0.5136\n",
      "Epoch [26180/30000], Loss: 0.5136\n",
      "Epoch [26200/30000], Loss: 0.5136\n",
      "Epoch [26220/30000], Loss: 0.5135\n",
      "Epoch [26240/30000], Loss: 0.5135\n",
      "Epoch [26260/30000], Loss: 0.5135\n",
      "Epoch [26280/30000], Loss: 0.5135\n",
      "Epoch [26300/30000], Loss: 0.5135\n",
      "Epoch [26320/30000], Loss: 0.5135\n",
      "Epoch [26340/30000], Loss: 0.5135\n",
      "Epoch [26360/30000], Loss: 0.5135\n",
      "Epoch [26380/30000], Loss: 0.5135\n",
      "Epoch [26400/30000], Loss: 0.5135\n",
      "Epoch [26420/30000], Loss: 0.5135\n",
      "Epoch [26440/30000], Loss: 0.5135\n",
      "Epoch [26460/30000], Loss: 0.5135\n",
      "Epoch [26480/30000], Loss: 0.5135\n",
      "Epoch [26500/30000], Loss: 0.5135\n",
      "Epoch [26520/30000], Loss: 0.5135\n",
      "Epoch [26540/30000], Loss: 0.5135\n",
      "Epoch [26560/30000], Loss: 0.5135\n",
      "Epoch [26580/30000], Loss: 0.5135\n",
      "Epoch [26600/30000], Loss: 0.5135\n",
      "Epoch [26620/30000], Loss: 0.5135\n",
      "Epoch [26640/30000], Loss: 0.5135\n",
      "Epoch [26660/30000], Loss: 0.5135\n",
      "Epoch [26680/30000], Loss: 0.5135\n",
      "Epoch [26700/30000], Loss: 0.5135\n",
      "Epoch [26720/30000], Loss: 0.5135\n",
      "Epoch [26740/30000], Loss: 0.5135\n",
      "Epoch [26760/30000], Loss: 0.5135\n",
      "Epoch [26780/30000], Loss: 0.5135\n",
      "Epoch [26800/30000], Loss: 0.5135\n",
      "Epoch [26820/30000], Loss: 0.5135\n",
      "Epoch [26840/30000], Loss: 0.5135\n",
      "Epoch [26860/30000], Loss: 0.5135\n",
      "Epoch [26880/30000], Loss: 0.5135\n",
      "Epoch [26900/30000], Loss: 0.5135\n",
      "Epoch [26920/30000], Loss: 0.5135\n",
      "Epoch [26940/30000], Loss: 0.5135\n",
      "Epoch [26960/30000], Loss: 0.5135\n",
      "Epoch [26980/30000], Loss: 0.5135\n",
      "Epoch [27000/30000], Loss: 0.5135\n",
      "Epoch [27020/30000], Loss: 0.5135\n",
      "Epoch [27040/30000], Loss: 0.5135\n",
      "Epoch [27060/30000], Loss: 0.5135\n",
      "Epoch [27080/30000], Loss: 0.5135\n",
      "Epoch [27100/30000], Loss: 0.5135\n",
      "Epoch [27120/30000], Loss: 0.5135\n",
      "Epoch [27140/30000], Loss: 0.5135\n",
      "Epoch [27160/30000], Loss: 0.5135\n",
      "Epoch [27180/30000], Loss: 0.5135\n",
      "Epoch [27200/30000], Loss: 0.5135\n",
      "Epoch [27220/30000], Loss: 0.5135\n",
      "Epoch [27240/30000], Loss: 0.5135\n",
      "Epoch [27260/30000], Loss: 0.5135\n",
      "Epoch [27280/30000], Loss: 0.5135\n",
      "Epoch [27300/30000], Loss: 0.5135\n",
      "Epoch [27320/30000], Loss: 0.5134\n",
      "Epoch [27340/30000], Loss: 0.5134\n",
      "Epoch [27360/30000], Loss: 0.5134\n",
      "Epoch [27380/30000], Loss: 0.5134\n",
      "Epoch [27400/30000], Loss: 0.5134\n",
      "Epoch [27420/30000], Loss: 0.5134\n",
      "Epoch [27440/30000], Loss: 0.5134\n",
      "Epoch [27460/30000], Loss: 0.5134\n",
      "Epoch [27480/30000], Loss: 0.5134\n",
      "Epoch [27500/30000], Loss: 0.5134\n",
      "Epoch [27520/30000], Loss: 0.5134\n",
      "Epoch [27540/30000], Loss: 0.5134\n",
      "Epoch [27560/30000], Loss: 0.5134\n",
      "Epoch [27580/30000], Loss: 0.5134\n",
      "Epoch [27600/30000], Loss: 0.5134\n",
      "Epoch [27620/30000], Loss: 0.5134\n",
      "Epoch [27640/30000], Loss: 0.5134\n",
      "Epoch [27660/30000], Loss: 0.5134\n",
      "Epoch [27680/30000], Loss: 0.5134\n",
      "Epoch [27700/30000], Loss: 0.5134\n",
      "Epoch [27720/30000], Loss: 0.5134\n",
      "Epoch [27740/30000], Loss: 0.5134\n",
      "Epoch [27760/30000], Loss: 0.5134\n",
      "Epoch [27780/30000], Loss: 0.5134\n",
      "Epoch [27800/30000], Loss: 0.5134\n",
      "Epoch [27820/30000], Loss: 0.5134\n",
      "Epoch [27840/30000], Loss: 0.5134\n",
      "Epoch [27860/30000], Loss: 0.5134\n",
      "Epoch [27880/30000], Loss: 0.5134\n",
      "Epoch [27900/30000], Loss: 0.5134\n",
      "Epoch [27920/30000], Loss: 0.5134\n",
      "Epoch [27940/30000], Loss: 0.5134\n",
      "Epoch [27960/30000], Loss: 0.5134\n",
      "Epoch [27980/30000], Loss: 0.5134\n",
      "Epoch [28000/30000], Loss: 0.5134\n",
      "Epoch [28020/30000], Loss: 0.5134\n",
      "Epoch [28040/30000], Loss: 0.5134\n",
      "Epoch [28060/30000], Loss: 0.5134\n",
      "Epoch [28080/30000], Loss: 0.5134\n",
      "Epoch [28100/30000], Loss: 0.5134\n",
      "Epoch [28120/30000], Loss: 0.5134\n",
      "Epoch [28140/30000], Loss: 0.5134\n",
      "Epoch [28160/30000], Loss: 0.5134\n",
      "Epoch [28180/30000], Loss: 0.5134\n",
      "Epoch [28200/30000], Loss: 0.5134\n",
      "Epoch [28220/30000], Loss: 0.5134\n",
      "Epoch [28240/30000], Loss: 0.5134\n",
      "Epoch [28260/30000], Loss: 0.5134\n",
      "Epoch [28280/30000], Loss: 0.5134\n",
      "Epoch [28300/30000], Loss: 0.5134\n",
      "Epoch [28320/30000], Loss: 0.5134\n",
      "Epoch [28340/30000], Loss: 0.5134\n",
      "Epoch [28360/30000], Loss: 0.5134\n",
      "Epoch [28380/30000], Loss: 0.5134\n",
      "Epoch [28400/30000], Loss: 0.5134\n",
      "Epoch [28420/30000], Loss: 0.5134\n",
      "Epoch [28440/30000], Loss: 0.5134\n",
      "Epoch [28460/30000], Loss: 0.5134\n",
      "Epoch [28480/30000], Loss: 0.5134\n",
      "Epoch [28500/30000], Loss: 0.5133\n",
      "Epoch [28520/30000], Loss: 0.5133\n",
      "Epoch [28540/30000], Loss: 0.5133\n",
      "Epoch [28560/30000], Loss: 0.5133\n",
      "Epoch [28580/30000], Loss: 0.5133\n",
      "Epoch [28600/30000], Loss: 0.5133\n",
      "Epoch [28620/30000], Loss: 0.5133\n",
      "Epoch [28640/30000], Loss: 0.5133\n",
      "Epoch [28660/30000], Loss: 0.5133\n",
      "Epoch [28680/30000], Loss: 0.5133\n",
      "Epoch [28700/30000], Loss: 0.5133\n",
      "Epoch [28720/30000], Loss: 0.5133\n",
      "Epoch [28740/30000], Loss: 0.5133\n",
      "Epoch [28760/30000], Loss: 0.5133\n",
      "Epoch [28780/30000], Loss: 0.5133\n",
      "Epoch [28800/30000], Loss: 0.5133\n",
      "Epoch [28820/30000], Loss: 0.5133\n",
      "Epoch [28840/30000], Loss: 0.5133\n",
      "Epoch [28860/30000], Loss: 0.5133\n",
      "Epoch [28880/30000], Loss: 0.5133\n",
      "Epoch [28900/30000], Loss: 0.5133\n",
      "Epoch [28920/30000], Loss: 0.5133\n",
      "Epoch [28940/30000], Loss: 0.5133\n",
      "Epoch [28960/30000], Loss: 0.5133\n",
      "Epoch [28980/30000], Loss: 0.5133\n",
      "Epoch [29000/30000], Loss: 0.5133\n",
      "Epoch [29020/30000], Loss: 0.5133\n",
      "Epoch [29040/30000], Loss: 0.5133\n",
      "Epoch [29060/30000], Loss: 0.5133\n",
      "Epoch [29080/30000], Loss: 0.5133\n",
      "Epoch [29100/30000], Loss: 0.5133\n",
      "Epoch [29120/30000], Loss: 0.5133\n",
      "Epoch [29140/30000], Loss: 0.5133\n",
      "Epoch [29160/30000], Loss: 0.5133\n",
      "Epoch [29180/30000], Loss: 0.5133\n",
      "Epoch [29200/30000], Loss: 0.5133\n",
      "Epoch [29220/30000], Loss: 0.5133\n",
      "Epoch [29240/30000], Loss: 0.5133\n",
      "Epoch [29260/30000], Loss: 0.5133\n",
      "Epoch [29280/30000], Loss: 0.5133\n",
      "Epoch [29300/30000], Loss: 0.5133\n",
      "Epoch [29320/30000], Loss: 0.5133\n",
      "Epoch [29340/30000], Loss: 0.5133\n",
      "Epoch [29360/30000], Loss: 0.5133\n",
      "Epoch [29380/30000], Loss: 0.5133\n",
      "Epoch [29400/30000], Loss: 0.5133\n",
      "Epoch [29420/30000], Loss: 0.5133\n",
      "Epoch [29440/30000], Loss: 0.5133\n",
      "Epoch [29460/30000], Loss: 0.5133\n",
      "Epoch [29480/30000], Loss: 0.5133\n",
      "Epoch [29500/30000], Loss: 0.5133\n",
      "Epoch [29520/30000], Loss: 0.5133\n",
      "Epoch [29540/30000], Loss: 0.5133\n",
      "Epoch [29560/30000], Loss: 0.5133\n",
      "Epoch [29580/30000], Loss: 0.5133\n",
      "Epoch [29600/30000], Loss: 0.5133\n",
      "Epoch [29620/30000], Loss: 0.5133\n",
      "Epoch [29640/30000], Loss: 0.5133\n",
      "Epoch [29660/30000], Loss: 0.5133\n",
      "Epoch [29680/30000], Loss: 0.5133\n",
      "Epoch [29700/30000], Loss: 0.5133\n",
      "Epoch [29720/30000], Loss: 0.5133\n",
      "Epoch [29740/30000], Loss: 0.5133\n",
      "Epoch [29760/30000], Loss: 0.5133\n",
      "Epoch [29780/30000], Loss: 0.5132\n",
      "Epoch [29800/30000], Loss: 0.5132\n",
      "Epoch [29820/30000], Loss: 0.5132\n",
      "Epoch [29840/30000], Loss: 0.5132\n",
      "Epoch [29860/30000], Loss: 0.5132\n",
      "Epoch [29880/30000], Loss: 0.5132\n",
      "Epoch [29900/30000], Loss: 0.5132\n",
      "Epoch [29920/30000], Loss: 0.5132\n",
      "Epoch [29940/30000], Loss: 0.5132\n",
      "Epoch [29960/30000], Loss: 0.5132\n",
      "Epoch [29980/30000], Loss: 0.5132\n",
      "Epoch [30000/30000], Loss: 0.5132\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "epochs = 30000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ## Forward pass\n",
    "    \n",
    "    # Compute the predicted outputs\n",
    "    outputs = model.forward(X_train)\n",
    "    \n",
    "    # Compute the loss function using the current predictions\n",
    "    loss = criterion(outputs, y_train)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    ## Backward pass\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad() # zero them from the previous iteration\n",
    "    loss.backward() # compute the gradients!\n",
    "    optimizer.step() # update the weights!\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e92c67b164cc38cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:54.428641Z",
     "start_time": "2024-11-04T21:17:54.357184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x284cf7610>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dc8f23d5dd6d1",
   "metadata": {},
   "source": [
    "## Evaluating the trained model\n",
    "- We can now evaluate the trained model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9769b7d89c1c61f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:18:08.804872Z",
     "start_time": "2024-11-04T21:18:08.800560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0061, 0.9939],\n",
       "        [0.9766, 0.0234],\n",
       "        [0.0020, 0.9980],\n",
       "        ...,\n",
       "        [0.0134, 0.9866],\n",
       "        [0.1292, 0.8708],\n",
       "        [0.1105, 0.8895]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the predicted outputs\n",
    "outputs = model(X_val)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f210e176f3e6ed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:18:10.449651Z",
     "start_time": "2024-11-04T21:18:10.442110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8300646926797651)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#compute the estimated probabilities of the positive class\n",
    "py_hat_val = outputs[:,1]\n",
    "\n",
    "# convert the probabilities to numpy format\n",
    "py_hat_val = py_hat_val.detach().numpy()\n",
    "\n",
    "# measure the area under the ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_val, py_hat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c1ad4bce1df9",
   "metadata": {},
   "source": [
    "### How did we do relative to random guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42521684cee8496",
   "metadata": {},
   "source": [
    "## Implementing an MLP with a single hidden layer\n",
    "- We will now implement a multilayer perceptron (MLP) with a single hidden layer\n",
    "- For this, we will need to define a new class that inherits from ```nn.Module```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "750d49766bb3d2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:18:20.127495Z",
     "start_time": "2024-11-04T21:18:20.124259Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SingleLayerMLP, self).__init__()\n",
    "        # First layer (input to hidden)\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        # Activation function (ReLU introduces non-linearity)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layer (hidden to output)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        # softmax activation for output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first layer, apply ReLU, then pass to output layer and apply softmax\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23a3d4bc816878",
   "metadata": {},
   "source": [
    "## Creating an instance of the single-layer MLP model\n",
    "- We need to select the number of units in our hidden layer\n",
    "- Remember that this is called a *hyperparameter* because it is not learned from the data\n",
    "- We will begin with an arbitrary choice of 5 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85c56744e22fa0c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:18:22.320942Z",
     "start_time": "2024-11-04T21:18:22.316630Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 5  # Choose an arbitrary number of neurons for the hidden layer\n",
    "model = SingleLayerMLP(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fd4bb4c36acea",
   "metadata": {},
   "source": [
    "## Inspecting the model architecture is good practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21623d4d342b6c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:18:23.820597Z",
     "start_time": "2024-11-04T21:18:23.818153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleLayerMLP(\n",
      "  (hidden): Linear(in_features=50, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (output): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81f5add8d5e819",
   "metadata": {},
   "source": [
    "## Define the optimizer and loss function (same as before)\n",
    "- Now we will train our single-layer MLP model using the same cross-entropy loss and stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7899d49b0c398bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:18:25.376435Z",
     "start_time": "2024-11-04T21:18:25.374047Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23be144e66d07721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:19:09.413638Z",
     "start_time": "2024-11-04T21:18:26.707374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30000], Loss: 0.7042\n",
      "Epoch [40/30000], Loss: 0.7029\n",
      "Epoch [60/30000], Loss: 0.7017\n",
      "Epoch [80/30000], Loss: 0.7006\n",
      "Epoch [100/30000], Loss: 0.6996\n",
      "Epoch [120/30000], Loss: 0.6987\n",
      "Epoch [140/30000], Loss: 0.6978\n",
      "Epoch [160/30000], Loss: 0.6970\n",
      "Epoch [180/30000], Loss: 0.6962\n",
      "Epoch [200/30000], Loss: 0.6955\n",
      "Epoch [220/30000], Loss: 0.6948\n",
      "Epoch [240/30000], Loss: 0.6942\n",
      "Epoch [260/30000], Loss: 0.6936\n",
      "Epoch [280/30000], Loss: 0.6930\n",
      "Epoch [300/30000], Loss: 0.6924\n",
      "Epoch [320/30000], Loss: 0.6918\n",
      "Epoch [340/30000], Loss: 0.6913\n",
      "Epoch [360/30000], Loss: 0.6907\n",
      "Epoch [380/30000], Loss: 0.6902\n",
      "Epoch [400/30000], Loss: 0.6896\n",
      "Epoch [420/30000], Loss: 0.6891\n",
      "Epoch [440/30000], Loss: 0.6885\n",
      "Epoch [460/30000], Loss: 0.6880\n",
      "Epoch [480/30000], Loss: 0.6874\n",
      "Epoch [500/30000], Loss: 0.6868\n",
      "Epoch [520/30000], Loss: 0.6862\n",
      "Epoch [540/30000], Loss: 0.6856\n",
      "Epoch [560/30000], Loss: 0.6850\n",
      "Epoch [580/30000], Loss: 0.6843\n",
      "Epoch [600/30000], Loss: 0.6836\n",
      "Epoch [620/30000], Loss: 0.6829\n",
      "Epoch [640/30000], Loss: 0.6822\n",
      "Epoch [660/30000], Loss: 0.6814\n",
      "Epoch [680/30000], Loss: 0.6806\n",
      "Epoch [700/30000], Loss: 0.6798\n",
      "Epoch [720/30000], Loss: 0.6789\n",
      "Epoch [740/30000], Loss: 0.6780\n",
      "Epoch [760/30000], Loss: 0.6770\n",
      "Epoch [780/30000], Loss: 0.6760\n",
      "Epoch [800/30000], Loss: 0.6750\n",
      "Epoch [820/30000], Loss: 0.6738\n",
      "Epoch [840/30000], Loss: 0.6726\n",
      "Epoch [860/30000], Loss: 0.6714\n",
      "Epoch [880/30000], Loss: 0.6701\n",
      "Epoch [900/30000], Loss: 0.6687\n",
      "Epoch [920/30000], Loss: 0.6672\n",
      "Epoch [940/30000], Loss: 0.6656\n",
      "Epoch [960/30000], Loss: 0.6640\n",
      "Epoch [980/30000], Loss: 0.6622\n",
      "Epoch [1000/30000], Loss: 0.6603\n",
      "Epoch [1020/30000], Loss: 0.6584\n",
      "Epoch [1040/30000], Loss: 0.6563\n",
      "Epoch [1060/30000], Loss: 0.6542\n",
      "Epoch [1080/30000], Loss: 0.6519\n",
      "Epoch [1100/30000], Loss: 0.6495\n",
      "Epoch [1120/30000], Loss: 0.6470\n",
      "Epoch [1140/30000], Loss: 0.6444\n",
      "Epoch [1160/30000], Loss: 0.6417\n",
      "Epoch [1180/30000], Loss: 0.6389\n",
      "Epoch [1200/30000], Loss: 0.6361\n",
      "Epoch [1220/30000], Loss: 0.6331\n",
      "Epoch [1240/30000], Loss: 0.6301\n",
      "Epoch [1260/30000], Loss: 0.6270\n",
      "Epoch [1280/30000], Loss: 0.6239\n",
      "Epoch [1300/30000], Loss: 0.6207\n",
      "Epoch [1320/30000], Loss: 0.6175\n",
      "Epoch [1340/30000], Loss: 0.6143\n",
      "Epoch [1360/30000], Loss: 0.6111\n",
      "Epoch [1380/30000], Loss: 0.6078\n",
      "Epoch [1400/30000], Loss: 0.6046\n",
      "Epoch [1420/30000], Loss: 0.6015\n",
      "Epoch [1440/30000], Loss: 0.5983\n",
      "Epoch [1460/30000], Loss: 0.5952\n",
      "Epoch [1480/30000], Loss: 0.5921\n",
      "Epoch [1500/30000], Loss: 0.5891\n",
      "Epoch [1520/30000], Loss: 0.5862\n",
      "Epoch [1540/30000], Loss: 0.5833\n",
      "Epoch [1560/30000], Loss: 0.5805\n",
      "Epoch [1580/30000], Loss: 0.5777\n",
      "Epoch [1600/30000], Loss: 0.5751\n",
      "Epoch [1620/30000], Loss: 0.5725\n",
      "Epoch [1640/30000], Loss: 0.5699\n",
      "Epoch [1660/30000], Loss: 0.5674\n",
      "Epoch [1680/30000], Loss: 0.5650\n",
      "Epoch [1700/30000], Loss: 0.5627\n",
      "Epoch [1720/30000], Loss: 0.5604\n",
      "Epoch [1740/30000], Loss: 0.5582\n",
      "Epoch [1760/30000], Loss: 0.5561\n",
      "Epoch [1780/30000], Loss: 0.5540\n",
      "Epoch [1800/30000], Loss: 0.5519\n",
      "Epoch [1820/30000], Loss: 0.5499\n",
      "Epoch [1840/30000], Loss: 0.5480\n",
      "Epoch [1860/30000], Loss: 0.5461\n",
      "Epoch [1880/30000], Loss: 0.5443\n",
      "Epoch [1900/30000], Loss: 0.5425\n",
      "Epoch [1920/30000], Loss: 0.5408\n",
      "Epoch [1940/30000], Loss: 0.5391\n",
      "Epoch [1960/30000], Loss: 0.5374\n",
      "Epoch [1980/30000], Loss: 0.5359\n",
      "Epoch [2000/30000], Loss: 0.5343\n",
      "Epoch [2020/30000], Loss: 0.5328\n",
      "Epoch [2040/30000], Loss: 0.5313\n",
      "Epoch [2060/30000], Loss: 0.5299\n",
      "Epoch [2080/30000], Loss: 0.5285\n",
      "Epoch [2100/30000], Loss: 0.5272\n",
      "Epoch [2120/30000], Loss: 0.5259\n",
      "Epoch [2140/30000], Loss: 0.5246\n",
      "Epoch [2160/30000], Loss: 0.5234\n",
      "Epoch [2180/30000], Loss: 0.5221\n",
      "Epoch [2200/30000], Loss: 0.5210\n",
      "Epoch [2220/30000], Loss: 0.5198\n",
      "Epoch [2240/30000], Loss: 0.5187\n",
      "Epoch [2260/30000], Loss: 0.5176\n",
      "Epoch [2280/30000], Loss: 0.5165\n",
      "Epoch [2300/30000], Loss: 0.5154\n",
      "Epoch [2320/30000], Loss: 0.5144\n",
      "Epoch [2340/30000], Loss: 0.5134\n",
      "Epoch [2360/30000], Loss: 0.5124\n",
      "Epoch [2380/30000], Loss: 0.5114\n",
      "Epoch [2400/30000], Loss: 0.5105\n",
      "Epoch [2420/30000], Loss: 0.5095\n",
      "Epoch [2440/30000], Loss: 0.5086\n",
      "Epoch [2460/30000], Loss: 0.5077\n",
      "Epoch [2480/30000], Loss: 0.5068\n",
      "Epoch [2500/30000], Loss: 0.5060\n",
      "Epoch [2520/30000], Loss: 0.5051\n",
      "Epoch [2540/30000], Loss: 0.5043\n",
      "Epoch [2560/30000], Loss: 0.5035\n",
      "Epoch [2580/30000], Loss: 0.5027\n",
      "Epoch [2600/30000], Loss: 0.5019\n",
      "Epoch [2620/30000], Loss: 0.5011\n",
      "Epoch [2640/30000], Loss: 0.5003\n",
      "Epoch [2660/30000], Loss: 0.4996\n",
      "Epoch [2680/30000], Loss: 0.4988\n",
      "Epoch [2700/30000], Loss: 0.4981\n",
      "Epoch [2720/30000], Loss: 0.4974\n",
      "Epoch [2740/30000], Loss: 0.4966\n",
      "Epoch [2760/30000], Loss: 0.4959\n",
      "Epoch [2780/30000], Loss: 0.4953\n",
      "Epoch [2800/30000], Loss: 0.4946\n",
      "Epoch [2820/30000], Loss: 0.4939\n",
      "Epoch [2840/30000], Loss: 0.4932\n",
      "Epoch [2860/30000], Loss: 0.4926\n",
      "Epoch [2880/30000], Loss: 0.4920\n",
      "Epoch [2900/30000], Loss: 0.4913\n",
      "Epoch [2920/30000], Loss: 0.4907\n",
      "Epoch [2940/30000], Loss: 0.4901\n",
      "Epoch [2960/30000], Loss: 0.4895\n",
      "Epoch [2980/30000], Loss: 0.4889\n",
      "Epoch [3000/30000], Loss: 0.4883\n",
      "Epoch [3020/30000], Loss: 0.4877\n",
      "Epoch [3040/30000], Loss: 0.4871\n",
      "Epoch [3060/30000], Loss: 0.4865\n",
      "Epoch [3080/30000], Loss: 0.4860\n",
      "Epoch [3100/30000], Loss: 0.4854\n",
      "Epoch [3120/30000], Loss: 0.4848\n",
      "Epoch [3140/30000], Loss: 0.4843\n",
      "Epoch [3160/30000], Loss: 0.4837\n",
      "Epoch [3180/30000], Loss: 0.4832\n",
      "Epoch [3200/30000], Loss: 0.4826\n",
      "Epoch [3220/30000], Loss: 0.4821\n",
      "Epoch [3240/30000], Loss: 0.4816\n",
      "Epoch [3260/30000], Loss: 0.4811\n",
      "Epoch [3280/30000], Loss: 0.4805\n",
      "Epoch [3300/30000], Loss: 0.4800\n",
      "Epoch [3320/30000], Loss: 0.4795\n",
      "Epoch [3340/30000], Loss: 0.4790\n",
      "Epoch [3360/30000], Loss: 0.4785\n",
      "Epoch [3380/30000], Loss: 0.4780\n",
      "Epoch [3400/30000], Loss: 0.4775\n",
      "Epoch [3420/30000], Loss: 0.4770\n",
      "Epoch [3440/30000], Loss: 0.4765\n",
      "Epoch [3460/30000], Loss: 0.4760\n",
      "Epoch [3480/30000], Loss: 0.4755\n",
      "Epoch [3500/30000], Loss: 0.4751\n",
      "Epoch [3520/30000], Loss: 0.4746\n",
      "Epoch [3540/30000], Loss: 0.4741\n",
      "Epoch [3560/30000], Loss: 0.4737\n",
      "Epoch [3580/30000], Loss: 0.4732\n",
      "Epoch [3600/30000], Loss: 0.4728\n",
      "Epoch [3620/30000], Loss: 0.4723\n",
      "Epoch [3640/30000], Loss: 0.4719\n",
      "Epoch [3660/30000], Loss: 0.4714\n",
      "Epoch [3680/30000], Loss: 0.4710\n",
      "Epoch [3700/30000], Loss: 0.4705\n",
      "Epoch [3720/30000], Loss: 0.4701\n",
      "Epoch [3740/30000], Loss: 0.4697\n",
      "Epoch [3760/30000], Loss: 0.4693\n",
      "Epoch [3780/30000], Loss: 0.4688\n",
      "Epoch [3800/30000], Loss: 0.4684\n",
      "Epoch [3820/30000], Loss: 0.4680\n",
      "Epoch [3840/30000], Loss: 0.4676\n",
      "Epoch [3860/30000], Loss: 0.4672\n",
      "Epoch [3880/30000], Loss: 0.4668\n",
      "Epoch [3900/30000], Loss: 0.4664\n",
      "Epoch [3920/30000], Loss: 0.4660\n",
      "Epoch [3940/30000], Loss: 0.4655\n",
      "Epoch [3960/30000], Loss: 0.4652\n",
      "Epoch [3980/30000], Loss: 0.4648\n",
      "Epoch [4000/30000], Loss: 0.4644\n",
      "Epoch [4020/30000], Loss: 0.4640\n",
      "Epoch [4040/30000], Loss: 0.4636\n",
      "Epoch [4060/30000], Loss: 0.4632\n",
      "Epoch [4080/30000], Loss: 0.4628\n",
      "Epoch [4100/30000], Loss: 0.4625\n",
      "Epoch [4120/30000], Loss: 0.4621\n",
      "Epoch [4140/30000], Loss: 0.4617\n",
      "Epoch [4160/30000], Loss: 0.4614\n",
      "Epoch [4180/30000], Loss: 0.4610\n",
      "Epoch [4200/30000], Loss: 0.4607\n",
      "Epoch [4220/30000], Loss: 0.4603\n",
      "Epoch [4240/30000], Loss: 0.4600\n",
      "Epoch [4260/30000], Loss: 0.4596\n",
      "Epoch [4280/30000], Loss: 0.4593\n",
      "Epoch [4300/30000], Loss: 0.4589\n",
      "Epoch [4320/30000], Loss: 0.4586\n",
      "Epoch [4340/30000], Loss: 0.4582\n",
      "Epoch [4360/30000], Loss: 0.4579\n",
      "Epoch [4380/30000], Loss: 0.4576\n",
      "Epoch [4400/30000], Loss: 0.4572\n",
      "Epoch [4420/30000], Loss: 0.4569\n",
      "Epoch [4440/30000], Loss: 0.4566\n",
      "Epoch [4460/30000], Loss: 0.4562\n",
      "Epoch [4480/30000], Loss: 0.4559\n",
      "Epoch [4500/30000], Loss: 0.4556\n",
      "Epoch [4520/30000], Loss: 0.4553\n",
      "Epoch [4540/30000], Loss: 0.4550\n",
      "Epoch [4560/30000], Loss: 0.4546\n",
      "Epoch [4580/30000], Loss: 0.4543\n",
      "Epoch [4600/30000], Loss: 0.4540\n",
      "Epoch [4620/30000], Loss: 0.4537\n",
      "Epoch [4640/30000], Loss: 0.4534\n",
      "Epoch [4660/30000], Loss: 0.4531\n",
      "Epoch [4680/30000], Loss: 0.4528\n",
      "Epoch [4700/30000], Loss: 0.4525\n",
      "Epoch [4720/30000], Loss: 0.4522\n",
      "Epoch [4740/30000], Loss: 0.4519\n",
      "Epoch [4760/30000], Loss: 0.4516\n",
      "Epoch [4780/30000], Loss: 0.4513\n",
      "Epoch [4800/30000], Loss: 0.4510\n",
      "Epoch [4820/30000], Loss: 0.4507\n",
      "Epoch [4840/30000], Loss: 0.4505\n",
      "Epoch [4860/30000], Loss: 0.4502\n",
      "Epoch [4880/30000], Loss: 0.4499\n",
      "Epoch [4900/30000], Loss: 0.4496\n",
      "Epoch [4920/30000], Loss: 0.4493\n",
      "Epoch [4940/30000], Loss: 0.4491\n",
      "Epoch [4960/30000], Loss: 0.4488\n",
      "Epoch [4980/30000], Loss: 0.4485\n",
      "Epoch [5000/30000], Loss: 0.4482\n",
      "Epoch [5020/30000], Loss: 0.4480\n",
      "Epoch [5040/30000], Loss: 0.4477\n",
      "Epoch [5060/30000], Loss: 0.4474\n",
      "Epoch [5080/30000], Loss: 0.4472\n",
      "Epoch [5100/30000], Loss: 0.4469\n",
      "Epoch [5120/30000], Loss: 0.4467\n",
      "Epoch [5140/30000], Loss: 0.4464\n",
      "Epoch [5160/30000], Loss: 0.4461\n",
      "Epoch [5180/30000], Loss: 0.4459\n",
      "Epoch [5200/30000], Loss: 0.4456\n",
      "Epoch [5220/30000], Loss: 0.4454\n",
      "Epoch [5240/30000], Loss: 0.4451\n",
      "Epoch [5260/30000], Loss: 0.4449\n",
      "Epoch [5280/30000], Loss: 0.4446\n",
      "Epoch [5300/30000], Loss: 0.4443\n",
      "Epoch [5320/30000], Loss: 0.4441\n",
      "Epoch [5340/30000], Loss: 0.4438\n",
      "Epoch [5360/30000], Loss: 0.4436\n",
      "Epoch [5380/30000], Loss: 0.4434\n",
      "Epoch [5400/30000], Loss: 0.4431\n",
      "Epoch [5420/30000], Loss: 0.4429\n",
      "Epoch [5440/30000], Loss: 0.4426\n",
      "Epoch [5460/30000], Loss: 0.4424\n",
      "Epoch [5480/30000], Loss: 0.4421\n",
      "Epoch [5500/30000], Loss: 0.4419\n",
      "Epoch [5520/30000], Loss: 0.4417\n",
      "Epoch [5540/30000], Loss: 0.4414\n",
      "Epoch [5560/30000], Loss: 0.4412\n",
      "Epoch [5580/30000], Loss: 0.4409\n",
      "Epoch [5600/30000], Loss: 0.4407\n",
      "Epoch [5620/30000], Loss: 0.4405\n",
      "Epoch [5640/30000], Loss: 0.4402\n",
      "Epoch [5660/30000], Loss: 0.4400\n",
      "Epoch [5680/30000], Loss: 0.4397\n",
      "Epoch [5700/30000], Loss: 0.4395\n",
      "Epoch [5720/30000], Loss: 0.4393\n",
      "Epoch [5740/30000], Loss: 0.4390\n",
      "Epoch [5760/30000], Loss: 0.4388\n",
      "Epoch [5780/30000], Loss: 0.4386\n",
      "Epoch [5800/30000], Loss: 0.4384\n",
      "Epoch [5820/30000], Loss: 0.4381\n",
      "Epoch [5840/30000], Loss: 0.4379\n",
      "Epoch [5860/30000], Loss: 0.4377\n",
      "Epoch [5880/30000], Loss: 0.4374\n",
      "Epoch [5900/30000], Loss: 0.4372\n",
      "Epoch [5920/30000], Loss: 0.4370\n",
      "Epoch [5940/30000], Loss: 0.4368\n",
      "Epoch [5960/30000], Loss: 0.4365\n",
      "Epoch [5980/30000], Loss: 0.4363\n",
      "Epoch [6000/30000], Loss: 0.4361\n",
      "Epoch [6020/30000], Loss: 0.4359\n",
      "Epoch [6040/30000], Loss: 0.4356\n",
      "Epoch [6060/30000], Loss: 0.4354\n",
      "Epoch [6080/30000], Loss: 0.4352\n",
      "Epoch [6100/30000], Loss: 0.4350\n",
      "Epoch [6120/30000], Loss: 0.4348\n",
      "Epoch [6140/30000], Loss: 0.4345\n",
      "Epoch [6160/30000], Loss: 0.4343\n",
      "Epoch [6180/30000], Loss: 0.4341\n",
      "Epoch [6200/30000], Loss: 0.4339\n",
      "Epoch [6220/30000], Loss: 0.4337\n",
      "Epoch [6240/30000], Loss: 0.4335\n",
      "Epoch [6260/30000], Loss: 0.4333\n",
      "Epoch [6280/30000], Loss: 0.4331\n",
      "Epoch [6300/30000], Loss: 0.4328\n",
      "Epoch [6320/30000], Loss: 0.4326\n",
      "Epoch [6340/30000], Loss: 0.4324\n",
      "Epoch [6360/30000], Loss: 0.4322\n",
      "Epoch [6380/30000], Loss: 0.4320\n",
      "Epoch [6400/30000], Loss: 0.4318\n",
      "Epoch [6420/30000], Loss: 0.4316\n",
      "Epoch [6440/30000], Loss: 0.4314\n",
      "Epoch [6460/30000], Loss: 0.4312\n",
      "Epoch [6480/30000], Loss: 0.4310\n",
      "Epoch [6500/30000], Loss: 0.4308\n",
      "Epoch [6520/30000], Loss: 0.4306\n",
      "Epoch [6540/30000], Loss: 0.4303\n",
      "Epoch [6560/30000], Loss: 0.4301\n",
      "Epoch [6580/30000], Loss: 0.4299\n",
      "Epoch [6600/30000], Loss: 0.4297\n",
      "Epoch [6620/30000], Loss: 0.4295\n",
      "Epoch [6640/30000], Loss: 0.4293\n",
      "Epoch [6660/30000], Loss: 0.4291\n",
      "Epoch [6680/30000], Loss: 0.4290\n",
      "Epoch [6700/30000], Loss: 0.4288\n",
      "Epoch [6720/30000], Loss: 0.4286\n",
      "Epoch [6740/30000], Loss: 0.4284\n",
      "Epoch [6760/30000], Loss: 0.4282\n",
      "Epoch [6780/30000], Loss: 0.4280\n",
      "Epoch [6800/30000], Loss: 0.4278\n",
      "Epoch [6820/30000], Loss: 0.4276\n",
      "Epoch [6840/30000], Loss: 0.4274\n",
      "Epoch [6860/30000], Loss: 0.4272\n",
      "Epoch [6880/30000], Loss: 0.4270\n",
      "Epoch [6900/30000], Loss: 0.4269\n",
      "Epoch [6920/30000], Loss: 0.4267\n",
      "Epoch [6940/30000], Loss: 0.4265\n",
      "Epoch [6960/30000], Loss: 0.4263\n",
      "Epoch [6980/30000], Loss: 0.4261\n",
      "Epoch [7000/30000], Loss: 0.4259\n",
      "Epoch [7020/30000], Loss: 0.4258\n",
      "Epoch [7040/30000], Loss: 0.4256\n",
      "Epoch [7060/30000], Loss: 0.4254\n",
      "Epoch [7080/30000], Loss: 0.4252\n",
      "Epoch [7100/30000], Loss: 0.4250\n",
      "Epoch [7120/30000], Loss: 0.4249\n",
      "Epoch [7140/30000], Loss: 0.4247\n",
      "Epoch [7160/30000], Loss: 0.4245\n",
      "Epoch [7180/30000], Loss: 0.4243\n",
      "Epoch [7200/30000], Loss: 0.4242\n",
      "Epoch [7220/30000], Loss: 0.4240\n",
      "Epoch [7240/30000], Loss: 0.4238\n",
      "Epoch [7260/30000], Loss: 0.4237\n",
      "Epoch [7280/30000], Loss: 0.4235\n",
      "Epoch [7300/30000], Loss: 0.4233\n",
      "Epoch [7320/30000], Loss: 0.4232\n",
      "Epoch [7340/30000], Loss: 0.4230\n",
      "Epoch [7360/30000], Loss: 0.4228\n",
      "Epoch [7380/30000], Loss: 0.4227\n",
      "Epoch [7400/30000], Loss: 0.4225\n",
      "Epoch [7420/30000], Loss: 0.4223\n",
      "Epoch [7440/30000], Loss: 0.4222\n",
      "Epoch [7460/30000], Loss: 0.4220\n",
      "Epoch [7480/30000], Loss: 0.4219\n",
      "Epoch [7500/30000], Loss: 0.4217\n",
      "Epoch [7520/30000], Loss: 0.4215\n",
      "Epoch [7540/30000], Loss: 0.4214\n",
      "Epoch [7560/30000], Loss: 0.4212\n",
      "Epoch [7580/30000], Loss: 0.4211\n",
      "Epoch [7600/30000], Loss: 0.4209\n",
      "Epoch [7620/30000], Loss: 0.4208\n",
      "Epoch [7640/30000], Loss: 0.4206\n",
      "Epoch [7660/30000], Loss: 0.4205\n",
      "Epoch [7680/30000], Loss: 0.4203\n",
      "Epoch [7700/30000], Loss: 0.4202\n",
      "Epoch [7720/30000], Loss: 0.4200\n",
      "Epoch [7740/30000], Loss: 0.4199\n",
      "Epoch [7760/30000], Loss: 0.4197\n",
      "Epoch [7780/30000], Loss: 0.4196\n",
      "Epoch [7800/30000], Loss: 0.4194\n",
      "Epoch [7820/30000], Loss: 0.4193\n",
      "Epoch [7840/30000], Loss: 0.4191\n",
      "Epoch [7860/30000], Loss: 0.4190\n",
      "Epoch [7880/30000], Loss: 0.4189\n",
      "Epoch [7900/30000], Loss: 0.4187\n",
      "Epoch [7920/30000], Loss: 0.4186\n",
      "Epoch [7940/30000], Loss: 0.4184\n",
      "Epoch [7960/30000], Loss: 0.4183\n",
      "Epoch [7980/30000], Loss: 0.4182\n",
      "Epoch [8000/30000], Loss: 0.4180\n",
      "Epoch [8020/30000], Loss: 0.4179\n",
      "Epoch [8040/30000], Loss: 0.4178\n",
      "Epoch [8060/30000], Loss: 0.4176\n",
      "Epoch [8080/30000], Loss: 0.4175\n",
      "Epoch [8100/30000], Loss: 0.4174\n",
      "Epoch [8120/30000], Loss: 0.4172\n",
      "Epoch [8140/30000], Loss: 0.4171\n",
      "Epoch [8160/30000], Loss: 0.4170\n",
      "Epoch [8180/30000], Loss: 0.4168\n",
      "Epoch [8200/30000], Loss: 0.4167\n",
      "Epoch [8220/30000], Loss: 0.4166\n",
      "Epoch [8240/30000], Loss: 0.4164\n",
      "Epoch [8260/30000], Loss: 0.4163\n",
      "Epoch [8280/30000], Loss: 0.4162\n",
      "Epoch [8300/30000], Loss: 0.4161\n",
      "Epoch [8320/30000], Loss: 0.4159\n",
      "Epoch [8340/30000], Loss: 0.4158\n",
      "Epoch [8360/30000], Loss: 0.4157\n",
      "Epoch [8380/30000], Loss: 0.4156\n",
      "Epoch [8400/30000], Loss: 0.4155\n",
      "Epoch [8420/30000], Loss: 0.4153\n",
      "Epoch [8440/30000], Loss: 0.4152\n",
      "Epoch [8460/30000], Loss: 0.4151\n",
      "Epoch [8480/30000], Loss: 0.4150\n",
      "Epoch [8500/30000], Loss: 0.4149\n",
      "Epoch [8520/30000], Loss: 0.4147\n",
      "Epoch [8540/30000], Loss: 0.4146\n",
      "Epoch [8560/30000], Loss: 0.4145\n",
      "Epoch [8580/30000], Loss: 0.4144\n",
      "Epoch [8600/30000], Loss: 0.4143\n",
      "Epoch [8620/30000], Loss: 0.4142\n",
      "Epoch [8640/30000], Loss: 0.4140\n",
      "Epoch [8660/30000], Loss: 0.4139\n",
      "Epoch [8680/30000], Loss: 0.4138\n",
      "Epoch [8700/30000], Loss: 0.4137\n",
      "Epoch [8720/30000], Loss: 0.4136\n",
      "Epoch [8740/30000], Loss: 0.4135\n",
      "Epoch [8760/30000], Loss: 0.4134\n",
      "Epoch [8780/30000], Loss: 0.4133\n",
      "Epoch [8800/30000], Loss: 0.4132\n",
      "Epoch [8820/30000], Loss: 0.4131\n",
      "Epoch [8840/30000], Loss: 0.4129\n",
      "Epoch [8860/30000], Loss: 0.4128\n",
      "Epoch [8880/30000], Loss: 0.4127\n",
      "Epoch [8900/30000], Loss: 0.4126\n",
      "Epoch [8920/30000], Loss: 0.4125\n",
      "Epoch [8940/30000], Loss: 0.4124\n",
      "Epoch [8960/30000], Loss: 0.4123\n",
      "Epoch [8980/30000], Loss: 0.4122\n",
      "Epoch [9000/30000], Loss: 0.4121\n",
      "Epoch [9020/30000], Loss: 0.4120\n",
      "Epoch [9040/30000], Loss: 0.4119\n",
      "Epoch [9060/30000], Loss: 0.4118\n",
      "Epoch [9080/30000], Loss: 0.4117\n",
      "Epoch [9100/30000], Loss: 0.4116\n",
      "Epoch [9120/30000], Loss: 0.4115\n",
      "Epoch [9140/30000], Loss: 0.4114\n",
      "Epoch [9160/30000], Loss: 0.4113\n",
      "Epoch [9180/30000], Loss: 0.4112\n",
      "Epoch [9200/30000], Loss: 0.4111\n",
      "Epoch [9220/30000], Loss: 0.4110\n",
      "Epoch [9240/30000], Loss: 0.4109\n",
      "Epoch [9260/30000], Loss: 0.4108\n",
      "Epoch [9280/30000], Loss: 0.4107\n",
      "Epoch [9300/30000], Loss: 0.4106\n",
      "Epoch [9320/30000], Loss: 0.4106\n",
      "Epoch [9340/30000], Loss: 0.4105\n",
      "Epoch [9360/30000], Loss: 0.4104\n",
      "Epoch [9380/30000], Loss: 0.4103\n",
      "Epoch [9400/30000], Loss: 0.4102\n",
      "Epoch [9420/30000], Loss: 0.4101\n",
      "Epoch [9440/30000], Loss: 0.4100\n",
      "Epoch [9460/30000], Loss: 0.4099\n",
      "Epoch [9480/30000], Loss: 0.4098\n",
      "Epoch [9500/30000], Loss: 0.4097\n",
      "Epoch [9520/30000], Loss: 0.4096\n",
      "Epoch [9540/30000], Loss: 0.4096\n",
      "Epoch [9560/30000], Loss: 0.4095\n",
      "Epoch [9580/30000], Loss: 0.4094\n",
      "Epoch [9600/30000], Loss: 0.4093\n",
      "Epoch [9620/30000], Loss: 0.4092\n",
      "Epoch [9640/30000], Loss: 0.4091\n",
      "Epoch [9660/30000], Loss: 0.4090\n",
      "Epoch [9680/30000], Loss: 0.4090\n",
      "Epoch [9700/30000], Loss: 0.4089\n",
      "Epoch [9720/30000], Loss: 0.4088\n",
      "Epoch [9740/30000], Loss: 0.4087\n",
      "Epoch [9760/30000], Loss: 0.4086\n",
      "Epoch [9780/30000], Loss: 0.4085\n",
      "Epoch [9800/30000], Loss: 0.4085\n",
      "Epoch [9820/30000], Loss: 0.4084\n",
      "Epoch [9840/30000], Loss: 0.4083\n",
      "Epoch [9860/30000], Loss: 0.4082\n",
      "Epoch [9880/30000], Loss: 0.4081\n",
      "Epoch [9900/30000], Loss: 0.4081\n",
      "Epoch [9920/30000], Loss: 0.4080\n",
      "Epoch [9940/30000], Loss: 0.4079\n",
      "Epoch [9960/30000], Loss: 0.4078\n",
      "Epoch [9980/30000], Loss: 0.4077\n",
      "Epoch [10000/30000], Loss: 0.4077\n",
      "Epoch [10020/30000], Loss: 0.4076\n",
      "Epoch [10040/30000], Loss: 0.4075\n",
      "Epoch [10060/30000], Loss: 0.4074\n",
      "Epoch [10080/30000], Loss: 0.4074\n",
      "Epoch [10100/30000], Loss: 0.4073\n",
      "Epoch [10120/30000], Loss: 0.4072\n",
      "Epoch [10140/30000], Loss: 0.4071\n",
      "Epoch [10160/30000], Loss: 0.4070\n",
      "Epoch [10180/30000], Loss: 0.4070\n",
      "Epoch [10200/30000], Loss: 0.4069\n",
      "Epoch [10220/30000], Loss: 0.4068\n",
      "Epoch [10240/30000], Loss: 0.4068\n",
      "Epoch [10260/30000], Loss: 0.4067\n",
      "Epoch [10280/30000], Loss: 0.4066\n",
      "Epoch [10300/30000], Loss: 0.4065\n",
      "Epoch [10320/30000], Loss: 0.4065\n",
      "Epoch [10340/30000], Loss: 0.4064\n",
      "Epoch [10360/30000], Loss: 0.4063\n",
      "Epoch [10380/30000], Loss: 0.4062\n",
      "Epoch [10400/30000], Loss: 0.4062\n",
      "Epoch [10420/30000], Loss: 0.4061\n",
      "Epoch [10440/30000], Loss: 0.4060\n",
      "Epoch [10460/30000], Loss: 0.4060\n",
      "Epoch [10480/30000], Loss: 0.4059\n",
      "Epoch [10500/30000], Loss: 0.4058\n",
      "Epoch [10520/30000], Loss: 0.4058\n",
      "Epoch [10540/30000], Loss: 0.4057\n",
      "Epoch [10560/30000], Loss: 0.4056\n",
      "Epoch [10580/30000], Loss: 0.4056\n",
      "Epoch [10600/30000], Loss: 0.4055\n",
      "Epoch [10620/30000], Loss: 0.4054\n",
      "Epoch [10640/30000], Loss: 0.4054\n",
      "Epoch [10660/30000], Loss: 0.4053\n",
      "Epoch [10680/30000], Loss: 0.4052\n",
      "Epoch [10700/30000], Loss: 0.4052\n",
      "Epoch [10720/30000], Loss: 0.4051\n",
      "Epoch [10740/30000], Loss: 0.4050\n",
      "Epoch [10760/30000], Loss: 0.4050\n",
      "Epoch [10780/30000], Loss: 0.4049\n",
      "Epoch [10800/30000], Loss: 0.4048\n",
      "Epoch [10820/30000], Loss: 0.4048\n",
      "Epoch [10840/30000], Loss: 0.4047\n",
      "Epoch [10860/30000], Loss: 0.4046\n",
      "Epoch [10880/30000], Loss: 0.4046\n",
      "Epoch [10900/30000], Loss: 0.4045\n",
      "Epoch [10920/30000], Loss: 0.4045\n",
      "Epoch [10940/30000], Loss: 0.4044\n",
      "Epoch [10960/30000], Loss: 0.4043\n",
      "Epoch [10980/30000], Loss: 0.4043\n",
      "Epoch [11000/30000], Loss: 0.4042\n",
      "Epoch [11020/30000], Loss: 0.4041\n",
      "Epoch [11040/30000], Loss: 0.4041\n",
      "Epoch [11060/30000], Loss: 0.4040\n",
      "Epoch [11080/30000], Loss: 0.4040\n",
      "Epoch [11100/30000], Loss: 0.4039\n",
      "Epoch [11120/30000], Loss: 0.4038\n",
      "Epoch [11140/30000], Loss: 0.4038\n",
      "Epoch [11160/30000], Loss: 0.4037\n",
      "Epoch [11180/30000], Loss: 0.4037\n",
      "Epoch [11200/30000], Loss: 0.4036\n",
      "Epoch [11220/30000], Loss: 0.4035\n",
      "Epoch [11240/30000], Loss: 0.4035\n",
      "Epoch [11260/30000], Loss: 0.4034\n",
      "Epoch [11280/30000], Loss: 0.4034\n",
      "Epoch [11300/30000], Loss: 0.4033\n",
      "Epoch [11320/30000], Loss: 0.4033\n",
      "Epoch [11340/30000], Loss: 0.4032\n",
      "Epoch [11360/30000], Loss: 0.4031\n",
      "Epoch [11380/30000], Loss: 0.4031\n",
      "Epoch [11400/30000], Loss: 0.4030\n",
      "Epoch [11420/30000], Loss: 0.4030\n",
      "Epoch [11440/30000], Loss: 0.4029\n",
      "Epoch [11460/30000], Loss: 0.4029\n",
      "Epoch [11480/30000], Loss: 0.4028\n",
      "Epoch [11500/30000], Loss: 0.4027\n",
      "Epoch [11520/30000], Loss: 0.4027\n",
      "Epoch [11540/30000], Loss: 0.4026\n",
      "Epoch [11560/30000], Loss: 0.4026\n",
      "Epoch [11580/30000], Loss: 0.4025\n",
      "Epoch [11600/30000], Loss: 0.4025\n",
      "Epoch [11620/30000], Loss: 0.4024\n",
      "Epoch [11640/30000], Loss: 0.4024\n",
      "Epoch [11660/30000], Loss: 0.4023\n",
      "Epoch [11680/30000], Loss: 0.4023\n",
      "Epoch [11700/30000], Loss: 0.4022\n",
      "Epoch [11720/30000], Loss: 0.4022\n",
      "Epoch [11740/30000], Loss: 0.4021\n",
      "Epoch [11760/30000], Loss: 0.4021\n",
      "Epoch [11780/30000], Loss: 0.4020\n",
      "Epoch [11800/30000], Loss: 0.4020\n",
      "Epoch [11820/30000], Loss: 0.4019\n",
      "Epoch [11840/30000], Loss: 0.4018\n",
      "Epoch [11860/30000], Loss: 0.4018\n",
      "Epoch [11880/30000], Loss: 0.4017\n",
      "Epoch [11900/30000], Loss: 0.4017\n",
      "Epoch [11920/30000], Loss: 0.4016\n",
      "Epoch [11940/30000], Loss: 0.4016\n",
      "Epoch [11960/30000], Loss: 0.4015\n",
      "Epoch [11980/30000], Loss: 0.4015\n",
      "Epoch [12000/30000], Loss: 0.4014\n",
      "Epoch [12020/30000], Loss: 0.4014\n",
      "Epoch [12040/30000], Loss: 0.4013\n",
      "Epoch [12060/30000], Loss: 0.4013\n",
      "Epoch [12080/30000], Loss: 0.4012\n",
      "Epoch [12100/30000], Loss: 0.4012\n",
      "Epoch [12120/30000], Loss: 0.4012\n",
      "Epoch [12140/30000], Loss: 0.4011\n",
      "Epoch [12160/30000], Loss: 0.4011\n",
      "Epoch [12180/30000], Loss: 0.4010\n",
      "Epoch [12200/30000], Loss: 0.4010\n",
      "Epoch [12220/30000], Loss: 0.4009\n",
      "Epoch [12240/30000], Loss: 0.4009\n",
      "Epoch [12260/30000], Loss: 0.4008\n",
      "Epoch [12280/30000], Loss: 0.4008\n",
      "Epoch [12300/30000], Loss: 0.4007\n",
      "Epoch [12320/30000], Loss: 0.4007\n",
      "Epoch [12340/30000], Loss: 0.4006\n",
      "Epoch [12360/30000], Loss: 0.4006\n",
      "Epoch [12380/30000], Loss: 0.4005\n",
      "Epoch [12400/30000], Loss: 0.4005\n",
      "Epoch [12420/30000], Loss: 0.4004\n",
      "Epoch [12440/30000], Loss: 0.4004\n",
      "Epoch [12460/30000], Loss: 0.4003\n",
      "Epoch [12480/30000], Loss: 0.4003\n",
      "Epoch [12500/30000], Loss: 0.4003\n",
      "Epoch [12520/30000], Loss: 0.4002\n",
      "Epoch [12540/30000], Loss: 0.4002\n",
      "Epoch [12560/30000], Loss: 0.4001\n",
      "Epoch [12580/30000], Loss: 0.4001\n",
      "Epoch [12600/30000], Loss: 0.4000\n",
      "Epoch [12620/30000], Loss: 0.4000\n",
      "Epoch [12640/30000], Loss: 0.3999\n",
      "Epoch [12660/30000], Loss: 0.3999\n",
      "Epoch [12680/30000], Loss: 0.3999\n",
      "Epoch [12700/30000], Loss: 0.3998\n",
      "Epoch [12720/30000], Loss: 0.3998\n",
      "Epoch [12740/30000], Loss: 0.3997\n",
      "Epoch [12760/30000], Loss: 0.3997\n",
      "Epoch [12780/30000], Loss: 0.3996\n",
      "Epoch [12800/30000], Loss: 0.3996\n",
      "Epoch [12820/30000], Loss: 0.3996\n",
      "Epoch [12840/30000], Loss: 0.3995\n",
      "Epoch [12860/30000], Loss: 0.3995\n",
      "Epoch [12880/30000], Loss: 0.3994\n",
      "Epoch [12900/30000], Loss: 0.3994\n",
      "Epoch [12920/30000], Loss: 0.3993\n",
      "Epoch [12940/30000], Loss: 0.3993\n",
      "Epoch [12960/30000], Loss: 0.3993\n",
      "Epoch [12980/30000], Loss: 0.3992\n",
      "Epoch [13000/30000], Loss: 0.3992\n",
      "Epoch [13020/30000], Loss: 0.3991\n",
      "Epoch [13040/30000], Loss: 0.3991\n",
      "Epoch [13060/30000], Loss: 0.3991\n",
      "Epoch [13080/30000], Loss: 0.3990\n",
      "Epoch [13100/30000], Loss: 0.3990\n",
      "Epoch [13120/30000], Loss: 0.3990\n",
      "Epoch [13140/30000], Loss: 0.3989\n",
      "Epoch [13160/30000], Loss: 0.3989\n",
      "Epoch [13180/30000], Loss: 0.3988\n",
      "Epoch [13200/30000], Loss: 0.3988\n",
      "Epoch [13220/30000], Loss: 0.3988\n",
      "Epoch [13240/30000], Loss: 0.3987\n",
      "Epoch [13260/30000], Loss: 0.3987\n",
      "Epoch [13280/30000], Loss: 0.3986\n",
      "Epoch [13300/30000], Loss: 0.3986\n",
      "Epoch [13320/30000], Loss: 0.3986\n",
      "Epoch [13340/30000], Loss: 0.3985\n",
      "Epoch [13360/30000], Loss: 0.3985\n",
      "Epoch [13380/30000], Loss: 0.3984\n",
      "Epoch [13400/30000], Loss: 0.3984\n",
      "Epoch [13420/30000], Loss: 0.3984\n",
      "Epoch [13440/30000], Loss: 0.3983\n",
      "Epoch [13460/30000], Loss: 0.3983\n",
      "Epoch [13480/30000], Loss: 0.3983\n",
      "Epoch [13500/30000], Loss: 0.3982\n",
      "Epoch [13520/30000], Loss: 0.3982\n",
      "Epoch [13540/30000], Loss: 0.3982\n",
      "Epoch [13560/30000], Loss: 0.3981\n",
      "Epoch [13580/30000], Loss: 0.3981\n",
      "Epoch [13600/30000], Loss: 0.3980\n",
      "Epoch [13620/30000], Loss: 0.3980\n",
      "Epoch [13640/30000], Loss: 0.3980\n",
      "Epoch [13660/30000], Loss: 0.3979\n",
      "Epoch [13680/30000], Loss: 0.3979\n",
      "Epoch [13700/30000], Loss: 0.3979\n",
      "Epoch [13720/30000], Loss: 0.3978\n",
      "Epoch [13740/30000], Loss: 0.3978\n",
      "Epoch [13760/30000], Loss: 0.3978\n",
      "Epoch [13780/30000], Loss: 0.3977\n",
      "Epoch [13800/30000], Loss: 0.3977\n",
      "Epoch [13820/30000], Loss: 0.3976\n",
      "Epoch [13840/30000], Loss: 0.3976\n",
      "Epoch [13860/30000], Loss: 0.3976\n",
      "Epoch [13880/30000], Loss: 0.3975\n",
      "Epoch [13900/30000], Loss: 0.3975\n",
      "Epoch [13920/30000], Loss: 0.3975\n",
      "Epoch [13940/30000], Loss: 0.3974\n",
      "Epoch [13960/30000], Loss: 0.3974\n",
      "Epoch [13980/30000], Loss: 0.3974\n",
      "Epoch [14000/30000], Loss: 0.3973\n",
      "Epoch [14020/30000], Loss: 0.3973\n",
      "Epoch [14040/30000], Loss: 0.3973\n",
      "Epoch [14060/30000], Loss: 0.3972\n",
      "Epoch [14080/30000], Loss: 0.3972\n",
      "Epoch [14100/30000], Loss: 0.3972\n",
      "Epoch [14120/30000], Loss: 0.3971\n",
      "Epoch [14140/30000], Loss: 0.3971\n",
      "Epoch [14160/30000], Loss: 0.3971\n",
      "Epoch [14180/30000], Loss: 0.3970\n",
      "Epoch [14200/30000], Loss: 0.3970\n",
      "Epoch [14220/30000], Loss: 0.3970\n",
      "Epoch [14240/30000], Loss: 0.3969\n",
      "Epoch [14260/30000], Loss: 0.3969\n",
      "Epoch [14280/30000], Loss: 0.3969\n",
      "Epoch [14300/30000], Loss: 0.3968\n",
      "Epoch [14320/30000], Loss: 0.3968\n",
      "Epoch [14340/30000], Loss: 0.3968\n",
      "Epoch [14360/30000], Loss: 0.3967\n",
      "Epoch [14380/30000], Loss: 0.3967\n",
      "Epoch [14400/30000], Loss: 0.3967\n",
      "Epoch [14420/30000], Loss: 0.3966\n",
      "Epoch [14440/30000], Loss: 0.3966\n",
      "Epoch [14460/30000], Loss: 0.3966\n",
      "Epoch [14480/30000], Loss: 0.3965\n",
      "Epoch [14500/30000], Loss: 0.3965\n",
      "Epoch [14520/30000], Loss: 0.3965\n",
      "Epoch [14540/30000], Loss: 0.3964\n",
      "Epoch [14560/30000], Loss: 0.3964\n",
      "Epoch [14580/30000], Loss: 0.3964\n",
      "Epoch [14600/30000], Loss: 0.3964\n",
      "Epoch [14620/30000], Loss: 0.3963\n",
      "Epoch [14640/30000], Loss: 0.3963\n",
      "Epoch [14660/30000], Loss: 0.3963\n",
      "Epoch [14680/30000], Loss: 0.3962\n",
      "Epoch [14700/30000], Loss: 0.3962\n",
      "Epoch [14720/30000], Loss: 0.3962\n",
      "Epoch [14740/30000], Loss: 0.3961\n",
      "Epoch [14760/30000], Loss: 0.3961\n",
      "Epoch [14780/30000], Loss: 0.3961\n",
      "Epoch [14800/30000], Loss: 0.3960\n",
      "Epoch [14820/30000], Loss: 0.3960\n",
      "Epoch [14840/30000], Loss: 0.3960\n",
      "Epoch [14860/30000], Loss: 0.3960\n",
      "Epoch [14880/30000], Loss: 0.3959\n",
      "Epoch [14900/30000], Loss: 0.3959\n",
      "Epoch [14920/30000], Loss: 0.3959\n",
      "Epoch [14940/30000], Loss: 0.3958\n",
      "Epoch [14960/30000], Loss: 0.3958\n",
      "Epoch [14980/30000], Loss: 0.3958\n",
      "Epoch [15000/30000], Loss: 0.3957\n",
      "Epoch [15020/30000], Loss: 0.3957\n",
      "Epoch [15040/30000], Loss: 0.3957\n",
      "Epoch [15060/30000], Loss: 0.3957\n",
      "Epoch [15080/30000], Loss: 0.3956\n",
      "Epoch [15100/30000], Loss: 0.3956\n",
      "Epoch [15120/30000], Loss: 0.3956\n",
      "Epoch [15140/30000], Loss: 0.3955\n",
      "Epoch [15160/30000], Loss: 0.3955\n",
      "Epoch [15180/30000], Loss: 0.3955\n",
      "Epoch [15200/30000], Loss: 0.3955\n",
      "Epoch [15220/30000], Loss: 0.3954\n",
      "Epoch [15240/30000], Loss: 0.3954\n",
      "Epoch [15260/30000], Loss: 0.3954\n",
      "Epoch [15280/30000], Loss: 0.3953\n",
      "Epoch [15300/30000], Loss: 0.3953\n",
      "Epoch [15320/30000], Loss: 0.3953\n",
      "Epoch [15340/30000], Loss: 0.3953\n",
      "Epoch [15360/30000], Loss: 0.3952\n",
      "Epoch [15380/30000], Loss: 0.3952\n",
      "Epoch [15400/30000], Loss: 0.3952\n",
      "Epoch [15420/30000], Loss: 0.3951\n",
      "Epoch [15440/30000], Loss: 0.3951\n",
      "Epoch [15460/30000], Loss: 0.3951\n",
      "Epoch [15480/30000], Loss: 0.3951\n",
      "Epoch [15500/30000], Loss: 0.3950\n",
      "Epoch [15520/30000], Loss: 0.3950\n",
      "Epoch [15540/30000], Loss: 0.3950\n",
      "Epoch [15560/30000], Loss: 0.3950\n",
      "Epoch [15580/30000], Loss: 0.3949\n",
      "Epoch [15600/30000], Loss: 0.3949\n",
      "Epoch [15620/30000], Loss: 0.3949\n",
      "Epoch [15640/30000], Loss: 0.3948\n",
      "Epoch [15660/30000], Loss: 0.3948\n",
      "Epoch [15680/30000], Loss: 0.3948\n",
      "Epoch [15700/30000], Loss: 0.3948\n",
      "Epoch [15720/30000], Loss: 0.3947\n",
      "Epoch [15740/30000], Loss: 0.3947\n",
      "Epoch [15760/30000], Loss: 0.3947\n",
      "Epoch [15780/30000], Loss: 0.3947\n",
      "Epoch [15800/30000], Loss: 0.3946\n",
      "Epoch [15820/30000], Loss: 0.3946\n",
      "Epoch [15840/30000], Loss: 0.3946\n",
      "Epoch [15860/30000], Loss: 0.3946\n",
      "Epoch [15880/30000], Loss: 0.3945\n",
      "Epoch [15900/30000], Loss: 0.3945\n",
      "Epoch [15920/30000], Loss: 0.3945\n",
      "Epoch [15940/30000], Loss: 0.3945\n",
      "Epoch [15960/30000], Loss: 0.3944\n",
      "Epoch [15980/30000], Loss: 0.3944\n",
      "Epoch [16000/30000], Loss: 0.3944\n",
      "Epoch [16020/30000], Loss: 0.3944\n",
      "Epoch [16040/30000], Loss: 0.3943\n",
      "Epoch [16060/30000], Loss: 0.3943\n",
      "Epoch [16080/30000], Loss: 0.3943\n",
      "Epoch [16100/30000], Loss: 0.3943\n",
      "Epoch [16120/30000], Loss: 0.3942\n",
      "Epoch [16140/30000], Loss: 0.3942\n",
      "Epoch [16160/30000], Loss: 0.3942\n",
      "Epoch [16180/30000], Loss: 0.3942\n",
      "Epoch [16200/30000], Loss: 0.3941\n",
      "Epoch [16220/30000], Loss: 0.3941\n",
      "Epoch [16240/30000], Loss: 0.3941\n",
      "Epoch [16260/30000], Loss: 0.3941\n",
      "Epoch [16280/30000], Loss: 0.3940\n",
      "Epoch [16300/30000], Loss: 0.3940\n",
      "Epoch [16320/30000], Loss: 0.3940\n",
      "Epoch [16340/30000], Loss: 0.3940\n",
      "Epoch [16360/30000], Loss: 0.3939\n",
      "Epoch [16380/30000], Loss: 0.3939\n",
      "Epoch [16400/30000], Loss: 0.3939\n",
      "Epoch [16420/30000], Loss: 0.3939\n",
      "Epoch [16440/30000], Loss: 0.3938\n",
      "Epoch [16460/30000], Loss: 0.3938\n",
      "Epoch [16480/30000], Loss: 0.3938\n",
      "Epoch [16500/30000], Loss: 0.3938\n",
      "Epoch [16520/30000], Loss: 0.3937\n",
      "Epoch [16540/30000], Loss: 0.3937\n",
      "Epoch [16560/30000], Loss: 0.3937\n",
      "Epoch [16580/30000], Loss: 0.3937\n",
      "Epoch [16600/30000], Loss: 0.3937\n",
      "Epoch [16620/30000], Loss: 0.3936\n",
      "Epoch [16640/30000], Loss: 0.3936\n",
      "Epoch [16660/30000], Loss: 0.3936\n",
      "Epoch [16680/30000], Loss: 0.3936\n",
      "Epoch [16700/30000], Loss: 0.3935\n",
      "Epoch [16720/30000], Loss: 0.3935\n",
      "Epoch [16740/30000], Loss: 0.3935\n",
      "Epoch [16760/30000], Loss: 0.3935\n",
      "Epoch [16780/30000], Loss: 0.3934\n",
      "Epoch [16800/30000], Loss: 0.3934\n",
      "Epoch [16820/30000], Loss: 0.3934\n",
      "Epoch [16840/30000], Loss: 0.3934\n",
      "Epoch [16860/30000], Loss: 0.3934\n",
      "Epoch [16880/30000], Loss: 0.3933\n",
      "Epoch [16900/30000], Loss: 0.3933\n",
      "Epoch [16920/30000], Loss: 0.3933\n",
      "Epoch [16940/30000], Loss: 0.3933\n",
      "Epoch [16960/30000], Loss: 0.3932\n",
      "Epoch [16980/30000], Loss: 0.3932\n",
      "Epoch [17000/30000], Loss: 0.3932\n",
      "Epoch [17020/30000], Loss: 0.3932\n",
      "Epoch [17040/30000], Loss: 0.3932\n",
      "Epoch [17060/30000], Loss: 0.3931\n",
      "Epoch [17080/30000], Loss: 0.3931\n",
      "Epoch [17100/30000], Loss: 0.3931\n",
      "Epoch [17120/30000], Loss: 0.3931\n",
      "Epoch [17140/30000], Loss: 0.3930\n",
      "Epoch [17160/30000], Loss: 0.3930\n",
      "Epoch [17180/30000], Loss: 0.3930\n",
      "Epoch [17200/30000], Loss: 0.3930\n",
      "Epoch [17220/30000], Loss: 0.3930\n",
      "Epoch [17240/30000], Loss: 0.3929\n",
      "Epoch [17260/30000], Loss: 0.3929\n",
      "Epoch [17280/30000], Loss: 0.3929\n",
      "Epoch [17300/30000], Loss: 0.3929\n",
      "Epoch [17320/30000], Loss: 0.3929\n",
      "Epoch [17340/30000], Loss: 0.3928\n",
      "Epoch [17360/30000], Loss: 0.3928\n",
      "Epoch [17380/30000], Loss: 0.3928\n",
      "Epoch [17400/30000], Loss: 0.3928\n",
      "Epoch [17420/30000], Loss: 0.3928\n",
      "Epoch [17440/30000], Loss: 0.3927\n",
      "Epoch [17460/30000], Loss: 0.3927\n",
      "Epoch [17480/30000], Loss: 0.3927\n",
      "Epoch [17500/30000], Loss: 0.3927\n",
      "Epoch [17520/30000], Loss: 0.3926\n",
      "Epoch [17540/30000], Loss: 0.3926\n",
      "Epoch [17560/30000], Loss: 0.3926\n",
      "Epoch [17580/30000], Loss: 0.3926\n",
      "Epoch [17600/30000], Loss: 0.3926\n",
      "Epoch [17620/30000], Loss: 0.3925\n",
      "Epoch [17640/30000], Loss: 0.3925\n",
      "Epoch [17660/30000], Loss: 0.3925\n",
      "Epoch [17680/30000], Loss: 0.3925\n",
      "Epoch [17700/30000], Loss: 0.3925\n",
      "Epoch [17720/30000], Loss: 0.3924\n",
      "Epoch [17740/30000], Loss: 0.3924\n",
      "Epoch [17760/30000], Loss: 0.3924\n",
      "Epoch [17780/30000], Loss: 0.3924\n",
      "Epoch [17800/30000], Loss: 0.3924\n",
      "Epoch [17820/30000], Loss: 0.3923\n",
      "Epoch [17840/30000], Loss: 0.3923\n",
      "Epoch [17860/30000], Loss: 0.3923\n",
      "Epoch [17880/30000], Loss: 0.3923\n",
      "Epoch [17900/30000], Loss: 0.3923\n",
      "Epoch [17920/30000], Loss: 0.3922\n",
      "Epoch [17940/30000], Loss: 0.3922\n",
      "Epoch [17960/30000], Loss: 0.3922\n",
      "Epoch [17980/30000], Loss: 0.3922\n",
      "Epoch [18000/30000], Loss: 0.3922\n",
      "Epoch [18020/30000], Loss: 0.3921\n",
      "Epoch [18040/30000], Loss: 0.3921\n",
      "Epoch [18060/30000], Loss: 0.3921\n",
      "Epoch [18080/30000], Loss: 0.3921\n",
      "Epoch [18100/30000], Loss: 0.3921\n",
      "Epoch [18120/30000], Loss: 0.3920\n",
      "Epoch [18140/30000], Loss: 0.3920\n",
      "Epoch [18160/30000], Loss: 0.3920\n",
      "Epoch [18180/30000], Loss: 0.3920\n",
      "Epoch [18200/30000], Loss: 0.3920\n",
      "Epoch [18220/30000], Loss: 0.3919\n",
      "Epoch [18240/30000], Loss: 0.3919\n",
      "Epoch [18260/30000], Loss: 0.3919\n",
      "Epoch [18280/30000], Loss: 0.3919\n",
      "Epoch [18300/30000], Loss: 0.3919\n",
      "Epoch [18320/30000], Loss: 0.3918\n",
      "Epoch [18340/30000], Loss: 0.3918\n",
      "Epoch [18360/30000], Loss: 0.3918\n",
      "Epoch [18380/30000], Loss: 0.3918\n",
      "Epoch [18400/30000], Loss: 0.3918\n",
      "Epoch [18420/30000], Loss: 0.3918\n",
      "Epoch [18440/30000], Loss: 0.3917\n",
      "Epoch [18460/30000], Loss: 0.3917\n",
      "Epoch [18480/30000], Loss: 0.3917\n",
      "Epoch [18500/30000], Loss: 0.3917\n",
      "Epoch [18520/30000], Loss: 0.3917\n",
      "Epoch [18540/30000], Loss: 0.3916\n",
      "Epoch [18560/30000], Loss: 0.3916\n",
      "Epoch [18580/30000], Loss: 0.3916\n",
      "Epoch [18600/30000], Loss: 0.3916\n",
      "Epoch [18620/30000], Loss: 0.3916\n",
      "Epoch [18640/30000], Loss: 0.3916\n",
      "Epoch [18660/30000], Loss: 0.3915\n",
      "Epoch [18680/30000], Loss: 0.3915\n",
      "Epoch [18700/30000], Loss: 0.3915\n",
      "Epoch [18720/30000], Loss: 0.3915\n",
      "Epoch [18740/30000], Loss: 0.3915\n",
      "Epoch [18760/30000], Loss: 0.3914\n",
      "Epoch [18780/30000], Loss: 0.3914\n",
      "Epoch [18800/30000], Loss: 0.3914\n",
      "Epoch [18820/30000], Loss: 0.3914\n",
      "Epoch [18840/30000], Loss: 0.3914\n",
      "Epoch [18860/30000], Loss: 0.3914\n",
      "Epoch [18880/30000], Loss: 0.3913\n",
      "Epoch [18900/30000], Loss: 0.3913\n",
      "Epoch [18920/30000], Loss: 0.3913\n",
      "Epoch [18940/30000], Loss: 0.3913\n",
      "Epoch [18960/30000], Loss: 0.3913\n",
      "Epoch [18980/30000], Loss: 0.3912\n",
      "Epoch [19000/30000], Loss: 0.3912\n",
      "Epoch [19020/30000], Loss: 0.3912\n",
      "Epoch [19040/30000], Loss: 0.3912\n",
      "Epoch [19060/30000], Loss: 0.3912\n",
      "Epoch [19080/30000], Loss: 0.3912\n",
      "Epoch [19100/30000], Loss: 0.3911\n",
      "Epoch [19120/30000], Loss: 0.3911\n",
      "Epoch [19140/30000], Loss: 0.3911\n",
      "Epoch [19160/30000], Loss: 0.3911\n",
      "Epoch [19180/30000], Loss: 0.3911\n",
      "Epoch [19200/30000], Loss: 0.3911\n",
      "Epoch [19220/30000], Loss: 0.3910\n",
      "Epoch [19240/30000], Loss: 0.3910\n",
      "Epoch [19260/30000], Loss: 0.3910\n",
      "Epoch [19280/30000], Loss: 0.3910\n",
      "Epoch [19300/30000], Loss: 0.3910\n",
      "Epoch [19320/30000], Loss: 0.3910\n",
      "Epoch [19340/30000], Loss: 0.3909\n",
      "Epoch [19360/30000], Loss: 0.3909\n",
      "Epoch [19380/30000], Loss: 0.3909\n",
      "Epoch [19400/30000], Loss: 0.3909\n",
      "Epoch [19420/30000], Loss: 0.3909\n",
      "Epoch [19440/30000], Loss: 0.3909\n",
      "Epoch [19460/30000], Loss: 0.3908\n",
      "Epoch [19480/30000], Loss: 0.3908\n",
      "Epoch [19500/30000], Loss: 0.3908\n",
      "Epoch [19520/30000], Loss: 0.3908\n",
      "Epoch [19540/30000], Loss: 0.3908\n",
      "Epoch [19560/30000], Loss: 0.3908\n",
      "Epoch [19580/30000], Loss: 0.3907\n",
      "Epoch [19600/30000], Loss: 0.3907\n",
      "Epoch [19620/30000], Loss: 0.3907\n",
      "Epoch [19640/30000], Loss: 0.3907\n",
      "Epoch [19660/30000], Loss: 0.3907\n",
      "Epoch [19680/30000], Loss: 0.3907\n",
      "Epoch [19700/30000], Loss: 0.3906\n",
      "Epoch [19720/30000], Loss: 0.3906\n",
      "Epoch [19740/30000], Loss: 0.3906\n",
      "Epoch [19760/30000], Loss: 0.3906\n",
      "Epoch [19780/30000], Loss: 0.3906\n",
      "Epoch [19800/30000], Loss: 0.3906\n",
      "Epoch [19820/30000], Loss: 0.3905\n",
      "Epoch [19840/30000], Loss: 0.3905\n",
      "Epoch [19860/30000], Loss: 0.3905\n",
      "Epoch [19880/30000], Loss: 0.3905\n",
      "Epoch [19900/30000], Loss: 0.3905\n",
      "Epoch [19920/30000], Loss: 0.3905\n",
      "Epoch [19940/30000], Loss: 0.3905\n",
      "Epoch [19960/30000], Loss: 0.3904\n",
      "Epoch [19980/30000], Loss: 0.3904\n",
      "Epoch [20000/30000], Loss: 0.3904\n",
      "Epoch [20020/30000], Loss: 0.3904\n",
      "Epoch [20040/30000], Loss: 0.3904\n",
      "Epoch [20060/30000], Loss: 0.3904\n",
      "Epoch [20080/30000], Loss: 0.3903\n",
      "Epoch [20100/30000], Loss: 0.3903\n",
      "Epoch [20120/30000], Loss: 0.3903\n",
      "Epoch [20140/30000], Loss: 0.3903\n",
      "Epoch [20160/30000], Loss: 0.3903\n",
      "Epoch [20180/30000], Loss: 0.3903\n",
      "Epoch [20200/30000], Loss: 0.3903\n",
      "Epoch [20220/30000], Loss: 0.3902\n",
      "Epoch [20240/30000], Loss: 0.3902\n",
      "Epoch [20260/30000], Loss: 0.3902\n",
      "Epoch [20280/30000], Loss: 0.3902\n",
      "Epoch [20300/30000], Loss: 0.3902\n",
      "Epoch [20320/30000], Loss: 0.3902\n",
      "Epoch [20340/30000], Loss: 0.3902\n",
      "Epoch [20360/30000], Loss: 0.3901\n",
      "Epoch [20380/30000], Loss: 0.3901\n",
      "Epoch [20400/30000], Loss: 0.3901\n",
      "Epoch [20420/30000], Loss: 0.3901\n",
      "Epoch [20440/30000], Loss: 0.3901\n",
      "Epoch [20460/30000], Loss: 0.3901\n",
      "Epoch [20480/30000], Loss: 0.3901\n",
      "Epoch [20500/30000], Loss: 0.3900\n",
      "Epoch [20520/30000], Loss: 0.3900\n",
      "Epoch [20540/30000], Loss: 0.3900\n",
      "Epoch [20560/30000], Loss: 0.3900\n",
      "Epoch [20580/30000], Loss: 0.3900\n",
      "Epoch [20600/30000], Loss: 0.3900\n",
      "Epoch [20620/30000], Loss: 0.3900\n",
      "Epoch [20640/30000], Loss: 0.3899\n",
      "Epoch [20660/30000], Loss: 0.3899\n",
      "Epoch [20680/30000], Loss: 0.3899\n",
      "Epoch [20700/30000], Loss: 0.3899\n",
      "Epoch [20720/30000], Loss: 0.3899\n",
      "Epoch [20740/30000], Loss: 0.3899\n",
      "Epoch [20760/30000], Loss: 0.3898\n",
      "Epoch [20780/30000], Loss: 0.3898\n",
      "Epoch [20800/30000], Loss: 0.3898\n",
      "Epoch [20820/30000], Loss: 0.3898\n",
      "Epoch [20840/30000], Loss: 0.3898\n",
      "Epoch [20860/30000], Loss: 0.3898\n",
      "Epoch [20880/30000], Loss: 0.3898\n",
      "Epoch [20900/30000], Loss: 0.3897\n",
      "Epoch [20920/30000], Loss: 0.3897\n",
      "Epoch [20940/30000], Loss: 0.3897\n",
      "Epoch [20960/30000], Loss: 0.3897\n",
      "Epoch [20980/30000], Loss: 0.3897\n",
      "Epoch [21000/30000], Loss: 0.3897\n",
      "Epoch [21020/30000], Loss: 0.3897\n",
      "Epoch [21040/30000], Loss: 0.3897\n",
      "Epoch [21060/30000], Loss: 0.3896\n",
      "Epoch [21080/30000], Loss: 0.3896\n",
      "Epoch [21100/30000], Loss: 0.3896\n",
      "Epoch [21120/30000], Loss: 0.3896\n",
      "Epoch [21140/30000], Loss: 0.3896\n",
      "Epoch [21160/30000], Loss: 0.3896\n",
      "Epoch [21180/30000], Loss: 0.3896\n",
      "Epoch [21200/30000], Loss: 0.3895\n",
      "Epoch [21220/30000], Loss: 0.3895\n",
      "Epoch [21240/30000], Loss: 0.3895\n",
      "Epoch [21260/30000], Loss: 0.3895\n",
      "Epoch [21280/30000], Loss: 0.3895\n",
      "Epoch [21300/30000], Loss: 0.3895\n",
      "Epoch [21320/30000], Loss: 0.3895\n",
      "Epoch [21340/30000], Loss: 0.3894\n",
      "Epoch [21360/30000], Loss: 0.3894\n",
      "Epoch [21380/30000], Loss: 0.3894\n",
      "Epoch [21400/30000], Loss: 0.3894\n",
      "Epoch [21420/30000], Loss: 0.3894\n",
      "Epoch [21440/30000], Loss: 0.3894\n",
      "Epoch [21460/30000], Loss: 0.3894\n",
      "Epoch [21480/30000], Loss: 0.3893\n",
      "Epoch [21500/30000], Loss: 0.3893\n",
      "Epoch [21520/30000], Loss: 0.3893\n",
      "Epoch [21540/30000], Loss: 0.3893\n",
      "Epoch [21560/30000], Loss: 0.3893\n",
      "Epoch [21580/30000], Loss: 0.3893\n",
      "Epoch [21600/30000], Loss: 0.3893\n",
      "Epoch [21620/30000], Loss: 0.3893\n",
      "Epoch [21640/30000], Loss: 0.3892\n",
      "Epoch [21660/30000], Loss: 0.3892\n",
      "Epoch [21680/30000], Loss: 0.3892\n",
      "Epoch [21700/30000], Loss: 0.3892\n",
      "Epoch [21720/30000], Loss: 0.3892\n",
      "Epoch [21740/30000], Loss: 0.3892\n",
      "Epoch [21760/30000], Loss: 0.3892\n",
      "Epoch [21780/30000], Loss: 0.3891\n",
      "Epoch [21800/30000], Loss: 0.3891\n",
      "Epoch [21820/30000], Loss: 0.3891\n",
      "Epoch [21840/30000], Loss: 0.3891\n",
      "Epoch [21860/30000], Loss: 0.3891\n",
      "Epoch [21880/30000], Loss: 0.3891\n",
      "Epoch [21900/30000], Loss: 0.3891\n",
      "Epoch [21920/30000], Loss: 0.3890\n",
      "Epoch [21940/30000], Loss: 0.3890\n",
      "Epoch [21960/30000], Loss: 0.3890\n",
      "Epoch [21980/30000], Loss: 0.3890\n",
      "Epoch [22000/30000], Loss: 0.3890\n",
      "Epoch [22020/30000], Loss: 0.3890\n",
      "Epoch [22040/30000], Loss: 0.3890\n",
      "Epoch [22060/30000], Loss: 0.3890\n",
      "Epoch [22080/30000], Loss: 0.3889\n",
      "Epoch [22100/30000], Loss: 0.3889\n",
      "Epoch [22120/30000], Loss: 0.3889\n",
      "Epoch [22140/30000], Loss: 0.3889\n",
      "Epoch [22160/30000], Loss: 0.3889\n",
      "Epoch [22180/30000], Loss: 0.3889\n",
      "Epoch [22200/30000], Loss: 0.3889\n",
      "Epoch [22220/30000], Loss: 0.3888\n",
      "Epoch [22240/30000], Loss: 0.3888\n",
      "Epoch [22260/30000], Loss: 0.3888\n",
      "Epoch [22280/30000], Loss: 0.3888\n",
      "Epoch [22300/30000], Loss: 0.3888\n",
      "Epoch [22320/30000], Loss: 0.3888\n",
      "Epoch [22340/30000], Loss: 0.3888\n",
      "Epoch [22360/30000], Loss: 0.3888\n",
      "Epoch [22380/30000], Loss: 0.3887\n",
      "Epoch [22400/30000], Loss: 0.3887\n",
      "Epoch [22420/30000], Loss: 0.3887\n",
      "Epoch [22440/30000], Loss: 0.3887\n",
      "Epoch [22460/30000], Loss: 0.3887\n",
      "Epoch [22480/30000], Loss: 0.3887\n",
      "Epoch [22500/30000], Loss: 0.3887\n",
      "Epoch [22520/30000], Loss: 0.3887\n",
      "Epoch [22540/30000], Loss: 0.3886\n",
      "Epoch [22560/30000], Loss: 0.3886\n",
      "Epoch [22580/30000], Loss: 0.3886\n",
      "Epoch [22600/30000], Loss: 0.3886\n",
      "Epoch [22620/30000], Loss: 0.3886\n",
      "Epoch [22640/30000], Loss: 0.3886\n",
      "Epoch [22660/30000], Loss: 0.3886\n",
      "Epoch [22680/30000], Loss: 0.3886\n",
      "Epoch [22700/30000], Loss: 0.3885\n",
      "Epoch [22720/30000], Loss: 0.3885\n",
      "Epoch [22740/30000], Loss: 0.3885\n",
      "Epoch [22760/30000], Loss: 0.3885\n",
      "Epoch [22780/30000], Loss: 0.3885\n",
      "Epoch [22800/30000], Loss: 0.3885\n",
      "Epoch [22820/30000], Loss: 0.3885\n",
      "Epoch [22840/30000], Loss: 0.3885\n",
      "Epoch [22860/30000], Loss: 0.3884\n",
      "Epoch [22880/30000], Loss: 0.3884\n",
      "Epoch [22900/30000], Loss: 0.3884\n",
      "Epoch [22920/30000], Loss: 0.3884\n",
      "Epoch [22940/30000], Loss: 0.3884\n",
      "Epoch [22960/30000], Loss: 0.3884\n",
      "Epoch [22980/30000], Loss: 0.3884\n",
      "Epoch [23000/30000], Loss: 0.3884\n",
      "Epoch [23020/30000], Loss: 0.3883\n",
      "Epoch [23040/30000], Loss: 0.3883\n",
      "Epoch [23060/30000], Loss: 0.3883\n",
      "Epoch [23080/30000], Loss: 0.3883\n",
      "Epoch [23100/30000], Loss: 0.3883\n",
      "Epoch [23120/30000], Loss: 0.3883\n",
      "Epoch [23140/30000], Loss: 0.3883\n",
      "Epoch [23160/30000], Loss: 0.3883\n",
      "Epoch [23180/30000], Loss: 0.3883\n",
      "Epoch [23200/30000], Loss: 0.3882\n",
      "Epoch [23220/30000], Loss: 0.3882\n",
      "Epoch [23240/30000], Loss: 0.3882\n",
      "Epoch [23260/30000], Loss: 0.3882\n",
      "Epoch [23280/30000], Loss: 0.3882\n",
      "Epoch [23300/30000], Loss: 0.3882\n",
      "Epoch [23320/30000], Loss: 0.3882\n",
      "Epoch [23340/30000], Loss: 0.3882\n",
      "Epoch [23360/30000], Loss: 0.3881\n",
      "Epoch [23380/30000], Loss: 0.3881\n",
      "Epoch [23400/30000], Loss: 0.3881\n",
      "Epoch [23420/30000], Loss: 0.3881\n",
      "Epoch [23440/30000], Loss: 0.3881\n",
      "Epoch [23460/30000], Loss: 0.3881\n",
      "Epoch [23480/30000], Loss: 0.3881\n",
      "Epoch [23500/30000], Loss: 0.3881\n",
      "Epoch [23520/30000], Loss: 0.3881\n",
      "Epoch [23540/30000], Loss: 0.3880\n",
      "Epoch [23560/30000], Loss: 0.3880\n",
      "Epoch [23580/30000], Loss: 0.3880\n",
      "Epoch [23600/30000], Loss: 0.3880\n",
      "Epoch [23620/30000], Loss: 0.3880\n",
      "Epoch [23640/30000], Loss: 0.3880\n",
      "Epoch [23660/30000], Loss: 0.3880\n",
      "Epoch [23680/30000], Loss: 0.3880\n",
      "Epoch [23700/30000], Loss: 0.3880\n",
      "Epoch [23720/30000], Loss: 0.3879\n",
      "Epoch [23740/30000], Loss: 0.3879\n",
      "Epoch [23760/30000], Loss: 0.3879\n",
      "Epoch [23780/30000], Loss: 0.3879\n",
      "Epoch [23800/30000], Loss: 0.3879\n",
      "Epoch [23820/30000], Loss: 0.3879\n",
      "Epoch [23840/30000], Loss: 0.3879\n",
      "Epoch [23860/30000], Loss: 0.3879\n",
      "Epoch [23880/30000], Loss: 0.3879\n",
      "Epoch [23900/30000], Loss: 0.3878\n",
      "Epoch [23920/30000], Loss: 0.3878\n",
      "Epoch [23940/30000], Loss: 0.3878\n",
      "Epoch [23960/30000], Loss: 0.3878\n",
      "Epoch [23980/30000], Loss: 0.3878\n",
      "Epoch [24000/30000], Loss: 0.3878\n",
      "Epoch [24020/30000], Loss: 0.3878\n",
      "Epoch [24040/30000], Loss: 0.3878\n",
      "Epoch [24060/30000], Loss: 0.3878\n",
      "Epoch [24080/30000], Loss: 0.3877\n",
      "Epoch [24100/30000], Loss: 0.3877\n",
      "Epoch [24120/30000], Loss: 0.3877\n",
      "Epoch [24140/30000], Loss: 0.3877\n",
      "Epoch [24160/30000], Loss: 0.3877\n",
      "Epoch [24180/30000], Loss: 0.3877\n",
      "Epoch [24200/30000], Loss: 0.3877\n",
      "Epoch [24220/30000], Loss: 0.3877\n",
      "Epoch [24240/30000], Loss: 0.3877\n",
      "Epoch [24260/30000], Loss: 0.3876\n",
      "Epoch [24280/30000], Loss: 0.3876\n",
      "Epoch [24300/30000], Loss: 0.3876\n",
      "Epoch [24320/30000], Loss: 0.3876\n",
      "Epoch [24340/30000], Loss: 0.3876\n",
      "Epoch [24360/30000], Loss: 0.3876\n",
      "Epoch [24380/30000], Loss: 0.3876\n",
      "Epoch [24400/30000], Loss: 0.3876\n",
      "Epoch [24420/30000], Loss: 0.3876\n",
      "Epoch [24440/30000], Loss: 0.3876\n",
      "Epoch [24460/30000], Loss: 0.3875\n",
      "Epoch [24480/30000], Loss: 0.3875\n",
      "Epoch [24500/30000], Loss: 0.3875\n",
      "Epoch [24520/30000], Loss: 0.3875\n",
      "Epoch [24540/30000], Loss: 0.3875\n",
      "Epoch [24560/30000], Loss: 0.3875\n",
      "Epoch [24580/30000], Loss: 0.3875\n",
      "Epoch [24600/30000], Loss: 0.3875\n",
      "Epoch [24620/30000], Loss: 0.3875\n",
      "Epoch [24640/30000], Loss: 0.3874\n",
      "Epoch [24660/30000], Loss: 0.3874\n",
      "Epoch [24680/30000], Loss: 0.3874\n",
      "Epoch [24700/30000], Loss: 0.3874\n",
      "Epoch [24720/30000], Loss: 0.3874\n",
      "Epoch [24740/30000], Loss: 0.3874\n",
      "Epoch [24760/30000], Loss: 0.3874\n",
      "Epoch [24780/30000], Loss: 0.3874\n",
      "Epoch [24800/30000], Loss: 0.3874\n",
      "Epoch [24820/30000], Loss: 0.3874\n",
      "Epoch [24840/30000], Loss: 0.3873\n",
      "Epoch [24860/30000], Loss: 0.3873\n",
      "Epoch [24880/30000], Loss: 0.3873\n",
      "Epoch [24900/30000], Loss: 0.3873\n",
      "Epoch [24920/30000], Loss: 0.3873\n",
      "Epoch [24940/30000], Loss: 0.3873\n",
      "Epoch [24960/30000], Loss: 0.3873\n",
      "Epoch [24980/30000], Loss: 0.3873\n",
      "Epoch [25000/30000], Loss: 0.3873\n",
      "Epoch [25020/30000], Loss: 0.3873\n",
      "Epoch [25040/30000], Loss: 0.3872\n",
      "Epoch [25060/30000], Loss: 0.3872\n",
      "Epoch [25080/30000], Loss: 0.3872\n",
      "Epoch [25100/30000], Loss: 0.3872\n",
      "Epoch [25120/30000], Loss: 0.3872\n",
      "Epoch [25140/30000], Loss: 0.3872\n",
      "Epoch [25160/30000], Loss: 0.3872\n",
      "Epoch [25180/30000], Loss: 0.3872\n",
      "Epoch [25200/30000], Loss: 0.3872\n",
      "Epoch [25220/30000], Loss: 0.3872\n",
      "Epoch [25240/30000], Loss: 0.3871\n",
      "Epoch [25260/30000], Loss: 0.3871\n",
      "Epoch [25280/30000], Loss: 0.3871\n",
      "Epoch [25300/30000], Loss: 0.3871\n",
      "Epoch [25320/30000], Loss: 0.3871\n",
      "Epoch [25340/30000], Loss: 0.3871\n",
      "Epoch [25360/30000], Loss: 0.3871\n",
      "Epoch [25380/30000], Loss: 0.3871\n",
      "Epoch [25400/30000], Loss: 0.3871\n",
      "Epoch [25420/30000], Loss: 0.3871\n",
      "Epoch [25440/30000], Loss: 0.3870\n",
      "Epoch [25460/30000], Loss: 0.3870\n",
      "Epoch [25480/30000], Loss: 0.3870\n",
      "Epoch [25500/30000], Loss: 0.3870\n",
      "Epoch [25520/30000], Loss: 0.3870\n",
      "Epoch [25540/30000], Loss: 0.3870\n",
      "Epoch [25560/30000], Loss: 0.3870\n",
      "Epoch [25580/30000], Loss: 0.3870\n",
      "Epoch [25600/30000], Loss: 0.3870\n",
      "Epoch [25620/30000], Loss: 0.3870\n",
      "Epoch [25640/30000], Loss: 0.3869\n",
      "Epoch [25660/30000], Loss: 0.3869\n",
      "Epoch [25680/30000], Loss: 0.3869\n",
      "Epoch [25700/30000], Loss: 0.3869\n",
      "Epoch [25720/30000], Loss: 0.3869\n",
      "Epoch [25740/30000], Loss: 0.3869\n",
      "Epoch [25760/30000], Loss: 0.3869\n",
      "Epoch [25780/30000], Loss: 0.3869\n",
      "Epoch [25800/30000], Loss: 0.3869\n",
      "Epoch [25820/30000], Loss: 0.3869\n",
      "Epoch [25840/30000], Loss: 0.3868\n",
      "Epoch [25860/30000], Loss: 0.3868\n",
      "Epoch [25880/30000], Loss: 0.3868\n",
      "Epoch [25900/30000], Loss: 0.3868\n",
      "Epoch [25920/30000], Loss: 0.3868\n",
      "Epoch [25940/30000], Loss: 0.3868\n",
      "Epoch [25960/30000], Loss: 0.3868\n",
      "Epoch [25980/30000], Loss: 0.3868\n",
      "Epoch [26000/30000], Loss: 0.3868\n",
      "Epoch [26020/30000], Loss: 0.3868\n",
      "Epoch [26040/30000], Loss: 0.3868\n",
      "Epoch [26060/30000], Loss: 0.3867\n",
      "Epoch [26080/30000], Loss: 0.3867\n",
      "Epoch [26100/30000], Loss: 0.3867\n",
      "Epoch [26120/30000], Loss: 0.3867\n",
      "Epoch [26140/30000], Loss: 0.3867\n",
      "Epoch [26160/30000], Loss: 0.3867\n",
      "Epoch [26180/30000], Loss: 0.3867\n",
      "Epoch [26200/30000], Loss: 0.3867\n",
      "Epoch [26220/30000], Loss: 0.3867\n",
      "Epoch [26240/30000], Loss: 0.3867\n",
      "Epoch [26260/30000], Loss: 0.3867\n",
      "Epoch [26280/30000], Loss: 0.3866\n",
      "Epoch [26300/30000], Loss: 0.3866\n",
      "Epoch [26320/30000], Loss: 0.3866\n",
      "Epoch [26340/30000], Loss: 0.3866\n",
      "Epoch [26360/30000], Loss: 0.3866\n",
      "Epoch [26380/30000], Loss: 0.3866\n",
      "Epoch [26400/30000], Loss: 0.3866\n",
      "Epoch [26420/30000], Loss: 0.3866\n",
      "Epoch [26440/30000], Loss: 0.3866\n",
      "Epoch [26460/30000], Loss: 0.3866\n",
      "Epoch [26480/30000], Loss: 0.3866\n",
      "Epoch [26500/30000], Loss: 0.3865\n",
      "Epoch [26520/30000], Loss: 0.3865\n",
      "Epoch [26540/30000], Loss: 0.3865\n",
      "Epoch [26560/30000], Loss: 0.3865\n",
      "Epoch [26580/30000], Loss: 0.3865\n",
      "Epoch [26600/30000], Loss: 0.3865\n",
      "Epoch [26620/30000], Loss: 0.3865\n",
      "Epoch [26640/30000], Loss: 0.3865\n",
      "Epoch [26660/30000], Loss: 0.3865\n",
      "Epoch [26680/30000], Loss: 0.3865\n",
      "Epoch [26700/30000], Loss: 0.3865\n",
      "Epoch [26720/30000], Loss: 0.3864\n",
      "Epoch [26740/30000], Loss: 0.3864\n",
      "Epoch [26760/30000], Loss: 0.3864\n",
      "Epoch [26780/30000], Loss: 0.3864\n",
      "Epoch [26800/30000], Loss: 0.3864\n",
      "Epoch [26820/30000], Loss: 0.3864\n",
      "Epoch [26840/30000], Loss: 0.3864\n",
      "Epoch [26860/30000], Loss: 0.3864\n",
      "Epoch [26880/30000], Loss: 0.3864\n",
      "Epoch [26900/30000], Loss: 0.3864\n",
      "Epoch [26920/30000], Loss: 0.3864\n",
      "Epoch [26940/30000], Loss: 0.3863\n",
      "Epoch [26960/30000], Loss: 0.3863\n",
      "Epoch [26980/30000], Loss: 0.3863\n",
      "Epoch [27000/30000], Loss: 0.3863\n",
      "Epoch [27020/30000], Loss: 0.3863\n",
      "Epoch [27040/30000], Loss: 0.3863\n",
      "Epoch [27060/30000], Loss: 0.3863\n",
      "Epoch [27080/30000], Loss: 0.3863\n",
      "Epoch [27100/30000], Loss: 0.3863\n",
      "Epoch [27120/30000], Loss: 0.3863\n",
      "Epoch [27140/30000], Loss: 0.3863\n",
      "Epoch [27160/30000], Loss: 0.3863\n",
      "Epoch [27180/30000], Loss: 0.3862\n",
      "Epoch [27200/30000], Loss: 0.3862\n",
      "Epoch [27220/30000], Loss: 0.3862\n",
      "Epoch [27240/30000], Loss: 0.3862\n",
      "Epoch [27260/30000], Loss: 0.3862\n",
      "Epoch [27280/30000], Loss: 0.3862\n",
      "Epoch [27300/30000], Loss: 0.3862\n",
      "Epoch [27320/30000], Loss: 0.3862\n",
      "Epoch [27340/30000], Loss: 0.3862\n",
      "Epoch [27360/30000], Loss: 0.3862\n",
      "Epoch [27380/30000], Loss: 0.3862\n",
      "Epoch [27400/30000], Loss: 0.3861\n",
      "Epoch [27420/30000], Loss: 0.3861\n",
      "Epoch [27440/30000], Loss: 0.3861\n",
      "Epoch [27460/30000], Loss: 0.3861\n",
      "Epoch [27480/30000], Loss: 0.3861\n",
      "Epoch [27500/30000], Loss: 0.3861\n",
      "Epoch [27520/30000], Loss: 0.3861\n",
      "Epoch [27540/30000], Loss: 0.3861\n",
      "Epoch [27560/30000], Loss: 0.3861\n",
      "Epoch [27580/30000], Loss: 0.3861\n",
      "Epoch [27600/30000], Loss: 0.3861\n",
      "Epoch [27620/30000], Loss: 0.3861\n",
      "Epoch [27640/30000], Loss: 0.3860\n",
      "Epoch [27660/30000], Loss: 0.3860\n",
      "Epoch [27680/30000], Loss: 0.3860\n",
      "Epoch [27700/30000], Loss: 0.3860\n",
      "Epoch [27720/30000], Loss: 0.3860\n",
      "Epoch [27740/30000], Loss: 0.3860\n",
      "Epoch [27760/30000], Loss: 0.3860\n",
      "Epoch [27780/30000], Loss: 0.3860\n",
      "Epoch [27800/30000], Loss: 0.3860\n",
      "Epoch [27820/30000], Loss: 0.3860\n",
      "Epoch [27840/30000], Loss: 0.3860\n",
      "Epoch [27860/30000], Loss: 0.3860\n",
      "Epoch [27880/30000], Loss: 0.3859\n",
      "Epoch [27900/30000], Loss: 0.3859\n",
      "Epoch [27920/30000], Loss: 0.3859\n",
      "Epoch [27940/30000], Loss: 0.3859\n",
      "Epoch [27960/30000], Loss: 0.3859\n",
      "Epoch [27980/30000], Loss: 0.3859\n",
      "Epoch [28000/30000], Loss: 0.3859\n",
      "Epoch [28020/30000], Loss: 0.3859\n",
      "Epoch [28040/30000], Loss: 0.3859\n",
      "Epoch [28060/30000], Loss: 0.3859\n",
      "Epoch [28080/30000], Loss: 0.3859\n",
      "Epoch [28100/30000], Loss: 0.3859\n",
      "Epoch [28120/30000], Loss: 0.3858\n",
      "Epoch [28140/30000], Loss: 0.3858\n",
      "Epoch [28160/30000], Loss: 0.3858\n",
      "Epoch [28180/30000], Loss: 0.3858\n",
      "Epoch [28200/30000], Loss: 0.3858\n",
      "Epoch [28220/30000], Loss: 0.3858\n",
      "Epoch [28240/30000], Loss: 0.3858\n",
      "Epoch [28260/30000], Loss: 0.3858\n",
      "Epoch [28280/30000], Loss: 0.3858\n",
      "Epoch [28300/30000], Loss: 0.3858\n",
      "Epoch [28320/30000], Loss: 0.3858\n",
      "Epoch [28340/30000], Loss: 0.3858\n",
      "Epoch [28360/30000], Loss: 0.3858\n",
      "Epoch [28380/30000], Loss: 0.3857\n",
      "Epoch [28400/30000], Loss: 0.3857\n",
      "Epoch [28420/30000], Loss: 0.3857\n",
      "Epoch [28440/30000], Loss: 0.3857\n",
      "Epoch [28460/30000], Loss: 0.3857\n",
      "Epoch [28480/30000], Loss: 0.3857\n",
      "Epoch [28500/30000], Loss: 0.3857\n",
      "Epoch [28520/30000], Loss: 0.3857\n",
      "Epoch [28540/30000], Loss: 0.3857\n",
      "Epoch [28560/30000], Loss: 0.3857\n",
      "Epoch [28580/30000], Loss: 0.3857\n",
      "Epoch [28600/30000], Loss: 0.3857\n",
      "Epoch [28620/30000], Loss: 0.3856\n",
      "Epoch [28640/30000], Loss: 0.3856\n",
      "Epoch [28660/30000], Loss: 0.3856\n",
      "Epoch [28680/30000], Loss: 0.3856\n",
      "Epoch [28700/30000], Loss: 0.3856\n",
      "Epoch [28720/30000], Loss: 0.3856\n",
      "Epoch [28740/30000], Loss: 0.3856\n",
      "Epoch [28760/30000], Loss: 0.3856\n",
      "Epoch [28780/30000], Loss: 0.3856\n",
      "Epoch [28800/30000], Loss: 0.3856\n",
      "Epoch [28820/30000], Loss: 0.3856\n",
      "Epoch [28840/30000], Loss: 0.3856\n",
      "Epoch [28860/30000], Loss: 0.3856\n",
      "Epoch [28880/30000], Loss: 0.3855\n",
      "Epoch [28900/30000], Loss: 0.3855\n",
      "Epoch [28920/30000], Loss: 0.3855\n",
      "Epoch [28940/30000], Loss: 0.3855\n",
      "Epoch [28960/30000], Loss: 0.3855\n",
      "Epoch [28980/30000], Loss: 0.3855\n",
      "Epoch [29000/30000], Loss: 0.3855\n",
      "Epoch [29020/30000], Loss: 0.3855\n",
      "Epoch [29040/30000], Loss: 0.3855\n",
      "Epoch [29060/30000], Loss: 0.3855\n",
      "Epoch [29080/30000], Loss: 0.3855\n",
      "Epoch [29100/30000], Loss: 0.3855\n",
      "Epoch [29120/30000], Loss: 0.3855\n",
      "Epoch [29140/30000], Loss: 0.3855\n",
      "Epoch [29160/30000], Loss: 0.3854\n",
      "Epoch [29180/30000], Loss: 0.3854\n",
      "Epoch [29200/30000], Loss: 0.3854\n",
      "Epoch [29220/30000], Loss: 0.3854\n",
      "Epoch [29240/30000], Loss: 0.3854\n",
      "Epoch [29260/30000], Loss: 0.3854\n",
      "Epoch [29280/30000], Loss: 0.3854\n",
      "Epoch [29300/30000], Loss: 0.3854\n",
      "Epoch [29320/30000], Loss: 0.3854\n",
      "Epoch [29340/30000], Loss: 0.3854\n",
      "Epoch [29360/30000], Loss: 0.3854\n",
      "Epoch [29380/30000], Loss: 0.3854\n",
      "Epoch [29400/30000], Loss: 0.3854\n",
      "Epoch [29420/30000], Loss: 0.3853\n",
      "Epoch [29440/30000], Loss: 0.3853\n",
      "Epoch [29460/30000], Loss: 0.3853\n",
      "Epoch [29480/30000], Loss: 0.3853\n",
      "Epoch [29500/30000], Loss: 0.3853\n",
      "Epoch [29520/30000], Loss: 0.3853\n",
      "Epoch [29540/30000], Loss: 0.3853\n",
      "Epoch [29560/30000], Loss: 0.3853\n",
      "Epoch [29580/30000], Loss: 0.3853\n",
      "Epoch [29600/30000], Loss: 0.3853\n",
      "Epoch [29620/30000], Loss: 0.3853\n",
      "Epoch [29640/30000], Loss: 0.3853\n",
      "Epoch [29660/30000], Loss: 0.3853\n",
      "Epoch [29680/30000], Loss: 0.3853\n",
      "Epoch [29700/30000], Loss: 0.3852\n",
      "Epoch [29720/30000], Loss: 0.3852\n",
      "Epoch [29740/30000], Loss: 0.3852\n",
      "Epoch [29760/30000], Loss: 0.3852\n",
      "Epoch [29780/30000], Loss: 0.3852\n",
      "Epoch [29800/30000], Loss: 0.3852\n",
      "Epoch [29820/30000], Loss: 0.3852\n",
      "Epoch [29840/30000], Loss: 0.3852\n",
      "Epoch [29860/30000], Loss: 0.3852\n",
      "Epoch [29880/30000], Loss: 0.3852\n",
      "Epoch [29900/30000], Loss: 0.3852\n",
      "Epoch [29920/30000], Loss: 0.3852\n",
      "Epoch [29940/30000], Loss: 0.3852\n",
      "Epoch [29960/30000], Loss: 0.3852\n",
      "Epoch [29980/30000], Loss: 0.3851\n",
      "Epoch [30000/30000], Loss: 0.3851\n"
     ]
    }
   ],
   "source": [
    "# Train the single-layer MLP model\n",
    "epochs = 30000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4d007caf1d7b3",
   "metadata": {},
   "source": [
    "## Evaluating the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dbb835ea10a7583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:19:17.530878Z",
     "start_time": "2024-11-04T21:19:17.524181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9659998359678498)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the predicted outputs\n",
    "outputs = model(X_val)\n",
    "\n",
    "# compute the estimated probabilities of the positive class\n",
    "py_hat_val = outputs[:,1]\n",
    "\n",
    "# convert the probabilities to numpy format\n",
    "py_hat_val = py_hat_val.detach().numpy()\n",
    "\n",
    "roc_auc_score(y_val, py_hat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f414b270d1e4206",
   "metadata": {},
   "source": [
    "### How did we do relative to logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdd467cb45130e",
   "metadata": {},
   "source": [
    "## Creating an MLP with an arbitrary number of hidden layers\n",
    "- PyTorch makes it easy to create MLPs with an arbitrary number of hidden layers\n",
    "- We will create another class inheriting from ```nn.Module``` that allows us to specify the number of hidden layers\n",
    "- In the code below, the list ```hidden_sizes``` contains the number of neurons in each hidden layer\n",
    "- The number of elements in this list defines the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86dd47adb3c3edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:21:40.169575Z",
     "start_time": "2024-11-04T21:21:40.165741Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MultiLayerMLP, self).__init__()\n",
    "        # Define the layers sequentially\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h in hidden_sizes: # hidden_sizes \n",
    "            # Add a hidden layer with ReLU activation\n",
    "            layers.append(nn.Linear(in_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h\n",
    "        # Add the final output layer\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        # Combine layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bcd2b1a3e52f8",
   "metadata": {},
   "source": [
    "## Creating an instance of the multi-layer MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "baa73affc747b41a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:23:55.695613Z",
     "start_time": "2024-11-04T21:23:55.690938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100, 50]  # Two hidden layers with 10 and 5 neurons, respectively\n",
    "model = MultiLayerMLP(input_size, hidden_sizes, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1455f1bcbdc5a7",
   "metadata": {},
   "source": [
    "## Training the model (same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9623ee210e079b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:23:58.237238Z",
     "start_time": "2024-11-04T21:23:58.233254Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ecbeaceb65f015e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:24:29.079707Z",
     "start_time": "2024-11-04T21:24:11.206569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/3000], Loss: 0.6523\n",
      "Epoch [40/3000], Loss: 0.6498\n",
      "Epoch [60/3000], Loss: 0.6471\n",
      "Epoch [80/3000], Loss: 0.6444\n",
      "Epoch [100/3000], Loss: 0.6416\n",
      "Epoch [120/3000], Loss: 0.6387\n",
      "Epoch [140/3000], Loss: 0.6357\n",
      "Epoch [160/3000], Loss: 0.6327\n",
      "Epoch [180/3000], Loss: 0.6296\n",
      "Epoch [200/3000], Loss: 0.6264\n",
      "Epoch [220/3000], Loss: 0.6232\n",
      "Epoch [240/3000], Loss: 0.6200\n",
      "Epoch [260/3000], Loss: 0.6167\n",
      "Epoch [280/3000], Loss: 0.6135\n",
      "Epoch [300/3000], Loss: 0.6102\n",
      "Epoch [320/3000], Loss: 0.6070\n",
      "Epoch [340/3000], Loss: 0.6038\n",
      "Epoch [360/3000], Loss: 0.6006\n",
      "Epoch [380/3000], Loss: 0.5974\n",
      "Epoch [400/3000], Loss: 0.5944\n",
      "Epoch [420/3000], Loss: 0.5913\n",
      "Epoch [440/3000], Loss: 0.5884\n",
      "Epoch [460/3000], Loss: 0.5854\n",
      "Epoch [480/3000], Loss: 0.5826\n",
      "Epoch [500/3000], Loss: 0.5798\n",
      "Epoch [520/3000], Loss: 0.5772\n",
      "Epoch [540/3000], Loss: 0.5746\n",
      "Epoch [560/3000], Loss: 0.5720\n",
      "Epoch [580/3000], Loss: 0.5696\n",
      "Epoch [600/3000], Loss: 0.5672\n",
      "Epoch [620/3000], Loss: 0.5649\n",
      "Epoch [640/3000], Loss: 0.5627\n",
      "Epoch [660/3000], Loss: 0.5605\n",
      "Epoch [680/3000], Loss: 0.5585\n",
      "Epoch [700/3000], Loss: 0.5565\n",
      "Epoch [720/3000], Loss: 0.5545\n",
      "Epoch [740/3000], Loss: 0.5527\n",
      "Epoch [760/3000], Loss: 0.5509\n",
      "Epoch [780/3000], Loss: 0.5491\n",
      "Epoch [800/3000], Loss: 0.5475\n",
      "Epoch [820/3000], Loss: 0.5458\n",
      "Epoch [840/3000], Loss: 0.5443\n",
      "Epoch [860/3000], Loss: 0.5427\n",
      "Epoch [880/3000], Loss: 0.5413\n",
      "Epoch [900/3000], Loss: 0.5398\n",
      "Epoch [920/3000], Loss: 0.5385\n",
      "Epoch [940/3000], Loss: 0.5371\n",
      "Epoch [960/3000], Loss: 0.5358\n",
      "Epoch [980/3000], Loss: 0.5346\n",
      "Epoch [1000/3000], Loss: 0.5333\n",
      "Epoch [1020/3000], Loss: 0.5321\n",
      "Epoch [1040/3000], Loss: 0.5310\n",
      "Epoch [1060/3000], Loss: 0.5298\n",
      "Epoch [1080/3000], Loss: 0.5287\n",
      "Epoch [1100/3000], Loss: 0.5277\n",
      "Epoch [1120/3000], Loss: 0.5266\n",
      "Epoch [1140/3000], Loss: 0.5256\n",
      "Epoch [1160/3000], Loss: 0.5246\n",
      "Epoch [1180/3000], Loss: 0.5236\n",
      "Epoch [1200/3000], Loss: 0.5226\n",
      "Epoch [1220/3000], Loss: 0.5217\n",
      "Epoch [1240/3000], Loss: 0.5208\n",
      "Epoch [1260/3000], Loss: 0.5199\n",
      "Epoch [1280/3000], Loss: 0.5190\n",
      "Epoch [1300/3000], Loss: 0.5181\n",
      "Epoch [1320/3000], Loss: 0.5173\n",
      "Epoch [1340/3000], Loss: 0.5164\n",
      "Epoch [1360/3000], Loss: 0.5156\n",
      "Epoch [1380/3000], Loss: 0.5148\n",
      "Epoch [1400/3000], Loss: 0.5140\n",
      "Epoch [1420/3000], Loss: 0.5132\n",
      "Epoch [1440/3000], Loss: 0.5124\n",
      "Epoch [1460/3000], Loss: 0.5117\n",
      "Epoch [1480/3000], Loss: 0.5109\n",
      "Epoch [1500/3000], Loss: 0.5102\n",
      "Epoch [1520/3000], Loss: 0.5094\n",
      "Epoch [1540/3000], Loss: 0.5087\n",
      "Epoch [1560/3000], Loss: 0.5080\n",
      "Epoch [1580/3000], Loss: 0.5072\n",
      "Epoch [1600/3000], Loss: 0.5065\n",
      "Epoch [1620/3000], Loss: 0.5058\n",
      "Epoch [1640/3000], Loss: 0.5051\n",
      "Epoch [1660/3000], Loss: 0.5045\n",
      "Epoch [1680/3000], Loss: 0.5038\n",
      "Epoch [1700/3000], Loss: 0.5031\n",
      "Epoch [1720/3000], Loss: 0.5024\n",
      "Epoch [1740/3000], Loss: 0.5018\n",
      "Epoch [1760/3000], Loss: 0.5011\n",
      "Epoch [1780/3000], Loss: 0.5005\n",
      "Epoch [1800/3000], Loss: 0.4998\n",
      "Epoch [1820/3000], Loss: 0.4992\n",
      "Epoch [1840/3000], Loss: 0.4985\n",
      "Epoch [1860/3000], Loss: 0.4979\n",
      "Epoch [1880/3000], Loss: 0.4973\n",
      "Epoch [1900/3000], Loss: 0.4966\n",
      "Epoch [1920/3000], Loss: 0.4960\n",
      "Epoch [1940/3000], Loss: 0.4954\n",
      "Epoch [1960/3000], Loss: 0.4948\n",
      "Epoch [1980/3000], Loss: 0.4941\n",
      "Epoch [2000/3000], Loss: 0.4935\n",
      "Epoch [2020/3000], Loss: 0.4929\n",
      "Epoch [2040/3000], Loss: 0.4923\n",
      "Epoch [2060/3000], Loss: 0.4917\n",
      "Epoch [2080/3000], Loss: 0.4911\n",
      "Epoch [2100/3000], Loss: 0.4905\n",
      "Epoch [2120/3000], Loss: 0.4899\n",
      "Epoch [2140/3000], Loss: 0.4893\n",
      "Epoch [2160/3000], Loss: 0.4887\n",
      "Epoch [2180/3000], Loss: 0.4881\n",
      "Epoch [2200/3000], Loss: 0.4875\n",
      "Epoch [2220/3000], Loss: 0.4869\n",
      "Epoch [2240/3000], Loss: 0.4863\n",
      "Epoch [2260/3000], Loss: 0.4857\n",
      "Epoch [2280/3000], Loss: 0.4851\n",
      "Epoch [2300/3000], Loss: 0.4845\n",
      "Epoch [2320/3000], Loss: 0.4840\n",
      "Epoch [2340/3000], Loss: 0.4834\n",
      "Epoch [2360/3000], Loss: 0.4828\n",
      "Epoch [2380/3000], Loss: 0.4822\n",
      "Epoch [2400/3000], Loss: 0.4817\n",
      "Epoch [2420/3000], Loss: 0.4811\n",
      "Epoch [2440/3000], Loss: 0.4805\n",
      "Epoch [2460/3000], Loss: 0.4799\n",
      "Epoch [2480/3000], Loss: 0.4794\n",
      "Epoch [2500/3000], Loss: 0.4788\n",
      "Epoch [2520/3000], Loss: 0.4782\n",
      "Epoch [2540/3000], Loss: 0.4777\n",
      "Epoch [2560/3000], Loss: 0.4771\n",
      "Epoch [2580/3000], Loss: 0.4765\n",
      "Epoch [2600/3000], Loss: 0.4760\n",
      "Epoch [2620/3000], Loss: 0.4754\n",
      "Epoch [2640/3000], Loss: 0.4748\n",
      "Epoch [2660/3000], Loss: 0.4743\n",
      "Epoch [2680/3000], Loss: 0.4737\n",
      "Epoch [2700/3000], Loss: 0.4731\n",
      "Epoch [2720/3000], Loss: 0.4726\n",
      "Epoch [2740/3000], Loss: 0.4720\n",
      "Epoch [2760/3000], Loss: 0.4714\n",
      "Epoch [2780/3000], Loss: 0.4708\n",
      "Epoch [2800/3000], Loss: 0.4703\n",
      "Epoch [2820/3000], Loss: 0.4697\n",
      "Epoch [2840/3000], Loss: 0.4691\n",
      "Epoch [2860/3000], Loss: 0.4685\n",
      "Epoch [2880/3000], Loss: 0.4680\n",
      "Epoch [2900/3000], Loss: 0.4674\n",
      "Epoch [2920/3000], Loss: 0.4668\n",
      "Epoch [2940/3000], Loss: 0.4662\n",
      "Epoch [2960/3000], Loss: 0.4657\n",
      "Epoch [2980/3000], Loss: 0.4651\n",
      "Epoch [3000/3000], Loss: 0.4645\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0c7309190c594",
   "metadata": {},
   "source": [
    "## Evaluating the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49f94b33a18f72c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:25:51.654759Z",
     "start_time": "2024-11-04T21:25:51.644952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9112716092354101)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the predicted outputs\n",
    "outputs = model(X_val)\n",
    "\n",
    "# compute the estimated probabilities of the positive class\n",
    "py_hat_val = outputs[:,1]\n",
    "\n",
    "# convert the probabilities to numpy format\n",
    "py_hat_val = py_hat_val.detach().numpy()\n",
    "\n",
    "roc_auc_score(y_val, py_hat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31accaef1dc74590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
